{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaf03429",
      "metadata": {
        "id": "eaf03429"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jaNNY3tsKG8I",
      "metadata": {
        "id": "jaNNY3tsKG8I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nu7i62hlGthO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu7i62hlGthO",
        "outputId": "5e86eb18-3554-4b4d-e308-a0e2a3064d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from ta-lib) (1.3.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from ta-lib) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta-lib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Ej4IeD7EeF0t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej4IeD7EeF0t",
        "outputId": "50c52f5c-1324-4f45-eba6-5b250b6c0ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: metaapi-cloud-sdk in /usr/local/lib/python3.12/dist-packages (29.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.12.15)\n",
            "Requirement already satisfied: python-engineio<4.0.0,>=3.14.2 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (4.15.0)\n",
            "Requirement already satisfied: iso8601 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2025.2)\n",
            "Requirement already satisfied: python-socketio<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (0.28.1)\n",
            "Requirement already satisfied: metaapi-cloud-copyfactory-sdk<13.0.0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (12.0.0)\n",
            "Requirement already satisfied: metaapi-cloud-metastats-sdk<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (6.0.0)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade metaapi-cloud-sdk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DF5CD6j5nmdo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5CD6j5nmdo",
        "outputId": "bb831eb6-f7d9-419a-b57c-e83deb0c1dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.7\n",
            "ATR(ndarray high, ndarray low, ndarray close, int timeperiod=-0x80000000)\n",
            "\n",
            "ATR(high, low, close[, timeperiod=?])\n",
            "\n",
            "Average True Range (Volatility Indicators)\n",
            "\n",
            "Inputs:\n",
            "    prices: ['high', 'low', 'close']\n",
            "Parameters:\n",
            "    timeperiod: 14\n",
            "Outputs:\n",
            "    real\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function talib._ta_lib.ATR(high, low, close, timeperiod=-2147483648)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import talib as ta\n",
        "print(ta.__version__)  # Should print something like 0.4.28\n",
        "print(ta.ATR.__doc__)  # Confirm ATR function works\n",
        "ta.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime as dt\n",
        "import logging\n",
        "import math\n",
        "import re\n",
        "\n",
        "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
        "\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from metaapi_cloud_sdk import MetaApi\n",
        "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import json, re, traceback, requests\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ==== MODEL LOADING (place this ABOVE `asyncio.run(main())`) ====\n",
        "from pathlib import Path\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "KXSLuBFY_SB2",
      "metadata": {
        "id": "KXSLuBFY_SB2"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost\n",
        "\n",
        "# --- Scikit-learn Models ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    VotingClassifier,\n",
        "    ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Scikit-learn Preprocessing ---\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    label_binarize)\n",
        "\n",
        "# --- Scikit-learn Model Selection ---\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV)\n",
        "\n",
        "# --- Scikit-learn Metrics ---\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ============================================================\n",
        "# 4. DEEP LEARNING (TENSORFLOW / KERAS)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# ============================================================\n",
        "# 5. MODEL PERSISTENCE\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "KFfn45ty82qv",
      "metadata": {
        "id": "KFfn45ty82qv"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "jE8s8oLz2mos",
      "metadata": {
        "id": "jE8s8oLz2mos"
      },
      "outputs": [],
      "source": [
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI'\n",
        "ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5RqjoAtFwl",
        "outputId": "f4e1babe-6124-46d0-bef0-1b5f32ff58f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Course Folder/Forex/XAUUSD/\n",
            "env: META_API_TOKEN=eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI\n",
            "env: ACCOUNT_ID=163d9a57-1f07-4e78-a6af-036efe867c1b\n"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# Set_Up (corregido)\n",
        "# =======================\n",
        "import os\n",
        "\n",
        "# --- Parámetros de control ---\n",
        "process = 'Train'\n",
        "SYMBOL = 'BTCUSD'\n",
        "\n",
        "# OJO: esta ruta apunta a XAUUSD; ajústala si vas a trabajar BTCUSD\n",
        "root_data = '/content/drive/MyDrive/Course Folder/Forex/XAUUSD/'\n",
        "print(root_data)\n",
        "\n",
        "rolling_window = 100\n",
        "\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "LOT      = 1.0\n",
        "COMMENT  = \"Insta_ml\"\n",
        "CANDLE_NUMBER = 100\n",
        "\n",
        "stoploss_1    = 1.0\n",
        "takeprofit_1  = 0.7\n",
        "\n",
        "stoploss_2    = 1.0\n",
        "takeprofit_2  = 2.0\n",
        "\n",
        "\n",
        "# Si estás en Colab/Unix, exporta antes:\n",
        "%env META_API_TOKEN = eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI\n",
        "%env ACCOUNT_ID = 163d9a57-1f07-4e78-a6af-036efe867c1b\n",
        "\n",
        "\n",
        "# Lee primero del entorno; si no existe, usa el respaldo (si lo pusieras)\n",
        "META_API_TOKEN = os.getenv(\"META_API_TOKEN\", \"\").strip()\n",
        "if not META_API_TOKEN:\n",
        "    raise RuntimeError(\n",
        "        \"META_API_TOKEN no está definido. \"\n",
        "        \"Configúralo como variable de entorno o asigna un respaldo (no recomendado).\"\n",
        "    )\n",
        "\n",
        "# ACCOUNT_ID: usa el del entorno si existe; si no, usa el que ya tenías como default\n",
        "ACCOUNT_ID = os.getenv(\"ACCOUNT_ID\", \"163d9a57-1f07-4e78-a6af-036efe867c1b\").strip()\n",
        "if not ACCOUNT_ID:\n",
        "    raise RuntimeError(\"ACCOUNT_ID no está definido.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dTlwjirFwRfj",
      "metadata": {
        "id": "dTlwjirFwRfj"
      },
      "outputs": [],
      "source": [
        "__CONNECTION_CHECKED = False\n",
        "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
        "\n",
        "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Conecta vía RPC y espera sincronización. Lanza excepción si no se logra.\n",
        "    Devuelve (account, rpc_conn).\n",
        "    \"\"\"\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "    global REGION\n",
        "    REGION = getattr(account, 'region', REGION) if 'REGION' in globals() else getattr(account, 'region', None)\n",
        "    if account.state not in ('DEPLOYED', 'CONNECTED'):\n",
        "        await account.deploy()\n",
        "    try:\n",
        "        await account.wait_connected()\n",
        "    except TimeoutException:\n",
        "        await api.close()\n",
        "        raise\n",
        "\n",
        "    # refrescamos para leer estado/connectionStatus\n",
        "    try:\n",
        "        await account.reload()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Verificar que la cuenta esté conectada al broker\n",
        "    status = getattr(account, 'connection_status', getattr(account, 'connectionStatus', None))\n",
        "    if status != 'CONNECTED':\n",
        "        raise TimeoutException(f'Cuenta {account_id} no está conectada al broker (connection_status={status}).')\n",
        "\n",
        "    # Conexión RPC + sincronización del terminal\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    try:\n",
        "        await rpc_conn.connect()\n",
        "        await rpc_conn.wait_synchronized()  # espera a que el terminal esté listo\n",
        "    except TimeoutException:\n",
        "        await rpc_conn.close()\n",
        "        raise\n",
        "\n",
        "    # Sonda rápida para confirmar conectividad real con el terminal\n",
        "    try:\n",
        "        _ = await rpc_conn.get_account_information()\n",
        "    except Exception:\n",
        "        # si falla la sonda, igual devolvemos la conexión (ya sincronizada)\n",
        "        pass\n",
        "\n",
        "    return account, rpc_conn\n",
        "\n",
        "def _run(coro):\n",
        "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "def check_connection_once(token: str, account_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida la conexión y sincronización SOLO la primera vez que se llama.\n",
        "    En llamadas posteriores no vuelve a conectar.\n",
        "    \"\"\"\n",
        "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
        "    if __CONNECTION_CHECKED:\n",
        "        print(\"ℹ️ Conexión ya validada en esta sesión; no se repite.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
        "        __ACCOUNT_CONN = (account, rpc_conn)\n",
        "        __CONNECTION_CHECKED = True\n",
        "        print(f\"✅ Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
        "        return True\n",
        "    except TimeoutException as e:\n",
        "        print(f\"❌ Timeout esperando sincronización. ¿La cuenta está CONNECTED al broker? Detalle: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ No fue posible validar la conexión. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def _safe_json_dump(value):\n",
        "    try:\n",
        "        return json.dumps(value, indent=2, default=str)\n",
        "    except Exception:\n",
        "        return str(value)\n",
        "\n",
        "def print_order_error_details(ctx: dict, err: Exception):\n",
        "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
        "    print(\"\\n\" + \"✘\" * 70)\n",
        "    print(\"❌ Order failed\")\n",
        "    print(\"• Exception type:\", type(err).__name__)\n",
        "    print(\"• Message       :\", str(err))\n",
        "\n",
        "    # Known useful attributes often present on MetaApi exceptions\n",
        "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
        "        if hasattr(err, attr):\n",
        "            val = getattr(err, attr)\n",
        "            if val:\n",
        "                print(f\"• {attr:12}: {_safe_json_dump(val)}\")\n",
        "\n",
        "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
        "    msg = str(err)\n",
        "    m = re.search(r\"\\{.*\\}\", msg)\n",
        "    if m:\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "            print(\"• parsed_json  :\", _safe_json_dump(payload))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Stack (useful while debugging)\n",
        "    print(\"• traceback    :\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Context of the attempt\n",
        "    print(\"• context      :\", _safe_json_dump(ctx))\n",
        "    print(\"✘\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RjShygBIwWRp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjShygBIwWRp",
        "outputId": "7db4a54d-feb5-4f84-ce45-8e61c2a3fe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-09T21:52:00.657795] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-a.agiliumtrade.ai shared server.\n",
            "[2025-09-09T21:52:00.659380] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-b.agiliumtrade.ai shared server.\n",
            "[2025-09-09T21:52:01.164892] new-york:0: MetaApi websocket client connected to the MetaApi server\n",
            "[2025-09-09T21:52:01.170091] new-york:1: MetaApi websocket client connected to the MetaApi server\n",
            "✅ Conectado y sincronizado con MetaApi. account_id=163d9a57-1f07-4e78-a6af-036efe867c1b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfxZvvtJeFh1",
      "metadata": {
        "id": "HfxZvvtJeFh1"
      },
      "source": [
        "# Real_Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Sx6TbzBOBzZl",
      "metadata": {
        "id": "Sx6TbzBOBzZl"
      },
      "outputs": [],
      "source": [
        "SYMBOL = \"BTCUSD\"\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "WARMUP_FEATURE_BARS = 900\n",
        "\n",
        "CANDEL_NUMBER = 100\n",
        "\n",
        "time_frame_data = \"1m\"\n",
        "FETCH_INTERVAL = 60\n",
        "\n",
        "\n",
        "LOT     = 1.0\n",
        "COMMENT = \"Insta_ml\"\n",
        "\n",
        "length_1 = 300\n",
        "length_2 = 410\n",
        "length_3 = 710\n",
        "length_4 = 870\n",
        "\n",
        "smooth_1 = 3\n",
        "smooth_2 = 3\n",
        "smooth_3 = 3\n",
        "smooth_4 = 5\n",
        "\n",
        "INITIAL_SL         = -2\n",
        "FIRST_STEP_ATR     = 0.5\n",
        "GAP_FIRST_STEP_ATR = 2\n",
        "\n",
        "REGION = \"new-york\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zhTndYVi1TEV",
      "metadata": {
        "id": "zhTndYVi1TEV"
      },
      "source": [
        "# Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "kp4yJdGAjoeA",
      "metadata": {
        "id": "kp4yJdGAjoeA"
      },
      "outputs": [],
      "source": [
        "def kalman_line(source, kalman_length: int, smooth: int):\n",
        "\n",
        "    n = len(source)\n",
        "    kf_c = np.empty(n)            # núcleo del filtro\n",
        "    velo_c = np.zeros(n)          # componente de velocidad\n",
        "\n",
        "    sqrt_term   = np.sqrt(kalman_length / 10000.0 * 2.0)\n",
        "    length_term = kalman_length / 10000.0\n",
        "\n",
        "    # --------- inicialización (mismo efecto que `var` en Pine) ----------\n",
        "    kf_c[0]   = source.iloc[0]    # nz(kf_c[1], source) para la primera barra\n",
        "    velo_c[0] = 0.0\n",
        "\n",
        "    # ------------------- bucle recursivo -------------------------------\n",
        "    for i in range(1, n):\n",
        "        prev_kf = kf_c[i-1] if not np.isnan(kf_c[i-1]) else source.iloc[i]\n",
        "        dk      = source.iloc[i] - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term          # parte \"suave\"\n",
        "        velo_c[i] = velo_c[i-1] + length_term * dk   # acumulamos velocidad\n",
        "        kf_c[i]   = smooth_c + velo_c[i]             # estimación final\n",
        "\n",
        "    # -------------------- EMA final (ta.ema) ----------------------------\n",
        "    kf_c_series = pd.Series(kf_c, index=source.index)\n",
        "    kalman_line = kf_c_series.ewm(span=smooth, adjust=False).mean()\n",
        "    return kalman_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "HahbLzEkjoeB",
      "metadata": {
        "id": "HahbLzEkjoeB"
      },
      "outputs": [],
      "source": [
        "def slope(src: pd.Series,\n",
        "          length_kal: int,\n",
        "          smooth_kal: int,\n",
        "          slopeLen: int,\n",
        "          offset: int) -> pd.DataFrame:\n",
        "\n",
        "    n = len(src)\n",
        "    kf_state = np.full(n, np.nan)\n",
        "    kf_velo  = np.zeros(n)\n",
        "    sqrt_factor = np.sqrt(length_kal / 10000.0 * 2.0)\n",
        "    vel_factor  = length_kal / 10000.0\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == 0:\n",
        "            prev_state = src.iloc[0]\n",
        "            prev_velo  = 0.0\n",
        "        else:\n",
        "            prev_state = kf_state[i-1] if not np.isnan(kf_state[i-1]) else src.iloc[i]\n",
        "            prev_velo  = kf_velo[i-1]\n",
        "\n",
        "        dk = src.iloc[i] - prev_state\n",
        "        smooth = prev_state + dk * sqrt_factor\n",
        "        kf_velo[i]  = prev_velo + vel_factor * dk\n",
        "        kf_state[i] = smooth + kf_velo[i]\n",
        "\n",
        "    # 2) EMA smoothing --------------------------------------------------\n",
        "    kal = pd.Series(kf_state, index=src.index).ewm(span=smooth_kal, adjust=False).mean()\n",
        "\n",
        "    # 3) Slope/divergence -----------------------------------------------\n",
        "    validLen = max(slopeLen, 1)\n",
        "    slope_div = kal.diff(validLen) / validLen\n",
        "    slope_signal = (slope_div > slope_div.shift(1)).astype(int)\n",
        "\n",
        "    # 4) Angle in degrees -----------------------------------------------\n",
        "    price_change = kal - kal.shift(validLen)\n",
        "    slope_angle = np.degrees(np.arctan(price_change))\n",
        "    slope_angle_signal = (slope_angle > slope_angle.shift(1)).astype(int)\n",
        "\n",
        "    # 5) Linear regression prediction ----------------------------------\n",
        "    def _linreg(y):\n",
        "        x = np.arange(len(y))\n",
        "        m, b = np.polyfit(x, y, 1)\n",
        "        return b + m * (len(y)-1)\n",
        "\n",
        "    slope_lin_reg = kal.rolling(window=slopeLen).apply(_linreg, raw=False)\n",
        "    slope_lin_reg = slope_lin_reg.shift(-offset)  # apply Pine-style offset\n",
        "    slope_lin_reg_signal = (slope_lin_reg > slope_lin_reg.shift(1)).astype(int)\n",
        "\n",
        "    # 6) Pack results ---------------------------------------------------\n",
        "    return pd.DataFrame({\n",
        "        'slope_div':            slope_div,\n",
        "        'slope_signal':         slope_signal,\n",
        "        'slope_angle':          slope_angle,\n",
        "        'slope_angle_signal':   slope_angle_signal,\n",
        "        'slope_lin_reg':        slope_lin_reg,\n",
        "        'slope_lin_reg_signal': slope_lin_reg_signal\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fB8uDm2F1TS1",
      "metadata": {
        "id": "fB8uDm2F1TS1"
      },
      "outputs": [],
      "source": [
        "# Function for creating features\n",
        "def create_features(stock_data):\n",
        "\n",
        "    short_periods = [3, 5, 7, 10, 15, 17]\n",
        "    long_periods = [20, 22, 66, 126, 252]\n",
        "\n",
        "    # Combined list of lookbacks\n",
        "    periods = short_periods + long_periods\n",
        "\n",
        "    # Initialise an empty DataFrame to store the results\n",
        "    features = pd.DataFrame(index=stock_data.index)\n",
        "\n",
        "    # Calculate technical indicators for each specified period\n",
        "    for period in periods:\n",
        "        # Relative Strength Index (RSI)\n",
        "        features[f'RSI_{period}'] = ta.RSI(\n",
        "            stock_data['Close'], timeperiod=period)\n",
        "\n",
        "        # Money Flow Index (MFI)\n",
        "        features[f'MFI_{period}'] = ta.MFI(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], stock_data['Volume'], timeperiod=period)\n",
        "\n",
        "        # Average Directional Index (ADX)\n",
        "        features[f'ADX_{period}'] = ta.ADX(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], timeperiod=period)\n",
        "\n",
        "        # On-Balance Volume (OBV)\n",
        "        features[f'OBV_{period}'] = ta.OBV(\n",
        "            stock_data['Close'], stock_data['Volume'])\n",
        "\n",
        "        # Accumulation/Distribution Line (AD)\n",
        "        features[f'AD_{period}'] = ta.AD(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], stock_data['Volume'])\n",
        "\n",
        "        # Rate of Change (ROCP)\n",
        "        features[f'ROCP_{period}'] = ta.ROCP(\n",
        "            stock_data['Close'], timeperiod=period)\n",
        "\n",
        "    # Calculate Simple Moving Average and Exponential Moving Average Crossovers\n",
        "    for short_period in short_periods:\n",
        "        for long_period in long_periods:\n",
        "            # SMA Crossover\n",
        "            features[f'SMA_Crossover_{short_period}_{long_period}'] = ta.SMA(\n",
        "                stock_data['Close'], timeperiod=short_period) - ta.SMA(stock_data['Close'], timeperiod=long_period)\n",
        "\n",
        "            # EMA Crossover\n",
        "            features[f'EMA_Crossover_{short_period}_{long_period}'] = ta.EMA(\n",
        "                stock_data['Close'], timeperiod=short_period) - ta.EMA(stock_data['Close'], timeperiod=long_period)\n",
        "\n",
        "\n",
        "    features.dropna(inplace=True)\n",
        "    return features\n",
        "\n",
        "def scale_features_data(features):\n",
        "    # Standardise the input data (X)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Split the dataset into training and testing sets as 80:20\n",
        "    train_data = features.iloc[:(int(len(features) * 0.8))]\n",
        "    test_data = features.iloc[(int(len(features) * 0.8)):]\n",
        "\n",
        "    # Scale the training and testing sets\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(\n",
        "        train_data), columns=features.columns, index=train_data.index)\n",
        "    X_test = pd.DataFrame(data=scaler.transform(test_data),\n",
        "                          columns=features.columns, index=test_data.index)\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def strategy_returns_dynamic_different_thresholds(prices, threshold):\n",
        "    curr_pos = 0\n",
        "    hold_days = 0\n",
        "    for dt in prices.index:\n",
        "        if curr_pos==0 or hold_days==20:\n",
        "            if prices.loc[dt, 'Rolling Returns'] >= threshold*prices.loc[dt, 'Yearly Stdev']:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 1\n",
        "\n",
        "            elif prices.loc[dt, 'Rolling Returns'] < -threshold*prices.loc[dt, 'Yearly Stdev']:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = -1\n",
        "\n",
        "            else:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 0\n",
        "\n",
        "            curr_pos = prices.loc[dt, 'Signal_'+str(threshold)]\n",
        "            hold_days = 0\n",
        "\n",
        "        elif curr_pos!=0:\n",
        "            hold_days+=1\n",
        "\n",
        "    prices['Signal_'+str(threshold)].ffill(inplace=True)\n",
        "    prices['Strategy Returns_'+str(threshold)] = prices['Close'].pct_change() * prices['Signal_'+str(threshold)].shift(1)\n",
        "    cumulative_strategy_returns = (prices['Strategy Returns_'+str(threshold)] +1).cumprod()\n",
        "\n",
        "    return cumulative_strategy_returns\n",
        "\n",
        "def strategy_returns_different_thresholds(prices, threshold):\n",
        "    curr_pos = 0\n",
        "    hold_days = 0\n",
        "    for dt in prices.index:\n",
        "        if curr_pos==0 or hold_days==20:\n",
        "            if prices.loc[dt, 'Rolling Returns'] >= threshold:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 1\n",
        "            elif prices.loc[dt, 'Rolling Returns'] < threshold:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = -1\n",
        "\n",
        "            curr_pos = prices.loc[dt, 'Signal_'+str(threshold)]\n",
        "            hold_days = 0\n",
        "\n",
        "        elif curr_pos!=0:\n",
        "            hold_days+=1\n",
        "\n",
        "    prices['Signal_'+str(threshold)].ffill(inplace=True)\n",
        "\n",
        "    prices['Strategy Returns_'+str(threshold)] = prices['Close'].pct_change() * prices['Signal_'+str(threshold)].shift(1)\n",
        "\n",
        "    cumulative_strategy_returns = (prices['Strategy Returns_'+str(threshold)] +1).cumprod()\n",
        "    return cumulative_strategy_returns\n",
        "\n",
        "def generate_trade_sheet(data):\n",
        "    trade_list = []  # Use a list to store trade data\n",
        "    current_position = 0\n",
        "    entry_date = ''\n",
        "    entry_price = ''\n",
        "    exit_date = ''\n",
        "    exit_price = ''\n",
        "    data.reset_index(inplace=True)\n",
        "\n",
        "    for i in data.index:\n",
        "\n",
        "        if current_position == 0:\n",
        "            entry_date = data.loc[i, 'Date']\n",
        "            entry_price = data.loc[i, 'Close']\n",
        "            current_position = data.loc[i, 'signal']\n",
        "\n",
        "        elif np.abs(data.loc[i, 'signal'] - data.loc[i-1, 'signal']) != 0:\n",
        "            exit_date = data.loc[i, 'Date']\n",
        "            exit_price = data.loc[i, 'Close']\n",
        "            trade_list.append(\n",
        "                (current_position, entry_date, round(entry_price,2), exit_date, round(exit_price,2))) # Append to list\n",
        "            current_position = 0\n",
        "\n",
        "    trade_sheet = pd.DataFrame(trade_list, columns=['Position', 'Entry Date', # Convert list to DataFrame\n",
        "                           'Entry Price', 'Exit Date', 'Exit Price'])\n",
        "    trade_sheet['PnL'] = round((trade_sheet['Exit Price'] - trade_sheet['Entry Price']) * trade_sheet['Position'],2)\n",
        "    return trade_sheet\n",
        "\n",
        "\n",
        "def trade_analytics(trades):\n",
        "    analytics = pd.DataFrame(index=['Strategy'])\n",
        "    analytics['Total PnL'] = round(trades.PnL.sum(),2)\n",
        "    analytics['Total Trades'] = len(trades.loc[trades.Position!=0])\n",
        "    analytics['Number of Winners'] = len(trades.loc[trades.PnL>0])\n",
        "    analytics['Number of Losers'] = len(trades.loc[trades.PnL<=0])\n",
        "    analytics['Win (%)'] = round(100*analytics['Number of Winners']/analytics['Total Trades'],2)\n",
        "    analytics['Loss (%)'] = round(100*analytics['Number of Losers']/analytics['Total Trades'],2)\n",
        "    analytics['Average Profit of Winning Trade'] = round(trades.loc[trades.PnL>0].PnL.mean(),2)\n",
        "    analytics['Average Loss of Losing Trade'] = round(np.abs(trades.loc[trades.PnL<=0].PnL.mean()),2)\n",
        "    trades['Entry Date'] = pd.to_datetime(trades['Entry Date'])\n",
        "    trades['Exit Date'] = pd.to_datetime(trades['Exit Date'])\n",
        "    holding_period = trades['Exit Date'] - trades['Entry Date']\n",
        "    analytics['Average Holding Time'] = holding_period.mean()\n",
        "    analytics['Profit Factor'] = round((analytics['Win (%)']/100*analytics['Average Profit of Winning Trade'])/(analytics['Loss (%)']/100*analytics['Average Loss of Losing Trade']),2)\n",
        "    return analytics.T\n",
        "\n",
        "def performance_metrics(data):\n",
        "    data.set_index('Date', inplace=True)\n",
        "    performance_metrics = pd.DataFrame(index=['Strategy'])\n",
        "    data['Strategy Returns'] = data.signal.shift(1) * data.Close.pct_change()\n",
        "    data['Cumulative Returns'] = (data['Strategy Returns'] + 1.0).cumprod()\n",
        "    data['Cumulative Benchmark Returns'] = (data['Close'].pct_change() +1).cumprod()\n",
        "    data['Cumulative Returns'].plot(figsize=(15, 7), label='Strategy Returns')\n",
        "    data['Cumulative Benchmark Returns'].plot(label='Benchmark Returns')\n",
        "    plt.title('Equity Curve', fontsize=14)\n",
        "    plt.ylabel('Cumulative Returns', fontsize = 12)\n",
        "    plt.xlabel('Date', fontsize = 12)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    days = len(data['Cumulative Returns'])\n",
        "    performance_metrics['CAGR'] = \"{0:.2f}%\".format(\n",
        "        (data['Cumulative Returns'].iloc[-1]**(252/days)-1)*100)\n",
        "    performance_metrics['Annualised Volatility'] = \"{0:.2f}%\".format(\n",
        "        data['Strategy Returns'].std()*np.sqrt(252) * 100)\n",
        "    risk_free_rate = 0.02/252\n",
        "    performance_metrics['Sharpe Ratio'] = round(np.sqrt(252)*(np.mean(data['Strategy Returns']) -\n",
        "                                                        (risk_free_rate))/np.std(data['Strategy Returns']),2)\n",
        "    data['Peak'] = data['Cumulative Returns'].cummax()\n",
        "    data['Drawdown'] = ((data['Cumulative Returns'] - data['Peak'])/data['Peak'])\n",
        "    performance_metrics['Maximum Drawdown'] =  \"{0:.2f}%\".format((data['Drawdown'].min())*100)\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title('Drawdowns', fontsize=14)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.plot(data['Drawdown'], color='red')\n",
        "    plt.fill_between(data['Drawdown'].index, data['Drawdown'].values, color='red')\n",
        "    plt.show()\n",
        "    print(performance_metrics.T)\n",
        "\n",
        "# Function for creating target variable\n",
        "def target_var(data, window_size=20):\n",
        "    target = pd.DataFrame(index=data.index)\n",
        "    target['signal'] = data.Close.pct_change(window_size).shift(-window_size)\n",
        "\n",
        "    # Drop the NaN values\n",
        "    target.dropna(inplace=True)\n",
        "\n",
        "    # Convert the change into binary signals: 1 for positive change, -1 for negative change\n",
        "    target['signal'] = np.where(target['signal'] > 0, 1, -1)\n",
        "\n",
        "    return target\n",
        "\n",
        "# Function to split and scale the data\n",
        "def train_test_split(features, target, split_proportion=0.8):\n",
        "\n",
        "    split_index = int(len(features) * split_proportion)\n",
        "\n",
        "    # Split the features dataset into training and testing sets\n",
        "    X_train = features.iloc[:split_index]\n",
        "    X_test = features.iloc[split_index:]\n",
        "    y_train = target.iloc[:split_index]\n",
        "    y_test = target.iloc[split_index:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "\n",
        "def compile_encoder_decoder_model(X_train, optimizer='adam', loss='mean_squared_error'):\n",
        "    # Define the architecture of the autoencoder model\n",
        "    model = Sequential()  # Create a sequential model\n",
        "\n",
        "    # Add a dense layer with 64 neurons and ReLU activation function as the input layer\n",
        "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    # Add a dense layer with 32 neurons and ReLU activation function\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # Add a dense layer with 8 neurons and ReLU activation function\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "\n",
        "    # Add another dense layer with 32 neurons and ReLU activation function\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(X_train.shape[1], activation='linear'))\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def calculate_portfolio_returns_csmom(monthly_returns, portfolio='long-short', lookback_months=12):\n",
        "    stock_monthly_returns = pd.DataFrame()\n",
        "\n",
        "    # Loop through each month after the lookback period\n",
        "    for i in range(lookback_months, len(monthly_returns)):\n",
        "        # Select the subset of monthly returns for the current lookback period\n",
        "        # Store the historical and holding monthly returns data and store in 'returns'\n",
        "        returns = monthly_returns[i - lookback_months:i + 1]\n",
        "\n",
        "        # Store the historical in 'trailing_returns'\n",
        "        trailing_returns = returns[:lookback_months]\n",
        "\n",
        "        # Extract the starting, ending, and holding months from the subset\n",
        "        starting_month = str(returns.index[0])[:7]\n",
        "        ending_month = str(returns.index[-2])[:7]\n",
        "        holding_month = str(returns.index[-1])[:7]\n",
        "\n",
        "        # Set returns data as the transposed scaled trailing returns\n",
        "        returns_data = trailing_returns.T\n",
        "\n",
        "        # Initialize the number of clusters and maximum number of stocks per cluster\n",
        "        num_clusters = 1\n",
        "        max_stocks_per_cluster = 10\n",
        "\n",
        "        # Perform hierarchical clustering using 'ward' linkage method\n",
        "        linkage_matrix = linkage(trailing_returns.T, method='ward')\n",
        "\n",
        "        # Assign cluster labels to stocks, ensuring each cluster has at most 10 stocks\n",
        "        clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "        # Assign the cluster labels to the original returns data\n",
        "        returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Adjust clusters until each cluster meets the constraint\n",
        "        while max(returns_data['Cluster'].value_counts()) > max_stocks_per_cluster:\n",
        "            num_clusters += 1\n",
        "            clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "            returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Define the minimum number of stocks in a cluster\n",
        "        minimum_stocks_in_cluster = 2\n",
        "\n",
        "        # Filter out clusters with fewer than the minimum number of stocks\n",
        "        filtered_clusters = returns_data.groupby('Cluster').filter(\n",
        "            lambda x: len(x) >= minimum_stocks_in_cluster)['Cluster'].unique()\n",
        "\n",
        "        # Assign the filtered cluster labels to the original price data\n",
        "        returns_data = returns_data[returns_data['Cluster'].isin(filtered_clusters)]\n",
        "\n",
        "        # Calculate the returns for each cluster and sum across clusters\n",
        "        cluster_returns = returns_data.groupby('Cluster').mean().sum(axis=1)\n",
        "\n",
        "        if portfolio == 'long-short':\n",
        "            # Identify stocks to go short and long based on cluster returns\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long and short\n",
        "            long_returns = hold_returns[long].mean()\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long and short portfolios for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long) + list(short)][holding_month]\n",
        "\n",
        "            # Adjust returns for short positions\n",
        "            monthly_portfolio_returns[short] *= -1\n",
        "\n",
        "        elif portfolio == 'long':\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long\n",
        "            long_returns = hold_returns[long].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long)][holding_month]\n",
        "\n",
        "        elif portfolio == 'short':\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go short\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the short portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                short)][holding_month]\n",
        "\n",
        "        # Append adjusted returns for the holding month to the stock_monthly_returns dataframe\n",
        "        stock_monthly_returns = stock_monthly_returns.append(monthly_portfolio_returns)\n",
        "\n",
        "    return stock_monthly_returns\n",
        "\n",
        "def plot_and_display_metrics_csmom(stock_monthly_returns):\n",
        "    portfolio_returns = stock_monthly_returns.mean(axis=1)\n",
        "    fig, ax = plt.subplots(figsize=(15, 7))\n",
        "    portfolio_returns.plot(ax=ax)\n",
        "\n",
        "    # Set the title and axis labels\n",
        "    ax.set_title('Portfolio Returns Over Time')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Returns')\n",
        "    ax.axhline(y=0, color='black', linestyle='-')\n",
        "\n",
        "    # Fill area below 0 with red color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns < 0, color='red', alpha=0.3)\n",
        "\n",
        "    # Fill area above 0 with green color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns >= 0, color='green', alpha=0.3)\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Calculate cumulative returns\n",
        "    cumulative_returns = (portfolio_returns + 1).cumprod()\n",
        "\n",
        "    # Convert index to datetime format\n",
        "    cumulative_returns.index = pd.to_datetime(cumulative_returns.index)\n",
        "\n",
        "    # Plot cumulative returns\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    cumulative_returns.plot()\n",
        "\n",
        "    # Labeling axes and title\n",
        "    plt.ylabel('Cumulative Returns', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Returns', fontsize=14)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate maximum cumulative returns up to each point\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "\n",
        "    # Calculate drawdown\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "\n",
        "    # Plot drawdown\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    # Fill area under the drawdown curve with red color\n",
        "    plt.fill_between(drawdown.index, drawdown, 0, color='red', alpha=0.3)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Drawdown', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # Display the metrics\n",
        "    # Calculate monthly Sharpe ratio\n",
        "    monthly_sharpe = portfolio_returns.mean() / portfolio_returns.std()\n",
        "\n",
        "    # Annualize Sharpe ratio for monthly data\n",
        "    sharpe = round(monthly_sharpe * ((12) ** 0.5),2)\n",
        "\n",
        "    # Calculate drawdown\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "    max_drawdown_index = drawdown.idxmin()\n",
        "    max_drawdown_date = max_drawdown_index.strftime('%Y-%m-%d')\n",
        "    max_drawdown_value = round(drawdown.min(),2)\n",
        "\n",
        "    # Create a DataFrame to hold the metrics\n",
        "    metrics = pd.DataFrame({\n",
        "        'Metric': ['Sharpe Ratio', 'Maximum Drawdown Date', 'Maximum Drawdown Value'],\n",
        "        'Value': [sharpe, max_drawdown_date, max_drawdown_value]\n",
        "    })\n",
        "\n",
        "    # Display metrics\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    display(metrics.rename_axis(None, axis=1))\n",
        "\n",
        "def predict_signals(X_test, aapl_test_prices_ts, model, scaler):\n",
        "    # Initialise current position\n",
        "    current_pos = 0\n",
        "\n",
        "    # Initialise count of holding days\n",
        "    hold_days = 0\n",
        "\n",
        "    # Iterate through the rows of test data\n",
        "    for dt, row in X_test.iterrows():\n",
        "        # Check if there is no position or holding period reaches 20 days\n",
        "        if current_pos == 0 or hold_days == 20:\n",
        "            # Prepare test data for prediction\n",
        "            test = pd.DataFrame(data=scaler.transform(\n",
        "                row.values.reshape(1, -1)), columns=X_test.columns)\n",
        "\n",
        "            # Generate signal based on test data\n",
        "            signal = model.predict(test)[-1]\n",
        "\n",
        "            # Update current position\n",
        "            current_pos = signal\n",
        "\n",
        "            # Update predicted and actual labels for the current date\n",
        "            aapl_test_prices_ts.loc[dt, 'signal'] = current_pos\n",
        "\n",
        "            # Reset holding days counter\n",
        "            hold_days = 0\n",
        "        elif current_pos != 0:\n",
        "            # If there is an existing position, increment holding days counter\n",
        "            hold_days += 1\n",
        "\n",
        "    # Forward fill the last observed value for 'y_pred'\n",
        "    aapl_test_prices_ts['signal'].ffill(inplace=True)\n",
        "\n",
        "    return aapl_test_prices_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Dunlv-TDqxB8",
      "metadata": {
        "id": "Dunlv-TDqxB8"
      },
      "outputs": [],
      "source": [
        "def get_parkinson(price_data, window=10, trading_periods=50, clean=True):\n",
        "    rs = (1.0 / (4.0 * math.log(2.0))) * ((price_data['High'] / price_data['Low']).apply(np.log))**2.0\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "RLxsuLKaqxE3",
      "metadata": {
        "id": "RLxsuLKaqxE3"
      },
      "outputs": [],
      "source": [
        "def get_HodgesTompkins(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    vol = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).std() * math.sqrt(trading_periods)\n",
        "\n",
        "    h = window\n",
        "    n = (log_return.count() - h) + 1\n",
        "\n",
        "    adj_factor = 1.0 / (1.0 - (h / n) + ((h**2 - 1) / (3 * n**2)))\n",
        "\n",
        "    result = vol * adj_factor\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "qFCu8J7hqxHi",
      "metadata": {
        "id": "qFCu8J7hqxHi"
      },
      "outputs": [],
      "source": [
        "def get_skew(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).skew()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "YnVDjmtDqxKU",
      "metadata": {
        "id": "YnVDjmtDqxKU"
      },
      "outputs": [],
      "source": [
        "def get_kurtosis(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).kurt()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "WJvLEk7EqxM6",
      "metadata": {
        "id": "WJvLEk7EqxM6"
      },
      "outputs": [],
      "source": [
        "def get_YangZhang(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High'] / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low'] / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    log_oc = (price_data['Open'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_oc_sq = log_oc**2\n",
        "\n",
        "    log_cc = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_cc_sq = log_cc**2\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    close_vol = log_cc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    open_vol = log_oc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    window_rs = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "\n",
        "    k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
        "    result = (open_vol + k * close_vol + (1 - k) * window_rs).apply(np.sqrt) * math.sqrt(trading_periods)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "OHmFUyfGqxPL",
      "metadata": {
        "id": "OHmFUyfGqxPL"
      },
      "outputs": [],
      "source": [
        "def get_RogersSatchell(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High']  / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low']   / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cSq3oNjkrdhT",
      "metadata": {
        "id": "cSq3oNjkrdhT"
      },
      "outputs": [],
      "source": [
        "def get_GermanKlass(price_data, window=22, trading_periods=50, clean=True):\n",
        "\n",
        "    log_hl = (price_data['High'] / price_data['Low']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = 0.5 * log_hl**2 - (2*math.log(2)-1) * log_co**2\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(window=window, center=False).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I23127P7lBq_",
      "metadata": {
        "id": "I23127P7lBq_"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "GC3ODp84C1SH",
      "metadata": {
        "id": "GC3ODp84C1SH"
      },
      "outputs": [],
      "source": [
        "def create_features(stock_data: pd.DataFrame) -> pd.DataFrame:\n",
        "    short_periods  = [3, 5, 7, 10, 15, 17]\n",
        "    long_periods   = [20, 22, 66, 126, 252]\n",
        "    kalman_periods = [100, 300, 500, 700, 900]\n",
        "    slope_length   = [3, 6, 9, 12, 15, 18, 21]\n",
        "\n",
        "    periods = short_periods + long_periods\n",
        "    features = pd.DataFrame(index=stock_data.index)\n",
        "\n",
        "    # Import tqdm for progress bar\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    # ── Indicadores por período ─────────────────────────────────────────\n",
        "    for period in tqdm(periods, desc=\"Calculating Period Indicators\"):\n",
        "        features[f'RSI_{period}'] = ta.RSI(stock_data['Close'], timeperiod=period)\n",
        "        features[f'MFI_{period}'] = ta.MFI(stock_data['High'], stock_data['Low'], stock_data['Close'],\n",
        "                                           stock_data['Volume'], timeperiod=period)\n",
        "        features[f'ADX_{period}'] = ta.ADX(stock_data['High'], stock_data['Low'], stock_data['Close'],\n",
        "                                           timeperiod=period)\n",
        "        # Nota: OBV y AD no usan window; conservamos el sufijo para mantener tu esquema\n",
        "        features[f'OBV_{period}'] = ta.OBV(stock_data['Close'], stock_data['Volume'])\n",
        "        features[f'AD_{period}']  = ta.AD(stock_data['High'], stock_data['Low'],\n",
        "                                          stock_data['Close'], stock_data['Volume'])\n",
        "        features[f'ROCP_{period}'] = ta.ROCP(stock_data['Close'], timeperiod=period)\n",
        "\n",
        "    # ── Series base (una sola vez) ─────────────────────────────────────\n",
        "    log_ret = np.log(stock_data['Close']).diff()\n",
        "    features['log_ret']       = log_ret\n",
        "    features['log_ret_dif1']  = np.log(stock_data['Close']).diff(1)\n",
        "    features['vola_10']       = log_ret.rolling(window=10, min_periods=10).std()\n",
        "    features['autocorr_1']    = log_ret.rolling(window=20, min_periods=20)\\\n",
        "                                       .apply(lambda x: x.autocorr(lag=1), raw=False)\n",
        "    features['log_t1']        = log_ret.shift(1)\n",
        "\n",
        "    # ── Estimadores de volatilidad (se escriben en `features`) ─────────\n",
        "    for i in tqdm(range(4, 20, 4), desc=\"Calculating Volatility Estimators\"):\n",
        "        features[f'parkinson_{i}']       = get_parkinson(stock_data,       window=i, trading_periods=50, clean=True)\n",
        "        features[f'GermanKlass_{i}']     = get_GermanKlass(stock_data,     window=i, trading_periods=50, clean=True)\n",
        "        features[f'RogersSatchell_{i}']  = get_RogersSatchell(stock_data,  window=i, trading_periods=50, clean=True)\n",
        "        features[f'YangZhang_{i}']       = get_YangZhang(stock_data,       window=i, trading_periods=50, clean=True)\n",
        "        features[f'HodgesTompkins_{i}']  = get_HodgesTompkins(stock_data,  window=i, trading_periods=50, clean=True)\n",
        "        features[f'kurtosis_{i}']        = get_kurtosis(stock_data,        window=i, clean=True)\n",
        "        features[f'skew_{i}']            = get_skew(stock_data,            window=i, clean=True)\n",
        "\n",
        "    # ── Cruces SMA / EMA ───────────────────────────────────────────────\n",
        "    for short_period in tqdm(short_periods, desc=\"Calculating SMA/EMA Crossovers\"):\n",
        "        for long_period in long_periods:\n",
        "            features[f'SMA_Crossover_{short_period}_{long_period}'] = (\n",
        "                ta.SMA(stock_data['Close'], timeperiod=short_period)\n",
        "                - ta.SMA(stock_data['Close'], timeperiod=long_period)\n",
        "            )\n",
        "            features[f'EMA_Crossover_{short_period}_{long_period}'] = (\n",
        "                ta.EMA(stock_data['Close'], timeperiod=short_period)\n",
        "                - ta.EMA(stock_data['Close'], timeperiod=long_period)\n",
        "            )\n",
        "\n",
        "    # ── Kalman y derivados ─────────────────────────────────────────────\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Kalman and Derivatives\"):\n",
        "        kal = pd.Series(kalman_line(stock_data['Close'], kalman_length=period, smooth=3),\n",
        "                        index=stock_data.index)\n",
        "        features[f'Kal_{period}']              = kal\n",
        "        features[f'Close_Kal_{period}_3']      = stock_data['Close'] - kal\n",
        "        features[f'Kal_change_{period}_3']     = kal.diff()\n",
        "\n",
        "    # ── Slopes ─────────────────────────────────────────────────────────\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Slopes\"):\n",
        "        for sLen in slope_length:\n",
        "            df_s = slope(stock_data['Close'], length_kal=period, smooth_kal=3,\n",
        "                         slopeLen=sLen, offset=-1)\n",
        "            features[f'slope_div_{period}_{sLen}']             = df_s['slope_div']\n",
        "            features[f'slope_signal_{period}_{sLen}']          = df_s['slope_signal']\n",
        "            features[f'slope_angle_{period}_{sLen}']           = df_s['slope_angle']\n",
        "            features[f'slope_angle_signal_{period}_{sLen}']    = df_s['slope_angle_signal']\n",
        "            features[f'slope_lin_reg_{period}_{sLen}']         = df_s['slope_lin_reg']\n",
        "            features[f'slope_lin_reg_signal_{period}_{sLen}']  = df_s['slope_lin_reg_signal']\n",
        "\n",
        "    features.dropna(inplace=True)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FDIu30P4Bmg-",
      "metadata": {
        "id": "FDIu30P4Bmg-"
      },
      "source": [
        "# Support Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "sKKJrsFBfOoT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKKJrsFBfOoT",
        "outputId": "5c44c38b-5d47-48c2-9af6-baf3c9db47f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Long model loaded from:  /content/drive/MyDrive/Course Folder/Forex/XAUUSD/Models/BTCUSD_Long_ml_model.joblib\n",
            "✅ Short model loaded from: /content/drive/MyDrive/Course Folder/Forex/XAUUSD/Models/BTCUSD_Short_ml_model.joblib\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make sure SYMBOL exists (your code elsewhere uses SYMBOL, not `symbol`)\n",
        "try:\n",
        "    SYMBOL\n",
        "except NameError:\n",
        "    SYMBOL = \"BTCUSD\"   # fallback; change if needed\n",
        "\n",
        "# Base folder where your models live\n",
        "root_data = \"/content/drive/MyDrive/Course Folder/Forex/XAUUSD/\"\n",
        "models_dir = Path(root_data) / \"Models\"\n",
        "\n",
        "# Build full paths (use straight quotes and SYMBOL)\n",
        "long_model_path  = models_dir / f\"{SYMBOL}_Long_ml_model.joblib\"\n",
        "short_model_path = models_dir / f\"{SYMBOL}_Short_ml_model.joblib\"\n",
        "\n",
        "# Helper to load and validate\n",
        "def _load_joblib_model(path: Path):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Model file not found: {path}\")\n",
        "    model = joblib.load(path)\n",
        "    return model\n",
        "\n",
        "# Load globals used by main()\n",
        "long_ml_model  = _load_joblib_model(long_model_path)\n",
        "short_ml_model = _load_joblib_model(short_model_path)\n",
        "\n",
        "print(f\"✅ Long model loaded from:  {long_model_path}\")\n",
        "print(f\"✅ Short model loaded from: {short_model_path}\")\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "EXPECTED_CLASSES = [0, 1, 2]\n",
        "\n",
        "def _ensure_prob_cols_present(df: pd.DataFrame, side: str) -> None:\n",
        "    \"\"\"Guarantee prob_0/1/2_{side} exist so they are always saved to CSV.\"\"\"\n",
        "    for k in EXPECTED_CLASSES:\n",
        "        col = f\"prob_{k}_{side}\"\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "def _reorder_ml(cols: list[str]) -> list[str]:\n",
        "    cols = cols.copy()\n",
        "    # unified first\n",
        "    uni = [\"label_ml\", \"prob_0\", \"prob_1\", \"prob_2\"]\n",
        "    for c in uni:\n",
        "        if c in cols:\n",
        "            cols.remove(c)\n",
        "    insert_at = cols.index(\"Type\") + 1 if \"Type\" in cols else len(cols)\n",
        "    for c in uni:\n",
        "        if c in df_out.columns:\n",
        "            cols.insert(insert_at, c)\n",
        "            insert_at += 1\n",
        "\n",
        "    # then side-specific (if you keep them)\n",
        "    ml_block = [\n",
        "        \"label_ml_long\", \"prob_0_long\", \"prob_1_long\", \"prob_2_long\",\n",
        "        \"label_ml_short\",\"prob_0_short\",\"prob_1_short\",\"prob_2_short\"\n",
        "    ]\n",
        "    for c in ml_block:\n",
        "        if c in cols:\n",
        "            cols.remove(c)\n",
        "    for c in ml_block:\n",
        "        if c in df_out.columns:\n",
        "            cols.insert(insert_at, c)\n",
        "            insert_at += 1\n",
        "    return cols\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────\n",
        "# FIRST-RUN INITIALIZER\n",
        "# ────────────────────────────────────────────────────────────────────\n",
        "async def initialize_csv_first_run(\n",
        "    account,\n",
        "    *,\n",
        "    bars: int = 900,\n",
        "    save_mode: str = \"all\",  # \"all\" = save full 900 rows; \"last\" = save only last row\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fetch `bars` candles, compute ATR + signals (+ ML if a trigger exists),\n",
        "    and write either the full dataset or just the last row to the CSV.\n",
        "\n",
        "    save_mode:\n",
        "      - \"all\"  → write all rows (what you asked for)\n",
        "      - \"last\" → compute with 900 but persist only the last bar\n",
        "    \"\"\"\n",
        "    # 1) Download history\n",
        "    df_hist = await get_candles_5m(account, start=None, limit=bars)\n",
        "    df_hist = (df_hist\n",
        "               .drop_duplicates(\"time\")\n",
        "               .sort_values(\"time\")\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "    # 2) ATR on all rows\n",
        "    if len(df_hist) >= 14:\n",
        "        df_hist[\"ATR\"] = ta.ATR(df_hist[\"high\"], df_hist[\"low\"], df_hist[\"close\"], 14).round(4)\n",
        "\n",
        "    # 3) Signals (and ML only if an entry trigger exists)\n",
        "    df_all = df_hist.copy()\n",
        "    generate_trade_signals(\n",
        "        df_all, length_1, length_2, length_3, length_4,\n",
        "        smooth_1, smooth_2, smooth_3, smooth_4\n",
        "    )\n",
        "    maybe_compute_ml_for_entry(\n",
        "        df_all,\n",
        "        long_model=long_ml_model,\n",
        "        short_model=short_ml_model,\n",
        "        feature_cols_long=FEATURES_LONG,\n",
        "        feature_cols_short=FEATURES_SHORT\n",
        "    )\n",
        "    sync_unified_ml_cols(df_all)\n",
        "    _ensure_order_cols(df_all)\n",
        "\n",
        "    # 4) Persist\n",
        "    if save_mode == \"all\":\n",
        "        df_all[\"source\"] = 1\n",
        "        stamp_system_time(df_all, mode=\"missing\")   # put System_time on every row\n",
        "        save_csv(df_all)\n",
        "        print(f\"✔ Archivo inicial creado: {len(df_all)} filas guardadas (se analizaron {bars} velas)\")\n",
        "    else:\n",
        "        last = df_all.tail(1).copy()\n",
        "        last[\"source\"] = 1\n",
        "        stamp_system_time(last, mode=\"last\")\n",
        "        save_csv(last)\n",
        "        print(f\"✔ Archivo inicial creado: 1 fila guardada (se analizaron {bars} velas)\")\n",
        "\n",
        "\n",
        "\n",
        "def has_new_entry_signal(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    True only if the *last* bar is an entry trigger: Open_Trade ∈ {+1, -1}.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return False\n",
        "    s = df[\"Open_Trade\"].iloc[-1]\n",
        "    if pd.isna(s):\n",
        "        return False\n",
        "    return s in (1, -1)\n",
        "\n",
        "def _ensure_unified_ml_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Make sure the 4 generic ML columns exist.\"\"\"\n",
        "    if \"label_ml\" not in df.columns:\n",
        "        df[\"label_ml\"] = np.nan\n",
        "    for k in (0, 1, 2):\n",
        "        c = f\"prob_{k}\"\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "\n",
        "\n",
        "def sync_unified_ml_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Populate the 4 generic columns from side-specific ones:\n",
        "      • If Open_Trade == +1  → copy from *_long\n",
        "      • If Open_Trade == -1  → copy from *_short\n",
        "      • Otherwise leave NaNs (no trade)\n",
        "    Works for all rows that have side predictions.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    _ensure_unified_ml_cols(df)\n",
        "\n",
        "    # Prefer rows with explicit Open_Trade signal\n",
        "    if \"Open_Trade\" in df.columns:\n",
        "        # LONG rows\n",
        "        mL = df[\"Open_Trade\"] == 1\n",
        "        if \"label_ml_long\" in df.columns:\n",
        "            df.loc[mL, \"label_ml\"] = df.loc[mL, \"label_ml_long\"]\n",
        "        for k in (0, 1, 2):\n",
        "            src = f\"prob_{k}_long\"\n",
        "            if src in df.columns:\n",
        "                df.loc[mL, f\"prob_{k}\"] = df.loc[mL, src]\n",
        "\n",
        "        # SHORT rows\n",
        "        mS = df[\"Open_Trade\"] == -1\n",
        "        if \"label_ml_short\" in df.columns:\n",
        "            df.loc[mS, \"label_ml\"] = df.loc[mS, \"label_ml_short\"]\n",
        "        for k in (0, 1, 2):\n",
        "            src = f\"prob_{k}_short\"\n",
        "            if src in df.columns:\n",
        "                df.loc[mS, f\"prob_{k}\"] = df.loc[mS, src]\n",
        "\n",
        "    # Fallback: if some rows have predictions but Open_Trade is NaN, copy whichever exists\n",
        "    if \"label_ml\" in df.columns:\n",
        "        if \"label_ml_long\" in df.columns:\n",
        "            mask = df[\"label_ml\"].isna() & df[\"label_ml_long\"].notna()\n",
        "            df.loc[mask, \"label_ml\"] = df.loc[mask, \"label_ml_long\"]\n",
        "            for k in (0, 1, 2):\n",
        "                src = f\"prob_{k}_long\"\n",
        "                if src in df.columns:\n",
        "                    df.loc[mask, f\"prob_{k}\"] = df.loc[mask, src]\n",
        "\n",
        "        if \"label_ml_short\" in df.columns:\n",
        "            mask = df[\"label_ml\"].isna() & df[\"label_ml_short\"].notna()\n",
        "            df.loc[mask, \"label_ml\"] = df.loc[mask, \"label_ml_short\"]\n",
        "            for k in (0, 1, 2):\n",
        "                src = f\"prob_{k}_short\"\n",
        "                if src in df.columns:\n",
        "                    df.loc[mask, f\"prob_{k}\"] = df.loc[mask, src]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def maybe_compute_ml_for_entry(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    long_model,\n",
        "    short_model,\n",
        "    feature_cols_long: list[str] | None = None,\n",
        "    feature_cols_short: list[str] | None = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs the heavy feature/ML block **only** when the last bar has an entry signal.\n",
        "    Otherwise, it NOOPs (and ensures ML columns exist so downstream code never breaks).\n",
        "    \"\"\"\n",
        "    # No entry trigger on the last bar → do nothing (but create empty ML cols)\n",
        "    if not has_new_entry_signal(df):\n",
        "        for side in (\"long\", \"short\"):\n",
        "            lab = f\"label_ml_{side}\"\n",
        "            if lab not in df.columns:\n",
        "                df[lab] = np.nan\n",
        "            for k in (0, 1, 2):\n",
        "                col = f\"prob_{k}_{side}\"\n",
        "                if col not in df.columns:\n",
        "                    df[col] = np.nan\n",
        "        return df\n",
        "\n",
        "    # Entry trigger right now → run the real ML block (this computes features internally)\n",
        "    return run_side_models_inplace(\n",
        "        df,\n",
        "        long_model=long_model,\n",
        "        short_model=short_model,\n",
        "        feature_cols_long=feature_cols_long,\n",
        "        feature_cols_short=feature_cols_short\n",
        "    )\n",
        "\n",
        "def prediction_table_action(trade_side: int, pred: int):\n",
        "    \"\"\"\n",
        "    Implements your table:\n",
        "      trade=+1 (long): pred 0→None, 1→(tp1,sl1), 2→(tp2,sl2)\n",
        "      trade=-1 (short): pred 0→None, 1→(tp1,sl1), 2→(tp2,sl2)\n",
        "    Returns (\"none\"|\"long\"|\"short\", tp_mult, sl_mult).\n",
        "    \"\"\"\n",
        "    if pred == 0 or not np.isfinite(pred):\n",
        "        return (\"none\", None, None)\n",
        "    if trade_side == 1:\n",
        "        return (\"long\",  \"takeprofit_1\" if pred == 1 else \"takeprofit_2\",\n",
        "                        \"stoploss_1\"   if pred == 1 else \"stoploss_2\")\n",
        "    if trade_side == -1:\n",
        "        return (\"short\", \"takeprofit_1\" if pred == 1 else \"takeprofit_2\",\n",
        "                        \"stoploss_1\"   if pred == 1 else \"stoploss_2\")\n",
        "    return (\"none\", None, None)\n",
        "\n",
        "\n",
        "\n",
        "def has_new_entry_signal(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"True only if the *last* bar is an entry trigger: Open_Trade ∈ {+1, -1}.\"\"\"\n",
        "    if df is None or df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return False\n",
        "    s = df[\"Open_Trade\"].iloc[-1]\n",
        "    return np.isfinite(s) and (s == 1 or s == -1)\n",
        "\n",
        "def _encoded_span_from(names: list[str]) -> int:\n",
        "    \"\"\"Return N if Encoded_0..Encoded_{N-1} appear in names; else 0.\"\"\"\n",
        "    if not names:\n",
        "        return 0\n",
        "    idxs = []\n",
        "    for n in names:\n",
        "        m = re.match(r\"^Encoded_(\\d+)$\", str(n))\n",
        "        if m:\n",
        "            idxs.append(int(m.group(1)))\n",
        "    return (max(idxs) + 1) if idxs else 0\n",
        "\n",
        "\n",
        "# --- add this helper (near your other small utils) -------------------\n",
        "def _preferred_feature_order(side: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return the exact column order to encode based on your files.\n",
        "    side: 'long' or 'short'\n",
        "    \"\"\"\n",
        "    if side == \"long\" and FEATURES_LONG:\n",
        "        return FEATURES_LONG\n",
        "    if side == \"short\" and FEATURES_SHORT:\n",
        "        return FEATURES_SHORT\n",
        "    return []\n",
        "\n",
        "\n",
        "def _ensure_10m_prefix_aliases(X: pd.DataFrame, required_cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If your CSV expects '10_min_10_min_' but you produced '10min_10min_' (or vice-versa),\n",
        "    make alias columns so required names exist without recomputing features.\n",
        "    \"\"\"\n",
        "    if not required_cols:\n",
        "        return X\n",
        "\n",
        "    pref_a = \"10min_10min_\"\n",
        "    pref_b = \"10_min_10_min_\"\n",
        "\n",
        "    have = set(X.columns)\n",
        "    for col in required_cols:\n",
        "        if not isinstance(col, str):\n",
        "            continue\n",
        "\n",
        "        # CSV wants B, we have A → create B from A\n",
        "        if col.startswith(pref_b) and col not in have:\n",
        "            alt = pref_a + col[len(pref_b):]\n",
        "            if alt in have:\n",
        "                X[col] = X[alt]\n",
        "\n",
        "        # CSV wants A, we have B → create A from B\n",
        "        if col.startswith(pref_a) and col not in have:\n",
        "            alt = pref_b + col[len(pref_a):]\n",
        "            if alt in have:\n",
        "                X[col] = X[alt]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def compute_10m_features_from_5m(df5: pd.DataFrame,\n",
        "                                 *,\n",
        "                                 prefix: str = \"10min_10min_\",\n",
        "                                 pre_scale: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    From a 5-minute OHLCV df (columns: time, open, high, low, close, volume, ...),\n",
        "    resample to 10m, compute indicators via `create_features`, optionally scale them,\n",
        "    prefix with `prefix`, then align back to the 5m time index (as-of, ffill).\n",
        "\n",
        "    Returns a dataframe indexed by the 5m timestamps with only the prefixed columns.\n",
        "    \"\"\"\n",
        "    if \"time\" in df5.columns:\n",
        "        idx5 = pd.to_datetime(df5[\"time\"], utc=True)\n",
        "        base = df5.set_index(idx5)\n",
        "    else:\n",
        "        base = df5.copy()\n",
        "        base.index = pd.to_datetime(base.index, utc=True)\n",
        "        idx5 = base.index\n",
        "\n",
        "    # robust lower/upper column map for resample\n",
        "    cols = {c.lower(): c for c in base.columns}\n",
        "    # minimal OHLCV present?\n",
        "    for need in (\"open\", \"high\", \"low\", \"close\"):\n",
        "        if need not in cols:\n",
        "            raise ValueError(f\"compute_10m_features_from_5m: missing column '{need}'\")\n",
        "\n",
        "    agg = {\n",
        "        cols.get(\"open\", \"open\"):  \"first\",\n",
        "        cols.get(\"high\", \"high\"):  \"max\",\n",
        "        cols.get(\"low\", \"low\"):    \"min\",\n",
        "        cols.get(\"close\", \"close\"): \"last\",\n",
        "    }\n",
        "    # volume-like fields are optional\n",
        "    if \"volume\" in cols:\n",
        "        agg[cols[\"volume\"]] = \"sum\"\n",
        "    if \"tickvolume\" in cols:\n",
        "        agg[cols[\"tickvolume\"]] = \"sum\"\n",
        "    if \"spread\" in cols:\n",
        "        agg[cols[\"spread\"]] = \"mean\"\n",
        "\n",
        "    ohlc10 = (base\n",
        "              .resample(\"10T\", label=\"right\", closed=\"right\")\n",
        "              .agg(agg)\n",
        "              .dropna(subset=[cols[\"open\"], cols[\"high\"], cols[\"low\"], cols[\"close\"]]))\n",
        "\n",
        "    # TA-Lib expects Title-case OHLCV names\n",
        "    block = ohlc10.rename(columns={\n",
        "        cols.get(\"open\", \"open\"):  \"Open\",\n",
        "        cols.get(\"high\", \"high\"):  \"High\",\n",
        "        cols.get(\"low\", \"low\"):    \"Low\",\n",
        "        cols.get(\"close\", \"close\"): \"Close\",\n",
        "        cols.get(\"volume\", \"volume\"): \"Volume\",\n",
        "    })\n",
        "\n",
        "    f10 = create_features(block)  # <- your function above\n",
        "\n",
        "    if pre_scale:\n",
        "        sc = StandardScaler()\n",
        "        f10 = pd.DataFrame(sc.fit_transform(f10), index=f10.index, columns=f10.columns)\n",
        "\n",
        "    # prefix\n",
        "    f10.columns = [f\"{prefix}{c}\" for c in f10.columns]\n",
        "\n",
        "    # align back to the 5m timestamps (as-of / forward fill)\n",
        "    aligned = f10.reindex(idx5.union(f10.index)).ffill().reindex(idx5)\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def append_10m_features_inplace(\n",
        "    df_5m: pd.DataFrame,\n",
        "    *,\n",
        "    prefix: str = \"10min_10min_\",\n",
        "    pre_scale: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Builds a 10-minute OHLCV from your 5m data, computes your `create_features`,\n",
        "    optionally scales them, and appends to df_5m with the given prefix.\n",
        "\n",
        "    Requirements:\n",
        "      - df_5m has at least ['time','open','high','low','close','volume'].\n",
        "      - `create_features(stock_data)` is defined and returns a DataFrame indexed by time.\n",
        "    \"\"\"\n",
        "    if df_5m is None or df_5m.empty or \"time\" not in df_5m.columns:\n",
        "        return\n",
        "\n",
        "    # Ensure we won't create a column literally named 0 (from accidental index resets)\n",
        "    df5 = df_5m.copy()\n",
        "    t = pd.to_datetime(df5[\"time\"], utc=True, errors=\"coerce\")\n",
        "    base = df5.set_index(t).sort_index()\n",
        "\n",
        "    # Build 10m OHLCV\n",
        "    agg = {\n",
        "        \"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\",\n",
        "        \"volume\": \"sum\"\n",
        "    }\n",
        "    # optional columns if present\n",
        "    if \"tickVolume\" in base.columns: agg[\"tickVolume\"] = \"sum\"\n",
        "    if \"spread\"     in base.columns: agg[\"spread\"]     = \"mean\"\n",
        "\n",
        "    ohlc10 = base.resample(\"10T\").agg(agg).dropna(subset=[\"open\",\"close\"])\n",
        "\n",
        "    # Map to Title-case columns for your create_features()\n",
        "    stock10 = ohlc10.copy()\n",
        "    stock10[\"Open\"]   = stock10[\"open\"]\n",
        "    stock10[\"High\"]   = stock10[\"high\"]\n",
        "    stock10[\"Low\"]    = stock10[\"low\"]\n",
        "    stock10[\"Close\"]  = stock10[\"close\"]\n",
        "    stock10[\"Volume\"] = stock10[\"volume\"]\n",
        "\n",
        "    # Compute indicators with your existing function\n",
        "    try:\n",
        "        feats10 = create_features(stock10[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]])\n",
        "    except Exception as e:\n",
        "        # If create_features fails, do nothing (but do not crash live loop)\n",
        "        print(f\"⚠️ 10m feature block skipped (create_features failed): {e}\")\n",
        "        return\n",
        "\n",
        "    # Scale the 10m features if requested\n",
        "    if pre_scale and not feats10.empty:\n",
        "        scaler_10 = StandardScaler()\n",
        "        feats10 = pd.DataFrame(\n",
        "            scaler_10.fit_transform(feats10),\n",
        "            index=feats10.index, columns=feats10.columns\n",
        "        )\n",
        "\n",
        "    # Bring the 10m features to 5m timestamps via asof/ffill\n",
        "    feats10 = feats10.sort_index()\n",
        "    # Align index (5m time index)\n",
        "    idx5 = base.index\n",
        "    feats10_5m = pd.merge_asof(\n",
        "        pd.DataFrame(index=idx5).sort_index(),\n",
        "        feats10.sort_index(),\n",
        "        left_index=True, right_index=True, direction=\"backward\"\n",
        "    )\n",
        "\n",
        "    # Prefix and append to original df_5m (in place)\n",
        "    feats10_5m.columns = [prefix + str(c) for c in feats10_5m.columns]\n",
        "    # Write back by 'time' (align on row order using the same index)\n",
        "    for col in feats10_5m.columns:\n",
        "        df_5m[col] = feats10_5m[col].values\n",
        "\n",
        "\n",
        "# === Utilidades de columnas / guardado ===\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la última fila\n",
        "    mode=\"missing\": rellena donde esté NaT/NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:  # \"missing\"\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Crea columnas con dtypes correctos; quita 'id' y 'actionType'. Incluye 'source', 'base_px', 'atr_base' y 'Real_SL'.\"\"\"\n",
        "    # Elimina columnas heredadas\n",
        "    for col in (\"id\", \"actionType\"):\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    col_types = {\n",
        "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
        "        \"orderId\":       \"string\",\n",
        "        \"magic\":         \"Int64\",\n",
        "        \"symbol\":        \"string\",\n",
        "        \"openPrice\":     \"float64\",\n",
        "        \"comment\":       \"string\",\n",
        "        \"Type\":          \"string\",               # ← única columna de dirección\n",
        "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
        "        \"Stop_Loss_atr\": \"float64\",\n",
        "        \"Stop_Loss_$\":   \"float64\",\n",
        "        \"Real_SL\":       \"float64\",              # ← NUEVO\n",
        "        \"ATR\":           \"float64\",\n",
        "        \"atr_mult_high\": \"float64\",\n",
        "        \"atr_mult_low\":  \"float64\",\n",
        "        \"trade_size\":    \"float64\",\n",
        "        \"profits\":       \"float64\",\n",
        "        \"base_px\":       \"float64\",\n",
        "        \"atr_base\":      \"float64\",\n",
        "        \"source\":        \"Int64\",\n",
        "    }\n",
        "\n",
        "    for c, dtp in col_types.items():\n",
        "        if c not in df.columns:\n",
        "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.NaT\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
        "            elif dtp == \"string\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
        "            else:\n",
        "                df[c] = np.nan\n",
        "\n",
        "    # normaliza sin romper datos existentes\n",
        "    for c, dtp in col_types.items():\n",
        "        try:\n",
        "            if dtp == \"string\": df[c] = df[c].astype(\"string\")\n",
        "            if dtp == \"Int64\":  df[c] = df[c].astype(\"Int64\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Reordena y guarda el CSV con formato estable:\n",
        "      • System_time antes de 'time'\n",
        "      • 'source' justo a la derecha de 'time' y antes de 'open'\n",
        "      • Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
        "      • Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
        "      • Bloque ML (label/prob_*) inmediatamente después de 'Type'\n",
        "    Formatea System_time/time/Entry_Date sin tz. Elimina 'id', 'brokerTime' y 'actionType' si aparecieran.\n",
        "    \"\"\"\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Quitar columnas heredadas si existen\n",
        "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    # Formatear tiempos como strings sin tz\n",
        "    for col in [\"System_time\", \"time\", \"Entry_Date\"]:\n",
        "        if col in df_out.columns:\n",
        "            ser = pd.to_datetime(df_out[col], errors=\"coerce\", utc=True)\n",
        "            df_out[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # ───────────────────── Helpers de reorden ─────────────────────\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols:\n",
        "            return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca Real_SL inmediatamente después de Stop_Loss_$; luego base_px y atr_base.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        # quitar para reinsertar\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns:\n",
        "                    cols.insert(i, c)\n",
        "                    i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns and c not in cols:\n",
        "                    cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca 'source' inmediatamente después de 'time' y antes de 'open' si aplica.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols:\n",
        "            cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            # si quedó después de 'open', muévelo antes\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols:\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else:\n",
        "                cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_ml(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca las columnas del modelo (predicción y probas) después de 'Type'.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        ml_block = [\n",
        "            \"label_ml_long\", \"prob_0_long\", \"prob_1_long\", \"prob_2_long\",\n",
        "            \"label_ml_short\", \"prob_0_short\", \"prob_1_short\", \"prob_2_short\"\n",
        "        ]\n",
        "        # eliminar del orden actual para reinsertarlas en bloque\n",
        "        for c in ml_block:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        insert_at = cols.index(\"Type\") + 1 if \"Type\" in cols else len(cols)\n",
        "        for c in ml_block:\n",
        "            if c in df_out.columns:\n",
        "                cols.insert(insert_at, c)\n",
        "                insert_at += 1\n",
        "        return cols\n",
        "    # ──────────────────────────────────────────────────────────────\n",
        "\n",
        "    # Orden base (System_time inmediatamente antes de 'time')\n",
        "    cols = list(df_out.columns)\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    # Aplicar reglas de reorden\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "    ordered = _reorder_ml(ordered)\n",
        "\n",
        "    # Guardar\n",
        "    df_out.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Migra CSV existente:\n",
        "      • Elimina 'id', 'brokerTime' y 'actionType' si existen.\n",
        "      • Agrega si faltan: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
        "      • Formatea System_time/time/Entry_Date sin tz.\n",
        "      • Reordena: System_time antes de time; source después de time y antes de open;\n",
        "                  Entry_Date antes de Stop_Loss_atr;\n",
        "                  Real_SL después de Stop_Loss_$; base_px/atr_base después de Real_SL.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\")\n",
        "\n",
        "    ensure_cols = {\n",
        "        \"System_time\": pd.NaT,\n",
        "        \"profits\":     np.nan,\n",
        "        \"trade_size\":  np.nan,\n",
        "        \"base_px\":     np.nan,\n",
        "        \"atr_base\":    np.nan,\n",
        "        \"source\":      pd.NA,\n",
        "        \"Real_SL\":     np.nan,  # ← NUEVO\n",
        "    }\n",
        "    for c, default in ensure_cols.items():\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    for col in [\"System_time\", \"time\", \"Entry_Date\"]:\n",
        "        if col in df.columns:\n",
        "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols: return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols: cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            if \"Real_SL\" in df.columns: cols.insert(i, \"Real_SL\"); i += 1\n",
        "            for c in [\"base_px\", \"atr_base\"]:\n",
        "                if c in df.columns:\n",
        "                    cols.insert(i, c); i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df.columns and c not in cols: cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols: cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols: cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else: cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    cols = list(df.columns)\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "\n",
        "    df.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "def _load_csv() -> pd.DataFrame:\n",
        "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware; elimina 'id', 'actionType' y 'brokerTime'.\"\"\"\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        FILE_PATH,\n",
        "        dtype={\n",
        "            \"orderId\": \"string\",\n",
        "            \"symbol\":  \"string\",\n",
        "            \"comment\": \"string\",\n",
        "            \"Type\":    \"string\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\")\n",
        "\n",
        "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la última fila; mode=\"missing\": rellena las que estén vacías.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "async def get_current_candle_snapshot(account,\n",
        "                                      rpc_conn,\n",
        "                                      symbol: str = SYMBOL,\n",
        "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve 1 fila con la vela más reciente (puede ser la vela en curso)\n",
        "    con campos: time, open, high, low, close, volume, tickVolume, spread.\n",
        "\n",
        "    Intenta primero vía RPC (candles en vivo) y, si falla, usa el\n",
        "    endpoint histórico como respaldo.\n",
        "    \"\"\"\n",
        "    def _to_row(c: dict) -> dict:\n",
        "        return {\n",
        "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
        "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
        "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
        "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
        "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
        "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
        "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
        "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
        "        }\n",
        "\n",
        "    # 1) Intento RPC (normalmente expone la vela actual)\n",
        "    try:\n",
        "        # Variantes posibles según SDK; probamos dos firmas comunes.\n",
        "        try:\n",
        "            # a) límite por cantidad\n",
        "            candles = await rpc_conn.get_candles(symbol=SYMBOL, timeframe=timeframe, limit=1)\n",
        "        except TypeError:\n",
        "            # b) rango por tiempo (últimos minutos); tomamos la última\n",
        "            to_ts   = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
        "            candles = await rpc_conn.get_candles(symbol=SYMBOL, timeframe=timeframe,\n",
        "                                                 start_time=from_ts, end_time=to_ts)\n",
        "        if candles:\n",
        "            row = _to_row(candles[-1])\n",
        "            return pd.DataFrame([row])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Respaldo: histórico (toma la más reciente cerrada si el broker no sirve la actual)\n",
        "    try:\n",
        "        candles = await account.get_historical_candles(symbol=SYMBOL, timeframe=timeframe, start_time=None, limit=1)\n",
        "        if candles:\n",
        "            row = _to_row(candles[-1])\n",
        "            return pd.DataFrame([row])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 3) Si todo falla, devolver DF vacío con columnas estandarizadas\n",
        "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# CONFIGURACIÓN GENERAL\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logging.getLogger(\"metaapi_cloud_sdk\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"websockets\").setLevel(logging.ERROR)\n",
        "\n",
        "#from metaapi_cloud_sdk.clients.error_handling import TradeException\n",
        "\n",
        "MetaApi.enable_logging()\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\"):\n",
        "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
        "\n",
        "###############################################################################\n",
        "# CONEXIÓN A METAAPI\n",
        "\n",
        "async def connect_metaapi(token: str, account_id: str):\n",
        "    api     = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "    conn    = account.get_rpc_connection()\n",
        "    await conn.connect(); await conn.wait_synchronized()\n",
        "    return account\n",
        "\n",
        "###############################################################################\n",
        "# DESCARGA DE VELAS\n",
        "\n",
        "async def get_candles_5m(account, start: dt.datetime | None, limit: int = CANDEL_NUMBER):\n",
        "    candles = await account.get_historical_candles(\n",
        "        symbol=SYMBOL, timeframe= time_frame_data, start_time=start, limit = limit)\n",
        "    return pd.DataFrame([{\n",
        "        \"time\": pd.to_datetime(c[\"time\"], utc=True),\n",
        "        \"open\": c[\"open\"], \"high\": c[\"high\"], \"low\": c[\"low\"], \"close\": c[\"close\"],\n",
        "        \"volume\": c[\"volume\"], \"tickVolume\": c[\"tickVolume\"], \"spread\": c[\"spread\"]\n",
        "    } for c in candles])\n",
        "\n",
        "###############################################################################\n",
        "# SINCRONIZAR CON EL RELOJ (próximo minuto:07)\n",
        "\n",
        "def seconds_until_next_m07() -> float:\n",
        "    now = dt.datetime.utcnow()\n",
        "    nxt = (now + dt.timedelta(minutes=1)).replace(second=3, microsecond=0)\n",
        "    return max((nxt - now).total_seconds(), 0.5)\n",
        "\n",
        "def seconds_until_next_5m03() -> float:\n",
        "    \"\"\"\n",
        "    Seconds until the next 5-minute boundary + 3s (to let the bar close on most feeds).\n",
        "    Triggers at hh:00:03, :05:03, :10:03, ... UTC.\n",
        "    \"\"\"\n",
        "    now = dt.datetime.utcnow()\n",
        "    # floor to current 5-minute block start\n",
        "    start_5m = now.replace(minute=(now.minute // 5) * 5, second=0, microsecond=0)\n",
        "    nxt = start_5m + dt.timedelta(minutes=5, seconds=3)\n",
        "    return max((nxt - now).total_seconds(), 0.5)\n",
        "\n",
        "\n",
        "def generate_trade_signals(\n",
        "    df: pd.DataFrame,\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula 4 líneas de Kalman sobre 'close' y crea:\n",
        "      • kal_1, kal_2, kal_3, kal_4\n",
        "      • Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
        "      • Close_Trade: -1 si kal_4 < kal_4.shift()  → cierra BUY\n",
        "                     +1 si kal_4 > kal_4.shift()  → cierra SELL\n",
        "        (señales consecutivas iguales se deduplican)\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if \"close\" not in df.columns:\n",
        "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
        "\n",
        "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").fillna(method=\"ffill\")\n",
        "\n",
        "    def _clamp_int(x, mn=1):\n",
        "        try: x = int(x)\n",
        "        except Exception: x = mn\n",
        "        return max(x, mn)\n",
        "\n",
        "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
        "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
        "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
        "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
        "\n",
        "    # Kalman lines\n",
        "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
        "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
        "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
        "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
        "\n",
        "    # Sesgo por k1..k3\n",
        "    k1_up, k2_up, k3_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1), df[\"kal_2\"] > df[\"kal_2\"].shift(1), df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
        "    k1_dn, k2_dn, k3_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1), df[\"kal_2\"] < df[\"kal_2\"].shift(1), df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
        "\n",
        "    bull = k1_up & k2_up & k3_up\n",
        "    bear = k1_dn & k2_dn & k3_dn\n",
        "    aux  = np.where(bull, 1, np.where(bear, -1, np.nan))\n",
        "    df[\"Open_Trade\"] = np.where(pd.Series(aux).shift(1) != aux, aux, np.nan)\n",
        "\n",
        "    # Cierre estrictamente por pendiente de kal_4\n",
        "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)   # +1 → cierra SELL\n",
        "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)   # -1 → cierra BUY\n",
        "    close_raw  = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
        "    close_sr   = pd.Series(close_raw)\n",
        "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "def _rest_place_order(auth_token: str,\n",
        "                      account_id: str,\n",
        "                      region: str,\n",
        "                      symbol: str,\n",
        "                      side: str,\n",
        "                      volume: float,\n",
        "                      comment: str = \"Kal\",\n",
        "                      magic: int | None = None,\n",
        "                      stop_loss: float | None = None,\n",
        "                      take_profit: float | None = None,\n",
        "                      timeout: int = 20):\n",
        "    side = side.upper().strip()\n",
        "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
        "    if side not in action_map:\n",
        "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
        "\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\n",
        "        \"symbol\": symbol,\n",
        "        \"actionType\": action_map[side],\n",
        "        \"volume\": float(volume),\n",
        "        \"comment\": str(comment)\n",
        "    }\n",
        "    if magic is not None:\n",
        "        try: payload[\"magic\"] = int(magic)\n",
        "        except Exception: pass\n",
        "    if stop_loss is not None:\n",
        "        try: payload[\"stopLoss\"] = float(stop_loss)\n",
        "        except Exception: pass\n",
        "    if take_profit is not None:\n",
        "        try: payload[\"takeProfit\"] = float(take_profit)\n",
        "        except Exception: pass\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- FIX 1: really use the symbol param here -------------------------------\n",
        "def _rest_get_positions(auth_token: str,\n",
        "                        account_id: str,\n",
        "                        region: str,\n",
        "                        symbol: str | None = None,\n",
        "                        timeout: int = 15):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
        "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    params = {}\n",
        "    if symbol:\n",
        "        params[\"symbol\"] = str(symbol)   # << was using the global SYMBOL\n",
        "    try:\n",
        "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        class _Dummy:\n",
        "            status_code = 0\n",
        "            def json(self): return {\"error\": str(e)}\n",
        "            text = str(e)\n",
        "        return _Dummy()\n",
        "\n",
        "\n",
        "def _rest_modify_position(auth_token: str,\n",
        "                          account_id: str,\n",
        "                          region: str,\n",
        "                          position_id: str,\n",
        "                          stop_loss: float | None = None,\n",
        "                          take_profit: float | None = None,\n",
        "                          timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Modifica una posición abierta (SL/TP) usando el endpoint REST de MetaApi.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
        "\n",
        "    if stop_loss is not None:\n",
        "        payload[\"stopLoss\"] = float(stop_loss)\n",
        "\n",
        "    if take_profit is not None:\n",
        "        payload[\"takeProfit\"] = float(take_profit)\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "# === DROP-IN REPLACEMENT ==============================================\n",
        "async def open_trade(df: pd.DataFrame,\n",
        "                     rpc_conn,\n",
        "                     symbol: str = \"BTCUSD\",\n",
        "                     lot: float = 1.0,\n",
        "                     comment: str = \"Kal\",\n",
        "                     magic: int = 900002):\n",
        "    \"\"\"\n",
        "    Entra solo si:\n",
        "      • Open_Trade ∈ {+1, -1}\n",
        "      • y la predicción del modelo del lado (label_ml_long/label_ml_short) ∈ {1, 2}\n",
        "\n",
        "    SL/TP al abrir (en precio) se calculan con ATR del último bar:\n",
        "      • Para BUY:  SL = close - ATR * sl_mult ; TP = close + ATR * tp_mult\n",
        "      • Para SELL: SL = close + ATR * sl_mult ; TP = close - ATR * tp_mult\n",
        "\n",
        "    Donde (sl_mult, tp_mult) dependen de la clase:\n",
        "      • class=1 → (stoploss_1, takeprofit_1)\n",
        "      • class=2 → (stoploss_2, takeprofit_2)\n",
        "      • class=0 → no abre\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    # ---------- helpers for existing open/magic sync ----------\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic): return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\"  in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt: return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    # ---------- already open? just sync and exit ----------\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    open_with_magic = [p for p in positions if _has_magic(p)]\n",
        "\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    row = df.index[-1]\n",
        "\n",
        "    if open_with_magic:\n",
        "        p = open_with_magic[0]\n",
        "        df.at[row, \"System_time\"] = now_utc\n",
        "        pm = p.get(\"magic\")\n",
        "        pc = str(p.get(\"comment\") or \"\")\n",
        "        clean_cmt, mag_from_cmt = _split_comment_magic(pc)\n",
        "\n",
        "        if pm is not None and str(pm).strip() != \"\":\n",
        "            try: df.at[row, \"magic\"] = int(pm)\n",
        "            except Exception: df.at[row, \"magic\"] = int(magic)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.at[row, \"magic\"] = int(magic)\n",
        "\n",
        "        df.at[row, \"symbol\"]    = str(p.get(\"symbol\") or symbol)\n",
        "        df.at[row, \"openPrice\"] = float(p.get(\"openPrice\") or p.get(\"price\") or np.nan)\n",
        "        df.at[row, \"comment\"]   = clean_cmt or str(comment)\n",
        "\n",
        "        vol = p.get(\"volume\") or p.get(\"lots\") or None\n",
        "        if vol is not None:\n",
        "            try: df.at[row, \"trade_size\"] = float(vol)\n",
        "            except Exception: pass\n",
        "\n",
        "        side = _side_of(p)\n",
        "        if side: df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
        "        if pd.isna(df.at[row, \"Entry_Date\"]): df.at[row, \"Entry_Date\"] = now_utc\n",
        "\n",
        "        save_csv(df)\n",
        "        print(\"ℹ Position already open; synced last row and skipped new order.\")\n",
        "        return\n",
        "\n",
        "    # ---------- side signal from Open_Trade ----------\n",
        "    sig = df[\"Open_Trade\"].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
        "    if side_req is None:\n",
        "        return\n",
        "\n",
        "    # ---------- read prediction for that side (0/1/2) ----------\n",
        "    pred_col = \"label_ml_long\" if sig == 1 else \"label_ml_short\"\n",
        "    pred_val = df[pred_col].iloc[-1] if pred_col in df.columns else np.nan\n",
        "    try:\n",
        "        pred_cls = int(pred_val)\n",
        "    except Exception:\n",
        "        pred_cls = np.nan\n",
        "\n",
        "    # Rule: class 0 → do not place any order\n",
        "    if not np.isfinite(pred_cls) or pred_cls == 0:\n",
        "        print(f\"ℹ No entry: {pred_col}={pred_val} → skip order.\")\n",
        "        return\n",
        "\n",
        "    # Choose ATR multiples from prediction\n",
        "    if pred_cls == 1:\n",
        "        sl_mult, tp_mult = float(stoploss_1), float(takeprofit_1)\n",
        "    elif pred_cls == 2:\n",
        "        sl_mult, tp_mult = float(stoploss_2), float(takeprofit_2)\n",
        "    else:\n",
        "        print(f\"ℹ Unknown class {pred_cls}; skip order.\")\n",
        "        return\n",
        "\n",
        "    # ---------- prices from ATR ----------\n",
        "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
        "    atr_val    = float(df[\"ATR\"].iloc[-1])   if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
        "    if not (np.isfinite(prev_close) and np.isfinite(atr_val)):\n",
        "        print(\"✘ Missing close/ATR → skip order.\")\n",
        "        return\n",
        "\n",
        "    if side_req == \"BUY\":\n",
        "        sl_to_send = prev_close - atr_val * sl_mult\n",
        "        tp_to_send = prev_close + atr_val * tp_mult\n",
        "    else:  # SELL\n",
        "        sl_to_send = prev_close + atr_val * sl_mult\n",
        "        tp_to_send = prev_close - atr_val * tp_mult\n",
        "\n",
        "    # ---------- send order with SL & TP ----------\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_place_order(\n",
        "            auth_token=META_API_TOKEN,\n",
        "            account_id=ACCOUNT_ID,\n",
        "            region=REGION,\n",
        "            symbol=symbol,\n",
        "            side=side_req,\n",
        "            volume=float(lot),\n",
        "            comment=str(comment),\n",
        "            magic=int(magic),\n",
        "            stop_loss=sl_to_send,\n",
        "            take_profit=tp_to_send,\n",
        "            timeout=20\n",
        "        )\n",
        "    )\n",
        "    if resp.status_code != 200:\n",
        "        try: err = resp.json()\n",
        "        except Exception: err = {\"raw\": resp.text[:500]}\n",
        "        print(\"✘ Order failed\", resp.status_code, json.dumps(err, indent=2, ensure_ascii=False))\n",
        "        return\n",
        "\n",
        "    data        = resp.json()\n",
        "    order_id    = str(data.get(\"orderId\") or \"\")\n",
        "    position_id = str(data.get(\"positionId\") or \"\")\n",
        "\n",
        "    # ---------- write metadata ----------\n",
        "    df.at[row, \"System_time\"]   = now_utc\n",
        "    df.at[row, \"orderId\"]       = order_id\n",
        "    df.at[row, \"magic\"]         = int(magic)\n",
        "    df.at[row, \"symbol\"]        = symbol\n",
        "    df.at[row, \"comment\"]       = str(comment)\n",
        "    df.at[row, \"Entry_Date\"]    = now_utc\n",
        "    df.at[row, \"trade_size\"]    = float(lot)\n",
        "    df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
        "    df.at[row, \"Stop_Loss_atr\"] = float(sl_mult)\n",
        "    # (opcional) conservar TP en el CSV para trazabilidad\n",
        "    try: df.at[row, \"Take_Profit_$\"] = float(tp_to_send)\n",
        "    except Exception: pass\n",
        "\n",
        "    # Try to fetch openPrice & type\n",
        "    open_price, fetched_typ = np.nan, None\n",
        "    if rpc_conn:\n",
        "        try:\n",
        "            for _ in range(60):\n",
        "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "                match = None\n",
        "                for p in (pos_list or []):\n",
        "                    pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                    if pid == position_id or pid == order_id or _has_magic(p):\n",
        "                        match = p; break\n",
        "                if match:\n",
        "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
        "                    if val is not None: open_price = float(val)\n",
        "                    t = _side_of(match)\n",
        "                    if t: fetched_typ = \"Long\" if t == \"BUY\" else \"Short\"\n",
        "                    break\n",
        "                await asyncio.sleep(0.5)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not np.isfinite(open_price):\n",
        "        try: open_price = float(df.at[row, \"close\"])\n",
        "        except Exception: open_price = np.nan\n",
        "    df.at[row, \"openPrice\"] = open_price\n",
        "    df.at[row, \"Type\"]      = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
        "\n",
        "    save_csv(df)\n",
        "    print(f\"✅ {side_req} placed | cls={pred_cls} | SL@{sl_to_send:.2f} (x{sl_mult}) | \"\n",
        "          f\"TP@{tp_to_send:.2f} (x{tp_mult}) | orderId={order_id} positionId={position_id}\")\n",
        "\n",
        "\n",
        "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Por bloque de trade:\n",
        "      • forward-fill de metadatos (incluye Type)\n",
        "      • fija 'base_px' y 'atr_base' con los valores de APERTURA del trade (constantes en el bloque)\n",
        "      • calcula atr_mult_* usando ese 'atr_base'\n",
        "      • calcula 'profits' SOLO si está NaN (para no pisar el de la API)\n",
        "      • propaga Real_SL en el bloque con el último valor no nulo\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    for c in ('time', 'Entry_Date'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
        "\n",
        "    # Reiniciamos solo columnas derivadas de ATR (no profits)\n",
        "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
        "        if c not in df.columns: df[c] = np.nan\n",
        "        else: df[c].values[:] = np.nan\n",
        "    if 'profits' not in df.columns:\n",
        "        df['profits'] = np.nan\n",
        "    if 'base_px'  not in df.columns: df['base_px']  = np.nan\n",
        "    if 'atr_base' not in df.columns: df['atr_base'] = np.nan\n",
        "    if 'Real_SL'  not in df.columns: df['Real_SL']  = np.nan\n",
        "\n",
        "    # Detectar inicios de trade (por orderId; fallback Entry_Date)\n",
        "    starts_mask = pd.Series(False, index=df.index)\n",
        "    if 'orderId' in df.columns:\n",
        "        oid = df['orderId']\n",
        "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
        "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
        "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
        "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
        "    if not starts_mask.any():\n",
        "        return df\n",
        "\n",
        "    groups = starts_mask.cumsum()\n",
        "    trade_ids = groups[starts_mask].unique()\n",
        "\n",
        "    meta_cols = [\"orderId\",\"magic\",\"symbol\",\"openPrice\",\"comment\",\"Type\",\"Entry_Date\",\"trade_size\"]\n",
        "\n",
        "    for gid in trade_ids:\n",
        "        mask = (groups == gid)\n",
        "        start_idx = df.index[mask][0]\n",
        "\n",
        "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
        "        try: base_px = float(base_px)\n",
        "        except Exception: base_px = np.nan\n",
        "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
        "            try: base_px = float(df.at[start_idx, 'close'])\n",
        "            except Exception: base_px = np.nan\n",
        "\n",
        "        atr_base = np.nan\n",
        "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
        "            try: atr_base = float(df.at[start_idx, 'atr_base'])\n",
        "            except Exception: atr_base = np.nan\n",
        "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
        "            try: atr_base = float(df.at[start_idx, 'ATR'])\n",
        "            except Exception: atr_base = np.nan\n",
        "\n",
        "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
        "\n",
        "        # Forward-fill de metadatos del bloque\n",
        "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
        "\n",
        "        if np.isfinite(base_px):  df.loc[mask, 'base_px']  = base_px\n",
        "        if np.isfinite(atr_base): df.loc[mask, 'atr_base'] = atr_base\n",
        "\n",
        "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long','Short'):\n",
        "            if typ == 'Long':\n",
        "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low']  = ((df.loc[mask,  'low'] - base_px) / atr_base).round(2)\n",
        "            else:\n",
        "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low']  = ((base_px - df.loc[mask,  'low']) / atr_base).round(2)\n",
        "\n",
        "        # Solo rellenar profits donde esté NaN (API manda en sync_stop_loss_from_df)\n",
        "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
        "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long','Short'):\n",
        "            m_nan = mask & df['profits'].isna()\n",
        "            if m_nan.any():\n",
        "                if typ == 'Long':\n",
        "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
        "                else:\n",
        "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
        "\n",
        "    # === Propagar Real_SL dentro de cada bloque (último valor no nulo) =======\n",
        "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
        "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
        "    else:\n",
        "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
        "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
        "\n",
        "    if starts.any():\n",
        "        grp = starts.cumsum()\n",
        "        for gid in grp[starts].unique():\n",
        "            m = (grp == gid)\n",
        "            ser = df.loc[m, 'Real_SL']\n",
        "            if ser.notna().any():\n",
        "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
        "                df.loc[m, 'Real_SL'] = val\n",
        "\n",
        "    return df\n",
        "\n",
        "def tick_dyn_atr(df: pd.DataFrame,\n",
        "                 initial_atr: float = INITIAL_SL,\n",
        "                 first_step_atr: float = FIRST_STEP_ATR,\n",
        "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Dinámica de stop en múltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
        "\n",
        "    Además de calcular la columna 'tick_dyn_atr' (múltiplos ATR),\n",
        "    **escribe el precio del stop en 'Stop_Loss_$'** a partir de:\n",
        "      • base_px\n",
        "      • atr_base\n",
        "      • Type ('Long' o 'Short')\n",
        "      • tick_dyn_atr (múltiplos)\n",
        "\n",
        "    Fórmulas:\n",
        "      Long  -> Stop_Loss_$ = base_px + atr_base * tick_dyn_atr\n",
        "      Short -> Stop_Loss_$ = base_px - atr_base * tick_dyn_atr\n",
        "    \"\"\"\n",
        "    col_name = 'tick_dyn_atr'\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = np.nan\n",
        "    if 'Stop_Loss_$' not in df.columns:\n",
        "        df['Stop_Loss_$'] = np.nan\n",
        "    # (opcional) mantener también el múltiplo en Stop_Loss_atr si existe esa columna\n",
        "    if 'Stop_Loss_atr' not in df.columns:\n",
        "        df['Stop_Loss_atr'] = np.nan\n",
        "\n",
        "    in_trade       = False\n",
        "    trade_active   = False\n",
        "    broken         = False\n",
        "    sl_val         = initial_atr\n",
        "    next_threshold = first_step_atr\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        # Detecta nueva apertura real de trade\n",
        "        new_open = (\n",
        "            (('orderId'   in df.columns) and pd.notna(row.get('orderId'))) or\n",
        "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
        "            (pd.notna(row.get('Entry_Date')))\n",
        "        )\n",
        "\n",
        "        if new_open and not in_trade:\n",
        "            in_trade       = True\n",
        "            trade_active   = True\n",
        "            broken         = False\n",
        "            sl_val         = initial_atr\n",
        "            next_threshold = first_step_atr\n",
        "            prev_sl        = sl_val\n",
        "\n",
        "            # Alinear Entry_Date si fuera necesario (usamos 'time')\n",
        "            entry_dt = row.get('time')\n",
        "            if entry_dt is not None:\n",
        "                df.at[idx, 'Entry_Date'] = entry_dt\n",
        "\n",
        "        if in_trade:\n",
        "            m_high = row.get('atr_mult_high', np.nan)\n",
        "            m_low  = row.get('atr_mult_low',  np.nan)\n",
        "            best_pnl = np.nanmax([m_high, m_low])\n",
        "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
        "\n",
        "            if trade_active and not broken:\n",
        "                # subir el múltiplo del stop cada vez que cruzamos un umbral\n",
        "                while best_pnl >= next_threshold:\n",
        "                    sl_val         += gap_first_step_atr\n",
        "                    next_threshold += gap_first_step_atr\n",
        "\n",
        "                # si volvemos por debajo del último SL, marcamos roto\n",
        "                below_prev = (\n",
        "                    (np.isfinite(m_high) and m_high < prev_sl) or\n",
        "                    (np.isfinite(m_low)  and m_low  < prev_sl)\n",
        "                )\n",
        "                if below_prev:\n",
        "                    broken       = True\n",
        "                    trade_active = False\n",
        "                    in_trade     = False\n",
        "\n",
        "            df.at[idx, col_name]      = np.nan if broken else sl_val\n",
        "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
        "            prev_sl = sl_val\n",
        "\n",
        "    # ── Convertir múltiplos a precio y guardarlo en Stop_Loss_$ ─────────────\n",
        "    # Requiere: base_px, atr_base, Type y tick_dyn_atr válidos\n",
        "    try:\n",
        "        sl_mult = df[col_name].astype(float)\n",
        "        base    = df['base_px'].astype(float)\n",
        "        atr     = df['atr_base'].astype(float)\n",
        "        typ     = df['Type'].astype('string')\n",
        "\n",
        "        # precio = base ± atr * múltiplos (según dirección)\n",
        "        stop_price = np.where(\n",
        "            (typ == 'Long')  & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "            base + atr * sl_mult,\n",
        "            np.where(\n",
        "                (typ == 'Short') & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "                base - atr * sl_mult,\n",
        "                np.nan\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Redondeo suave (mantén o ajusta si lo prefieres con más decimales)\n",
        "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
        "    except Exception:\n",
        "        # si faltan columnas o tipos, no rompemos el flujo\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- REST fallback to close a position --------------------------------------\n",
        "def _rest_close_position(auth_token: str,\n",
        "                         account_id: str,\n",
        "                         region: str,\n",
        "                         position_id: str,\n",
        "                         timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Cierra una posición por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    \"\"\"\n",
        "    Positions with short RPC timeouts + REST fallback.\n",
        "    Avoids hangs when websocket subscribe is flaky in Colab.\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    positions = []\n",
        "\n",
        "    async def _rpc_try(call, *args, **kwargs):\n",
        "        try:\n",
        "            return await asyncio.wait_for(call(*args, **kwargs), timeout=4)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    # 1) RPC (with symbol), then RPC (all)\n",
        "    if rpc_conn:\n",
        "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
        "        if not positions:\n",
        "            positions = await _rpc_try(rpc_conn.get_positions)\n",
        "\n",
        "    # 2) REST fallback\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "\n",
        "    return positions\n",
        "\n",
        "\n",
        "\n",
        "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"✘ No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt:  return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close_rpc(pid: str) -> bool:\n",
        "        try:\n",
        "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
        "                return True\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid  = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "\n",
        "        ok = await _try_close_rpc(pid)\n",
        "        if not ok:\n",
        "            # REST fallback\n",
        "            r = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
        "            ok = getattr(r, \"status_code\", 0) == 200\n",
        "\n",
        "        if ok:\n",
        "            print(f\"✅ Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "        else:\n",
        "            print(f\"✘ No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
        "\n",
        "\n",
        "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
        "                                 rpc_conn,\n",
        "                                 symbol: str = SYMBOL,\n",
        "                                 magic: int = 900002,\n",
        "                                 tol: float = 0.01) -> None:\n",
        "    \"\"\"\n",
        "    Copia desde la posición viva los campos de mercado al DF.\n",
        "    ¡Importante!: 'profits' solo se escribe en la ÚLTIMA FILA para no\n",
        "    sobreescribir el histórico de filas previas.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    # 1) Leer posiciones (robusto a SDK sin 'symbol=')\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt:\n",
        "            return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    # Preferir misma symbol + magic; si no, cualquiera con magic\n",
        "    pos = next((p for p in positions if _has_magic(p) and _sym_ok(p)), None)\n",
        "    if not pos:\n",
        "        pos = next((p for p in positions if _has_magic(p)), None)\n",
        "    if not pos:\n",
        "        return\n",
        "\n",
        "    order_id    = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    magic_val   = pos.get(\"magic\")\n",
        "    symbol_val  = pos.get(\"symbol\")\n",
        "    open_price  = pos.get(\"openPrice\") or pos.get(\"price\")\n",
        "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\") or None\n",
        "    stop_loss   = pos.get(\"stopLoss\")\n",
        "    volume_val  = pos.get(\"volume\") or pos.get(\"lots\")\n",
        "    profit_val  = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
        "                   else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
        "\n",
        "    # Type → Long/Short\n",
        "    t = pos.get(\"type\")\n",
        "    typ = None\n",
        "    if isinstance(t, str):\n",
        "        tu = t.upper()\n",
        "        if \"BUY\" in tu:  typ = \"Long\"\n",
        "        if \"SELL\" in tu: typ = \"Short\"\n",
        "    elif t == 0:\n",
        "        typ = \"Long\"\n",
        "    elif t == 1:\n",
        "        typ = \"Short\"\n",
        "\n",
        "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
        "\n",
        "    clean_cmt, mag_from_cmt = _split_comment_magic(str(comment_raw or \"\"))\n",
        "\n",
        "    # 3) Determinar bloque del trade activo donde escribir (sin tocar 'profits' del bloque)\n",
        "    block_mask = pd.Series(False, index=df.index)\n",
        "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
        "        block_mask = (df[\"orderId\"] == order_id)\n",
        "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
        "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
        "        starts = ed.notna() & (ed != ed.shift(1))\n",
        "        if starts.any():\n",
        "            last_start = df.index[starts].max()\n",
        "            block_mask = (df.index >= last_start)\n",
        "        else:\n",
        "            block_mask = ed.notna() & (ed >= entry_dt)\n",
        "    if not block_mask.any():\n",
        "        block_mask.iloc[-1] = True  # asegura al menos la última fila\n",
        "\n",
        "    # 4) Escribir valores de mercado en el bloque (EXCEPTO 'profits')\n",
        "    try:\n",
        "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
        "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.loc[block_mask, \"magic\"] = int(magic)\n",
        "\n",
        "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
        "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
        "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
        "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
        "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
        "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
        "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
        "        # ⛔ NO: df.loc[block_mask, \"profits\"] = profit_val  (no sobrescribir histórico)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 5) Escribir SIEMPRE en la ÚLTIMA FILA los valores actuales (incluye 'profits')\n",
        "    last_idx = df.index[-1]\n",
        "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
        "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
        "    elif mag_from_cmt is not None:\n",
        "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
        "    else:\n",
        "        df.at[last_idx, \"magic\"] = int(magic)\n",
        "\n",
        "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
        "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
        "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
        "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
        "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
        "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
        "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
        "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
        "\n",
        "\n",
        "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Devuelve (prev, last) con los dos últimos valores no-NaN distintos.\n",
        "    Si no hay prev distinto, prev = np.nan.\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        return (np.nan, np.nan)\n",
        "    last = float(s.iloc[-1])\n",
        "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
        "    return (prev, last)\n",
        "\n",
        "\n",
        "# --- FIX 2: find position by magic robustly (with fallbacks) ---------------\n",
        "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
        "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        # magic can be under different keys/types depending on source\n",
        "        for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\"):\n",
        "            if key in p and p[key] is not None:\n",
        "                try:\n",
        "                    if int(p[key]) == int(magic):\n",
        "                        return True\n",
        "                except Exception:\n",
        "                    # sometimes it's a plain string \"900002\"\n",
        "                    if str(p[key]).strip() == str(magic):\n",
        "                        return True\n",
        "        # optional safety: encoded in comment (not your case, but harmless)\n",
        "        if f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    # Prefer same symbol first\n",
        "    for p in positions:\n",
        "        if _sym_ok(p) and _has_magic(p):\n",
        "            return p\n",
        "    # Then any matching magic (in case broker returns different casing/suffix)\n",
        "    for p in positions:\n",
        "        if _has_magic(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- FIX 3: stronger fallback in SL sync/modify ----------------------------\n",
        "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
        "                                     rpc_conn,\n",
        "                                     *,\n",
        "                                     symbol: str,\n",
        "                                     magic: int,\n",
        "                                     auth_token: str,\n",
        "                                     account_id: str,\n",
        "                                     region: str,\n",
        "                                     tol: float = 0.0) -> dict:\n",
        "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
        "    if not np.isfinite(last_sl):\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vacío\"}\n",
        "\n",
        "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
        "    if not changed:\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
        "\n",
        "    # A) try by magic (now robust & with REST fallback)\n",
        "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "\n",
        "    # B) if still not found, try to match the last orderId in the CSV to broker position id\n",
        "    if not pos:\n",
        "        last_oid = None\n",
        "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
        "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
        "        if last_oid:\n",
        "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "            for p in positions:\n",
        "                pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                if pid == last_oid:\n",
        "                    pos = p\n",
        "                    break\n",
        "\n",
        "    if not pos:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posición con ese magic\"}\n",
        "\n",
        "    position_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    if not position_id:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vacío\"}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_modify_position(\n",
        "            auth_token=auth_token,\n",
        "            account_id=account_id,\n",
        "            region=region,\n",
        "            position_id=position_id,\n",
        "            stop_loss=float(last_sl),\n",
        "            timeout=15\n",
        "        )\n",
        "    )\n",
        "    ok = getattr(resp, \"status_code\", 0) == 200\n",
        "    err = None\n",
        "    if not ok:\n",
        "        try: err = json.dumps(resp.json())[:300]\n",
        "        except Exception: err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
        "\n",
        "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
        "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
        "\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    # 1) RPC with symbol\n",
        "    try:\n",
        "        positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    # 2) RPC without symbol\n",
        "    if not positions:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions() or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "    # 3) REST fallback\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n",
        "\n",
        "# Rutas a listas de columnas (una por lado)\n",
        "FEATS_LONG_PATH  = f\"{root_data}Results/{SYMBOL}_Long_M5M10_ImportantCols.csv\"\n",
        "FEATS_SHORT_PATH = f\"{root_data}Results/{SYMBOL}_Short_M5M10_ImportantCols.csv\"\n",
        "\n",
        "def _load_feature_list(path: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Reads a 1-column CSV (with or without header), removes junk entries like '0',\n",
        "    'Unnamed:*', '', and de-duplicates while preserving order.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"⚠️ Feature list not found: {path}. Falling back to model introspection.\")\n",
        "        return []\n",
        "\n",
        "    dfc = pd.read_csv(path, header=None)\n",
        "    # If there is an accidental header or extra columns, take the first column\n",
        "    if dfc.shape[1] > 1:\n",
        "        dfc = dfc.iloc[:, [0]]\n",
        "\n",
        "    raw = dfc.iloc[:, 0].astype(str).tolist()\n",
        "\n",
        "    def _ok(s: str) -> bool:\n",
        "        s2 = s.strip()\n",
        "        if s2 == \"\" or s2.lower().startswith(\"unnamed\") or s2.lower() == \"index\" or s2.isdigit():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    cleaned = [s.strip() for s in raw if _ok(s)]\n",
        "    # de-dup preserve order\n",
        "    seen, out = set(), []\n",
        "    for c in cleaned:\n",
        "        if c not in seen:\n",
        "            out.append(c)\n",
        "            seen.add(c)\n",
        "    return out\n",
        "\n",
        "\n",
        "FEATURES_LONG  = _load_feature_list(FEATS_LONG_PATH)\n",
        "FEATURES_SHORT = _load_feature_list(FEATS_SHORT_PATH)\n",
        "\n",
        "def _safe_create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uses your create_features(df) on 5m OHLCV (Title-case),\n",
        "    and ALSO carries through any precomputed 10m feature columns already\n",
        "    present on `df` (prefix '10min_10min_' or '10_min_10_min').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_in = df.copy()\n",
        "        col_map = {\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"}\n",
        "        for lo, hi in col_map.items():\n",
        "            if lo in df_in.columns and hi not in df_in.columns:\n",
        "                df_in[hi] = df_in[lo]\n",
        "\n",
        "        feats = create_features(df_in)  # your big feature builder (5m)\n",
        "\n",
        "        # carry 10m block that we already appended to df\n",
        "        extra_cols = [c for c in df.columns\n",
        "                      if c.startswith(\"10min_10min_\") or c.startswith(\"10_min_10_min\")]\n",
        "        if extra_cols:\n",
        "            # ensure same index length; most flows use the df index as the feature index\n",
        "            feats = feats.reindex(df.index)\n",
        "            feats = feats.join(df[extra_cols], how=\"left\")\n",
        "\n",
        "        return feats\n",
        "    except NameError:\n",
        "        # if create_features is unavailable, fall back to numeric columns only\n",
        "        return df.select_dtypes(include=[np.number]).copy()\n",
        "    except Exception:\n",
        "        return df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "def _expected_features_from_model(model) -> list[str] | None:\n",
        "    \"\"\"\n",
        "    Best-effort introspection for sklearn-style models to get the exact\n",
        "    feature name list used at fit time. Returns None if not available.\n",
        "    \"\"\"\n",
        "    # 1) Direct attribute (most sklearn estimators/pipelines after fit)\n",
        "    for attr in (\"feature_names_in_\",):\n",
        "        if hasattr(model, attr):\n",
        "            try:\n",
        "                names = list(getattr(model, attr))\n",
        "                if names:\n",
        "                    return [str(c) for c in names]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # 2) sklearn Pipeline / Voting / Bagging wrappers\n",
        "    for attr in (\"estimators_\", \"named_estimators_\", \"steps\", \"named_steps\"):\n",
        "        if hasattr(model, attr):\n",
        "            try:\n",
        "                obj = getattr(model, attr)\n",
        "                # pipeline.steps → [('scaler', StandardScaler), ('clf', ...)]\n",
        "                if isinstance(obj, list):  # steps or list of estimators\n",
        "                    for _, step in obj:\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "                elif isinstance(obj, dict):  # named_steps / named_estimators_\n",
        "                    for step in obj.values():\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "                elif isinstance(obj, (tuple, set)):\n",
        "                    for step in obj:\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _align_matrix(X: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Alinea X a 'cols': agrega faltantes con 0.0 y ordena columnas.\"\"\"\n",
        "    if not cols:\n",
        "        # sin lista → usar todas las columnas disponibles\n",
        "        Z = X.copy()\n",
        "        return Z.fillna(0.0)\n",
        "    missing = [c for c in cols if c not in X.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            X[c] = 0.0\n",
        "        print(f\"⚠️ Faltan {len(missing)} features en live: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
        "    Z = X.loc[:, cols].copy()\n",
        "    return Z.fillna(0.0)\n",
        "\n",
        "def _predict_with_optional_proba(model, X: pd.DataFrame):\n",
        "    \"\"\"Devuelve (y_pred, proba, classes) funcionando con sklearn/keras.\"\"\"\n",
        "    proba, classes = None, None\n",
        "    # try predict_proba (sklearn)\n",
        "    try:\n",
        "        proba = model.predict_proba(X)\n",
        "        try:   classes = np.array(model.classes_)\n",
        "        except Exception: classes = np.arange(proba.shape[1])\n",
        "    except Exception:\n",
        "        # fallback keras: predict → proba\n",
        "        try:\n",
        "            raw = model.predict(X)\n",
        "            raw = np.asarray(raw)\n",
        "            if raw.ndim == 2:\n",
        "                proba = raw\n",
        "                if proba.shape[1] == 1:  # binario [p1] → [p0,p1]\n",
        "                    p1 = proba[:, 0]\n",
        "                    proba = np.column_stack([1.0 - p1, p1])\n",
        "                classes = np.arange(proba.shape[1])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # y_pred\n",
        "    try:\n",
        "        y_pred = model.predict(X)\n",
        "        y_pred = np.asarray(y_pred).ravel()\n",
        "    except Exception:\n",
        "        if proba is not None:\n",
        "            y_pred = np.argmax(proba, axis=1)\n",
        "        else:\n",
        "            raise RuntimeError(\"El modelo no soporta predict/predict_proba compatibles.\")\n",
        "    if classes is None and proba is not None:\n",
        "        classes = np.arange(proba.shape[1])\n",
        "    return y_pred, proba, classes\n",
        "\n",
        "# --- REPLACE your run_side_models_inplace with this version ------------\n",
        "def run_side_models_inplace(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    long_model,\n",
        "    short_model,\n",
        "    feature_cols_long: list[str] | None,\n",
        "    feature_cols_short: list[str] | None,\n",
        "    max_lookback: int = 1200,          # enough for Kalman 1000 + slope/vol windows\n",
        "    triggers_per_side: int = 1         # only predict at the latest trigger(s)\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute features & run ML ONLY where a trade could open:\n",
        "      - Open_Trade == +1 → run LONG model at those bars\n",
        "      - Open_Trade == -1 → run SHORT model at those bars\n",
        "\n",
        "    We build features on a compact tail slice (max_lookback) and only\n",
        "    materialize predictions on the trigger rows.\n",
        "    \"\"\"\n",
        "\n",
        "    if df is None or df.empty or (\"Open_Trade\" not in df.columns):\n",
        "        return df\n",
        "\n",
        "    # --- which rows really need predictions?\n",
        "    idx_long_all  = df.index[df[\"Open_Trade\"] ==  1]\n",
        "    idx_short_all = df.index[df[\"Open_Trade\"] == -1]\n",
        "\n",
        "    if len(idx_long_all) == 0 and len(idx_short_all) == 0:\n",
        "        # ensure prob columns exist, then nothing to do\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "        return df\n",
        "\n",
        "    # take the most recent trigger(s) per side to avoid recomputing everything\n",
        "    idx_long_targets  = list(idx_long_all[-triggers_per_side:])   if len(idx_long_all)  else []\n",
        "    idx_short_targets = list(idx_short_all[-triggers_per_side:])  if len(idx_short_all) else []\n",
        "\n",
        "    # quick exit: if the last trigger rows already have labels, skip\n",
        "    def _missing_any(pred_col: str, targets: list) -> bool:\n",
        "        if not targets:\n",
        "            return False\n",
        "        if pred_col not in df.columns:\n",
        "            return True\n",
        "        return df.loc[targets, pred_col].isna().any()\n",
        "\n",
        "    need_long  = _missing_any(\"label_ml_long\",  idx_long_targets)\n",
        "    need_short = _missing_any(\"label_ml_short\", idx_short_targets)\n",
        "    if not (need_long or need_short):\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "        return df\n",
        "\n",
        "    # ---- pick a compact tail slice to compute features once\n",
        "    # we’ll include enough history and then only read the rows we care about\n",
        "    tail = df.tail(max_lookback + 50).copy()\n",
        "\n",
        "    # 10m block is expensive → compute it only if required by the model’s feature lists\n",
        "    model_req_long  = _expected_features_from_model(long_model)  or []\n",
        "    model_req_short = _expected_features_from_model(short_model) or []\n",
        "\n",
        "    csv_req_long    = _preferred_feature_order(\"long\")  or (feature_cols_long  or [])\n",
        "    csv_req_short   = _preferred_feature_order(\"short\") or (feature_cols_short or [])\n",
        "\n",
        "    req_long  = model_req_long  or csv_req_long\n",
        "    req_short = model_req_short or csv_req_short\n",
        "\n",
        "    def _needs_10m(required_cols: list[str]) -> bool:\n",
        "        return any(isinstance(c, str) and (c.startswith(\"10min_10min_\") or c.startswith(\"10_min_10_min_\"))\n",
        "                   for c in (required_cols or []))\n",
        "\n",
        "    try:\n",
        "        if _needs_10m(req_long) or _needs_10m(req_short):\n",
        "            append_10m_features_inplace(tail, prefix=\"10min_10min_\", pre_scale=True)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 10m feature block skipped: {e}\")\n",
        "\n",
        "    # Build base feature matrix once on the tail\n",
        "    feats_all = _safe_create_features(tail)\n",
        "\n",
        "    # Fix '10min' vs '10_min' aliasing if needed\n",
        "    feats_all = _ensure_10m_prefix_aliases(feats_all, (req_long or []) + (req_short or []))\n",
        "\n",
        "    # Ensure Encoded_* columns (if expected)\n",
        "    enc_n_long  = _encoded_span_from(req_long)\n",
        "    enc_n_short = _encoded_span_from(req_short)\n",
        "    for i in range(max(enc_n_long, enc_n_short)):\n",
        "        c = f\"Encoded_{i}\"\n",
        "        if c not in feats_all.columns:\n",
        "            feats_all[c] = 0.0\n",
        "\n",
        "    # Align matrices exactly to each side’s requirements\n",
        "    def _align_exact(X: pd.DataFrame, req: list[str]) -> pd.DataFrame:\n",
        "        if not req:\n",
        "            Z = X.select_dtypes(include=[np.number]).copy()\n",
        "            return Z.fillna(0.0)\n",
        "        missing = [c for c in req if c not in X.columns]\n",
        "        for c in missing:\n",
        "            X[c] = 0.0\n",
        "        Z = X.loc[:, req].copy()\n",
        "        return Z.fillna(0.0)\n",
        "\n",
        "    X_long_all  = _align_exact(feats_all.copy(), req_long)\n",
        "    X_short_all = _align_exact(feats_all.copy(), req_short)\n",
        "\n",
        "    # restrict to the trigger rows that actually live inside the tail slice\n",
        "    tail_idx = feats_all.index\n",
        "    idx_long  = [i for i in idx_long_targets  if i in tail_idx] if need_long  else []\n",
        "    idx_short = [i for i in idx_short_targets if i in tail_idx] if need_short else []\n",
        "\n",
        "    # helper → works with sklearn or keras-like models\n",
        "    def _predict_with_optional_proba(model, X: pd.DataFrame):\n",
        "        proba, classes = None, None\n",
        "        try:\n",
        "            proba = model.predict_proba(X)\n",
        "            try:\n",
        "                classes = np.array(model.classes_)\n",
        "            except Exception:\n",
        "                classes = np.arange(proba.shape[1])\n",
        "        except Exception:\n",
        "            try:\n",
        "                raw = model.predict(X)\n",
        "                raw = np.asarray(raw)\n",
        "                if raw.ndim == 2:\n",
        "                    proba = raw\n",
        "                    if proba.shape[1] == 1:\n",
        "                        p1 = proba[:, 0]\n",
        "                        proba = np.column_stack([1.0 - p1, p1])\n",
        "                    classes = np.arange(proba.shape[1])\n",
        "            except Exception:\n",
        "                pass\n",
        "        try:\n",
        "            y_pred = model.predict(X)\n",
        "            y_pred = np.asarray(y_pred).ravel()\n",
        "        except Exception:\n",
        "            if proba is not None:\n",
        "                y_pred = np.argmax(proba, axis=1)\n",
        "            else:\n",
        "                raise RuntimeError(\"El modelo no soporta predict/predict_proba compatibles.\")\n",
        "        if classes is None and proba is not None:\n",
        "            classes = np.arange(proba.shape[1])\n",
        "        return y_pred, proba, classes\n",
        "\n",
        "    # LONG side only where Open_Trade == +1\n",
        "    if idx_long:\n",
        "        X = X_long_all.loc[idx_long]\n",
        "        yL, pL, cL = _predict_with_optional_proba(long_model, X)\n",
        "        df.loc[idx_long, \"label_ml_long\"] = yL\n",
        "        if pL is not None:\n",
        "            for j in range(pL.shape[1]):\n",
        "                cname = f\"prob_{(cL[j] if cL is not None else j)}_long\"\n",
        "                df.loc[idx_long, cname] = pL[:, j]\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "\n",
        "    # SHORT side only where Open_Trade == -1\n",
        "    if idx_short:\n",
        "        X = X_short_all.loc[idx_short]\n",
        "        yS, pS, cS = _predict_with_optional_proba(short_model, X)\n",
        "        df.loc[idx_short, \"label_ml_short\"] = yS\n",
        "        if pS is not None:\n",
        "            for j in range(pS.shape[1]):\n",
        "                cname = f\"prob_{(cS[j] if cS is not None else j)}_short\"\n",
        "                df.loc[idx_short, cname] = pS[:, j]\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5cvhz3M-fTIw",
      "metadata": {
        "id": "5cvhz3M-fTIw"
      },
      "outputs": [],
      "source": [
        "# ========================= MAIN (copy/paste) =========================\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    First run:\n",
        "      • fetch 900 5m candles\n",
        "      • compute ATR + signals (+ ML if an entry trigger exists)\n",
        "      • save ALL 900 rows to CSV (source=1, System_time filled)\n",
        "\n",
        "    Then:\n",
        "      • poll at 5m+3s while no position\n",
        "      • when a position with magic exists, follow every 20s:\n",
        "          - refresh data, recompute signals, close by Close_Trade\n",
        "          - recompute atr_close + tick_dyn_atr\n",
        "          - push SL change if Stop_Loss_$ changed\n",
        "          - sync market fields from API (incl. Real_SL)\n",
        "          - save to CSV\n",
        "    \"\"\"\n",
        "    # 0) Connect MetaApi / RPC\n",
        "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "    asyncio.create_task(collect_10m_data(account))\n",
        "\n",
        "    # ---- strategy params ----------------------------------------------------\n",
        "    MAGIC    = 900002\n",
        "    bars_init = 900  # << first-run bars to compute features on\n",
        "\n",
        "    length_1 = 300\n",
        "    length_2 = 410\n",
        "    length_3 = 710\n",
        "    length_4 = 870\n",
        "\n",
        "    smooth_1 = 3\n",
        "    smooth_2 = 3\n",
        "    smooth_3 = 3\n",
        "    smooth_4 = 5\n",
        "\n",
        "    # ---- small helpers (local) ---------------------------------------------\n",
        "    def _frame_delta(tf: str) -> dt.timedelta:\n",
        "        tf = (tf or \"\").strip().lower()\n",
        "        if tf.endswith(\"mn\"):\n",
        "            tf = tf[:-2] + \"m\"\n",
        "        if tf.endswith(\"m\"):\n",
        "            return dt.timedelta(minutes=int(tf[:-1] or \"1\"))\n",
        "        if tf.endswith(\"h\"):\n",
        "            return dt.timedelta(hours=int(tf[:-1] or \"1\"))\n",
        "        if tf.endswith(\"d\"):\n",
        "            return dt.timedelta(days=int(tf[:-1] or \"1\"))\n",
        "        return dt.timedelta(minutes=1)\n",
        "\n",
        "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
        "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        secs  = int((ts - epoch).total_seconds())\n",
        "        step  = int(delta.total_seconds()) or 60\n",
        "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
        "\n",
        "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
        "        # try RPC\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "        # REST fallback\n",
        "        if not positions:\n",
        "            r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "            if getattr(r, \"status_code\", 0) == 200:\n",
        "                try:\n",
        "                    positions = r.json() or []\n",
        "                except Exception:\n",
        "                    positions = []\n",
        "        if not positions:\n",
        "            return False\n",
        "\n",
        "        for p in positions:\n",
        "            pm = p.get(\"magic\")\n",
        "            has_magic = False\n",
        "            if pm is not None:\n",
        "                try:\n",
        "                    has_magic = int(pm) == int(magic)\n",
        "                except Exception:\n",
        "                    has_magic = False\n",
        "            if not has_magic and f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "                has_magic = True\n",
        "            if has_magic:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    async def _get_api_type(rpc_conn, symbol: str, magic: int) -> Optional[str]:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "        def _has_magic(p):\n",
        "            pm = p.get(\"magic\", None)\n",
        "            ok = False\n",
        "            if pm is not None:\n",
        "                try:\n",
        "                    ok = int(pm) == int(magic)\n",
        "                except Exception:\n",
        "                    ok = False\n",
        "            if not ok:\n",
        "                ok = f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "            return ok\n",
        "        def _side_of(p):\n",
        "            t = p.get(\"type\")\n",
        "            if isinstance(t, str):\n",
        "                tt = t.upper()\n",
        "                if \"BUY\" in tt:  return \"BUY\"\n",
        "                if \"SELL\" in tt: return \"SELL\"\n",
        "            if t == 0: return \"BUY\"\n",
        "            if t == 1: return \"SELL\"\n",
        "            return \"\"\n",
        "        for p in positions:\n",
        "            if not _has_magic(p):\n",
        "                continue\n",
        "            s = _side_of(p)\n",
        "            if s:\n",
        "                return \"Long\" if s == \"BUY\" else \"Short\"\n",
        "        return None\n",
        "\n",
        "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: Optional[str]) -> None:\n",
        "        if not api_type or df_all.empty:\n",
        "            return\n",
        "        last_oid = df_all.get(\"orderId\")\n",
        "        if last_oid is not None and last_oid.notna().any():\n",
        "            last_oid_val = last_oid.dropna().iloc[-1]\n",
        "            mask = df_all[\"orderId\"] == last_oid_val\n",
        "        else:\n",
        "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
        "            starts = ed.notna() & (ed != ed.shift(1))\n",
        "            if starts.any():\n",
        "                start_idx = df_all.index[starts][-1]\n",
        "                mask = (df_all.index >= start_idx)\n",
        "            else:\n",
        "                mask = pd.Series(False, index=df_all.index)\n",
        "        if mask.any():\n",
        "            df_all.loc[mask, \"Type\"] = api_type\n",
        "        else:\n",
        "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
        "\n",
        "    # 1) First-run: create/migrate CSV ----------------------------------------\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        # --- fetch 900 bars, compute, and SAVE ALL ROWS ---\n",
        "        df_hist = await get_candles_5m(account, start=None, limit=bars_init)\n",
        "        df_hist = (df_hist\n",
        "                   .drop_duplicates(\"time\")\n",
        "                   .sort_values(\"time\")\n",
        "                   .reset_index(drop=True))\n",
        "\n",
        "        if len(df_hist) >= 14:\n",
        "            df_hist[\"ATR\"] = ta.ATR(df_hist[\"high\"], df_hist[\"low\"], df_hist[\"close\"], 14).round(4)\n",
        "\n",
        "        df_all = df_hist.copy()\n",
        "\n",
        "        generate_trade_signals(\n",
        "            df_all, length_1, length_2, length_3, length_4,\n",
        "            smooth_1, smooth_2, smooth_3, smooth_4\n",
        "        )\n",
        "\n",
        "        maybe_compute_ml_for_entry(\n",
        "            df_all,\n",
        "            long_model=long_ml_model,\n",
        "            short_model=short_ml_model,\n",
        "            feature_cols_long=FEATURES_LONG,\n",
        "            feature_cols_short=FEATURES_SHORT\n",
        "        )\n",
        "        sync_unified_ml_cols(df_all)\n",
        "        _ensure_order_cols(df_all)\n",
        "\n",
        "        # mark provenance and stamp time on every row\n",
        "        df_all[\"source\"] = 1\n",
        "        stamp_system_time(df_all, mode=\"missing\")\n",
        "\n",
        "        save_csv(df_all)\n",
        "        print(f\"✔ Archivo inicial creado: {len(df_all)} filas guardadas (se analizaron {bars_init} velas)\")\n",
        "    else:\n",
        "        migrate_csv_if_needed(FILE_PATH)\n",
        "\n",
        "    # 2) Live loop -------------------------------------------------------------\n",
        "    while True:\n",
        "\n",
        "        # ───────── NO open position → wait to next 5m+03s tick ─────────\n",
        "        if not await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "            await asyncio.sleep(FETCH_INTERVAL)\n",
        "\n",
        "            now_utc   = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            delta     = _frame_delta(time_frame_data)\n",
        "            this_bar  = _floor_to_frame(now_utc, delta)\n",
        "            prev_bar  = this_bar - delta\n",
        "\n",
        "            # fetch the last closed bar only\n",
        "            df_new = await get_candles_5m(account, start=None, limit=50)\n",
        "            df_new = (df_new[df_new[\"time\"] <= prev_bar]\n",
        "                      .drop_duplicates(\"time\")\n",
        "                      .sort_values(\"time\"))\n",
        "            if not df_new.empty:\n",
        "                df_new = df_new.iloc[[-1]]\n",
        "\n",
        "            df_all = _load_csv()\n",
        "            existing_times = set(pd.to_datetime(df_all[\"time\"], utc=True)) if not df_all.empty and \"time\" in df_all.columns else set()\n",
        "            if df_all.empty:\n",
        "                df_all = df_new.copy()\n",
        "            else:\n",
        "                df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
        "                            .drop_duplicates(\"time\")\n",
        "                            .sort_values(\"time\")\n",
        "                            .reset_index(drop=True))\n",
        "\n",
        "            if len(df_all) >= 14:\n",
        "                df_all[\"ATR\"] = ta.ATR(df_all[\"high\"], df_all[\"low\"], df_all[\"close\"], 14).round(4)\n",
        "\n",
        "            generate_trade_signals(\n",
        "                df_all, length_1, length_2, length_3, length_4,\n",
        "                smooth_1, smooth_2, smooth_3, smooth_4\n",
        "            )\n",
        "            maybe_compute_ml_for_entry(\n",
        "                df_all,\n",
        "                long_model=long_ml_model,\n",
        "                short_model=short_ml_model,\n",
        "                feature_cols_long=FEATURES_LONG,\n",
        "                feature_cols_short=FEATURES_SHORT\n",
        "            )\n",
        "            _ensure_order_cols(df_all)\n",
        "            sync_unified_ml_cols(df_all)\n",
        "\n",
        "            # mark new row(s) as source=1\n",
        "            if not df_new.empty:\n",
        "                new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
        "                if new_times:\n",
        "                    df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
        "\n",
        "            # try to open trade (if ML permits)\n",
        "            await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT, comment=COMMENT, magic=MAGIC)\n",
        "\n",
        "            # sync type and market fields, then save\n",
        "            api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
        "            _sync_type_in_df(df_all, api_type)\n",
        "            await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "            stamp_system_time(df_all, \"last\")\n",
        "            save_csv(df_all)\n",
        "            print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"),\n",
        "                  \"| actualización (modo búsqueda de entrada)\")\n",
        "\n",
        "        # ───────── OPEN position → follow every 20s ─────────\n",
        "        else:\n",
        "            print(\"▶ Modo seguimiento: posición con magic 900002 detectada\")\n",
        "\n",
        "            while await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "\n",
        "                await asyncio.sleep(FETCH_INTERVAL)\n",
        "\n",
        "                # load CSV and last time\n",
        "                df_all = _load_csv()\n",
        "                last_time = None\n",
        "                if not df_all.empty and \"time\" in df_all.columns:\n",
        "                    last_time = pd.to_datetime(df_all[\"time\"], errors=\"coerce\", utc=True).max()\n",
        "\n",
        "                # live snapshot; append only strictly new timestamps\n",
        "                df_new = await get_current_candle_snapshot(account, rpc_conn, symbol=SYMBOL, timeframe=time_frame_data)\n",
        "                df_new = df_new.drop_duplicates(\"time\").sort_values(\"time\")\n",
        "                df_inc = df_new[df_new[\"time\"] > last_time] if last_time is not None and pd.notna(last_time) else df_new\n",
        "\n",
        "                if not df_inc.empty:\n",
        "                    df_all = pd.concat([df_all, df_inc], ignore_index=True)\n",
        "\n",
        "                # recalc ATR and signals\n",
        "                if len(df_all) >= 14:\n",
        "                    df_all[\"ATR\"] = ta.ATR(df_all[\"high\"], df_all[\"low\"], df_all[\"close\"], 14).round(4)\n",
        "\n",
        "                generate_trade_signals(\n",
        "                    df_all, length_1, length_2, length_3, length_4,\n",
        "                    smooth_1, smooth_2, smooth_3, smooth_4\n",
        "                )\n",
        "                maybe_compute_ml_for_entry(\n",
        "                    df_all,\n",
        "                    long_model=long_ml_model,\n",
        "                    short_model=short_ml_model,\n",
        "                    feature_cols_long=FEATURES_LONG,\n",
        "                    feature_cols_short=FEATURES_SHORT\n",
        "                )\n",
        "                sync_unified_ml_cols(df_all)\n",
        "\n",
        "                # close order if Close_Trade says so\n",
        "                await close_order(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC, close_col=\"Close_Trade\")\n",
        "\n",
        "                # recompute ATR-based tracking + dynamic SL\n",
        "                atr_close(df_all)\n",
        "                tick_dyn_atr(df_all)\n",
        "\n",
        "                # if Stop_Loss_$ changed, push modification\n",
        "                sl_res = await modify_stoploss_if_changed(\n",
        "                    df_all, rpc_conn,\n",
        "                    symbol=SYMBOL, magic=MAGIC,\n",
        "                    auth_token=META_API_TOKEN,\n",
        "                    account_id=ACCOUNT_ID,\n",
        "                    region=REGION,\n",
        "                    tol=0.0\n",
        "                )\n",
        "\n",
        "                # always sync market fields (including Real_SL) from API\n",
        "                await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "                # stamp and mark source for new rows, then save\n",
        "                stamp_system_time(df_all, \"last\")\n",
        "                _ensure_order_cols(df_all)\n",
        "                if not df_inc.empty:\n",
        "                    df_all.loc[df_all[\"time\"].isin(df_inc[\"time\"]), \"source\"] = 9\n",
        "                else:\n",
        "                    df_all.at[df_all.index[-1], \"source\"] = 9\n",
        "                save_csv(df_all)\n",
        "\n",
        "                # logs\n",
        "                base = (dt.datetime.utcnow().strftime(\"%H:%M:%S\")\n",
        "                        + \" | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\")\n",
        "                print(base)\n",
        "                if sl_res.get(\"changed\"):\n",
        "                    if sl_res.get(\"sent\"):\n",
        "                        print(f\"   ↳ SL actualizado en broker a {sl_res['price']:.2f} (positionId={sl_res.get('position_id','?')})\")\n",
        "                    else:\n",
        "                        msg_err = sl_res.get(\"err\") or \"error desconocido\"\n",
        "                        price = sl_res.get(\"price\", np.nan)\n",
        "                        print(f\"   ↳ SL cambió a {price:.2f} pero no se envió ({msg_err})\")\n",
        "\n",
        "            print(\"⏹ La posición con magic 900002 se cerró → regreso a búsqueda de entrada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "68155fee",
      "metadata": {
        "id": "68155fee"
      },
      "outputs": [],
      "source": [
        "# Background task to store 10-minute candles\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import asyncio\n",
        "\n",
        "FILE_PATH_10M = \"xauusd_data_10m.csv\"\n",
        "BARS_INIT_10M = 900\n",
        "\n",
        "async def collect_10m_data(account, symbol=SYMBOL, path=FILE_PATH_10M):\n",
        "    \"\"\"Fetch initial 10m candles (900 bars) and append new candle every 10 minutes.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe='10m', start_time=None, limit=BARS_INIT_10M)\n",
        "        df = pd.DataFrame([{ 'time': pd.to_datetime(c['time'], utc=True), 'open': c['open'], 'high': c['high'], 'low': c['low'], 'close': c['close'], 'volume': c.get('volume'), 'tickVolume': c.get('tickVolume'), 'spread': c.get('spread') } for c in candles])\n",
        "        if not df.empty:\n",
        "            df['time'] = pd.to_datetime(df['time'], utc=True).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            df.to_csv(path, index=False)\n",
        "    while True:\n",
        "        await asyncio.sleep(600)\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe='10m', start_time=None, limit=1)\n",
        "        if candles:\n",
        "            c = candles[-1]\n",
        "            row = pd.DataFrame([{ 'time': pd.to_datetime(c['time'], utc=True), 'open': c['open'], 'high': c['high'], 'low': c['low'], 'close': c['close'], 'volume': c.get('volume'), 'tickVolume': c.get('tickVolume'), 'spread': c.get('spread') }])\n",
        "            row['time'] = pd.to_datetime(row['time'], utc=True).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            if os.path.exists(path):\n",
        "                existing = pd.read_csv(path)\n",
        "                existing_times = pd.to_datetime(existing['time'], errors='coerce')\n",
        "                row_time = pd.to_datetime(row['time']).iloc[0]\n",
        "                if row_time not in existing_times:\n",
        "                    row.to_csv(path, mode='a', header=False, index=False)\n",
        "            else:\n",
        "                row.to_csv(path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W481PrVufZ8Z",
      "metadata": {
        "id": "W481PrVufZ8Z"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "yI6v2q_xfRtg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "yI6v2q_xfRtg",
        "outputId": "d3c13fa4-5629-4a56-8dfc-11b18aab0b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:SubscriptionManager:163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n",
            "ERROR:SubscriptionManager:163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-09T21:53:12.875653] 163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:SubscriptionManager:163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n",
            "ERROR:SubscriptionManager:163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n",
            "ERROR:SubscriptionManager:163d9a57-1f07-4e78-a6af-036efe867c1b:0: Failed to subscribe {\"name\": \"TimeoutException\", \"message\": \"It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\", \"trace\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/subscription_manager.py\\\", line 110, in subscribe_task\\n    await self.subscribe(account_id, instance_number)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1625, in rpc_request\\n    return await self._make_request(account_id, instance_number, request, timeout_in_seconds)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/dist-packages/metaapi_cloud_sdk/clients/metaapi/metaapi_websocket_client.py\\\", line 1696, in _make_request\\n    resolve = await asyncio.wait_for(\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 520, in wait_for\\n    return await fut\\n           ^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 289, in __await__\\n    yield self  # This tells Task to wait for completion.\\n    ^^^^^^^^^^\\n  File \\\"/usr/lib/python3.12/asyncio/tasks.py\\\", line 385, in __wakeup\\n    future.result()\\n  File \\\"/usr/lib/python3.12/asyncio/futures.py\\\", line 202, in result\\n    raise self._exception.with_traceback(self._exception_tb)\\nmetaapi_cloud_sdk.clients.timeout_exception.TimeoutException: It seems like the account 163d9a57-1f07-4e78-a6af-036efe867c1b is not connected to broker yet or SDK settings you use does not match the account region. Please make sure account is connected to broker before retrying the request. Please make sure you pass a region option to MetaApi constructor for Java SDK. Please make sure you do not pass region option to MetaApi constructor for javascript and python SDKs.\\n\"}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3675050402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# solo en notebooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "###############################################################################\n",
        "# EJECUCIÓN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()     # solo en notebooks\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eaf03429",
        "m8CIB8vltFfH",
        "zhTndYVi1TEV"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}