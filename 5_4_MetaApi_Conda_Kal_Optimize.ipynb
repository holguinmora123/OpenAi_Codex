{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1c1fd1",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/holguinmora123/OpenAi_Codex/blob/main/5_4_MetaApi_Conda_Kal_Optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf03429",
   "metadata": {
    "id": "eaf03429"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nu7i62hlGthO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nu7i62hlGthO",
    "outputId": "92bfea75-e8e6-48db-9e0b-fe9446c16e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta-lib in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
      "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from ta-lib) (1.3.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from ta-lib) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta-lib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ta-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Ej4IeD7EeF0t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej4IeD7EeF0t",
    "outputId": "afd576e2-050f-4a13-d4e4-ae0f12ec029d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: metaapi-cloud-sdk in /usr/local/lib/python3.12/dist-packages (29.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
      "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.12.15)\n",
      "Requirement already satisfied: python-engineio<4.0.0,>=3.14.2 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (4.15.0)\n",
      "Requirement already satisfied: iso8601 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.1.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2025.2)\n",
      "Requirement already satisfied: python-socketio<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (4.6.1)\n",
      "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.32.4)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (0.28.1)\n",
      "Requirement already satisfied: metaapi-cloud-copyfactory-sdk<13.0.0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (12.0.0)\n",
      "Requirement already satisfied: metaapi-cloud-metastats-sdk<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (6.0.0)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.20.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (15.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade metaapi-cloud-sdk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda1a01c",
   "metadata": {
    "id": "bda1a01c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import logging\n",
    "import talib as ta\n",
    "\n",
    "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
    "\n",
    "import random\n",
    "import asyncio\n",
    "\n",
    "from metaapi_cloud_sdk import MetaApi\n",
    "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "#from collections.abc import Sequence\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import json, re, traceback, requests\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "try:\n",
    "    from metaapi_cloud_sdk import MetaApi\n",
    "except Exception:\n",
    "    MetaApi = None  # permite importar el módulo incluso sin el SDK instalado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7JweuZy755ym",
   "metadata": {
    "id": "7JweuZy755ym"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "KFfn45ty82qv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFfn45ty82qv",
    "outputId": "734e0eb7-379e-4172-b52c-6687de2b9418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m8CIB8vltFfH",
   "metadata": {
    "id": "m8CIB8vltFfH"
   },
   "source": [
    "# Set_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "EB5RqjoAtFwl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB5RqjoAtFwl",
    "outputId": "5f1ccc18-981d-49b5-f6e9-5dda19605491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Course Folder/Forex/XAUUSD/\n"
     ]
    }
   ],
   "source": [
    "process = 'Train'\n",
    "SYMBOL = 'XAUUSD'\n",
    "\n",
    "root_data = f'/content/drive/MyDrive/Course Folder/Forex/{SYMBOL}/'\n",
    "print(root_data)\n",
    "\n",
    "rolling_window = 100\n",
    "\n",
    "FILE_PATH = 'xauusd_data.csv'\n",
    "\n",
    "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoiY29weWZhY3RvcnktYXBpIiwibWV0aG9kcyI6WyJjb3B5ZmFjdG9yeS1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoibXQtbWFuYWdlci1hcGkiLCJtZXRob2RzIjpbIm10LW1hbmFnZXItYXBpOnJlc3Q6ZGVhbGluZzoqOioiLCJtdC1tYW5hZ2VyLWFwaTpyZXN0OnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJiaWxsaW5nLWFwaSIsIm1ldGhvZHMiOlsiYmlsbGluZy1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTk3NjkyMTMsImV4cCI6MTc2NzU0NTIxM30.icWvqBmO9uUbHubK_Vr95aLr9fvHft_Qv-kH197w9V9JZfVeqoBY96Kf37sc2nI6sjoP_VoMshRRn7hFV1-rn5ve6-TR8wBxVGeX8ny7HcS70CJZzVtIrqjtWdxn4UB5dbTT5dUlNvGKjRLELFYPRcH-kP1YPXqwoS8EYTdh3KgKKJ-wr7B6vJnoPyL87ew80J-5r4yZYWl_c3lLOxoEJzUvVQ41mRwbuTLcVXZB3d9u0nfjP-Uufn43a25lizzvT4MmfP6jt0OL2Lh23M08LL25Zr_spCw7kz-KZAPM300AAIKWQ30Bh9weKGh8gFrUxxofRWhBmPnnGhvGuNk5q5xDoI17yv64t7WJ_VgPzPseK4A5SHM56Ifhh6CNZ-xgd1CyJzwA6oqP1TEVXrrWtkGM3ese2HvIS9sTlRw7sP9fBMCgYWecp98nk9D_3nawg4BxuPciwFyJuY8Afqbwfj1cUKMPsaDgWEkxDsdZ9jL1ZNJf2FXN_rRmyZX5YUw3RLifsU_66e2nsw2YqtN2E_HdugBvBm24KW2gKM0QeWihjfNtuI4Ve7MaaLhON5RfacOCAIhIYB47hZDccNTzvXznYmDhmInNcVNgohYig7F3cP8hETmsJd0TUdRgT80IXta4HLWQQnSCKxoszsBtv4k5US0PCM-5HZxEy7shxhk'\n",
    "#ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'\n",
    "ACCOUNT_ID     = 'c7fd85f7-11c2-4c25-9369-e32f58f48dc7'\n",
    "\n",
    "LOT      = 1.0\n",
    "COMMENT  = \"Kalman\"\n",
    "CANDLE_NUMBER = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dTlwjirFwRfj",
   "metadata": {
    "id": "dTlwjirFwRfj"
   },
   "outputs": [],
   "source": [
    "# cache simple en módulo\n",
    "__CONNECTION_CHECKED = False\n",
    "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
    "\n",
    "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
    "    \"\"\"\n",
    "    Conecta vía RPC y espera sincronización. Lanza excepción si no se logra.\n",
    "    Devuelve (account, rpc_conn).\n",
    "    \"\"\"\n",
    "    api = MetaApi(token)\n",
    "    account = await api.metatrader_account_api.get_account(account_id)\n",
    "\n",
    "    # refrescamos para leer estado/connectionStatus\n",
    "    try:\n",
    "        await account.reload()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Conexión RPC + sincronización del terminal\n",
    "    rpc_conn = account.get_rpc_connection()\n",
    "    await rpc_conn.connect()\n",
    "    await rpc_conn.wait_synchronized()  # espera a que el terminal esté listo\n",
    "\n",
    "    # Sonda rápida para confirmar conectividad real con el terminal\n",
    "    try:\n",
    "        _ = await rpc_conn.get_account_information()\n",
    "    except Exception:\n",
    "        # si falla la sonda, igual devolvemos la conexión (ya sincronizada)\n",
    "        pass\n",
    "\n",
    "    return account, rpc_conn\n",
    "\n",
    "def _run(coro):\n",
    "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
    "    try:\n",
    "        return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "def check_connection_once(token: str, account_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida la conexión y sincronización SOLO la primera vez que se llama.\n",
    "    En llamadas posteriores no vuelve a conectar.\n",
    "    \"\"\"\n",
    "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
    "    if __CONNECTION_CHECKED:\n",
    "        print(\"ℹ️ Conexión ya validada en esta sesión; no se repite.\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
    "        __ACCOUNT_CONN = (account, rpc_conn)\n",
    "        __CONNECTION_CHECKED = True\n",
    "        print(f\"✅ Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
    "        return True\n",
    "    except TimeoutException as e:\n",
    "        print(f\"❌ Timeout esperando sincronización. ¿La cuenta está CONNECTED al broker? Detalle: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ No fue posible validar la conexión. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def _safe_json_dump(value):\n",
    "    try:\n",
    "        return json.dumps(value, indent=2, default=str)\n",
    "    except Exception:\n",
    "        return str(value)\n",
    "\n",
    "def print_order_error_details(ctx: dict, err: Exception):\n",
    "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
    "    print(\"\\n\" + \"✘\" * 70)\n",
    "    print(\"❌ Order failed\")\n",
    "    print(\"• Exception type:\", type(err).__name__)\n",
    "    print(\"• Message       :\", str(err))\n",
    "\n",
    "    # Known useful attributes often present on MetaApi exceptions\n",
    "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
    "        if hasattr(err, attr):\n",
    "            val = getattr(err, attr)\n",
    "            if val:\n",
    "                print(f\"• {attr:12}: {_safe_json_dump(val)}\")\n",
    "\n",
    "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
    "    msg = str(err)\n",
    "    m = re.search(r\"\\{.*\\}\", msg)\n",
    "    if m:\n",
    "        try:\n",
    "            payload = json.loads(m.group(0))\n",
    "            print(\"• parsed_json  :\", _safe_json_dump(payload))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Stack (useful while debugging)\n",
    "    print(\"• traceback    :\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Context of the attempt\n",
    "    print(\"• context      :\", _safe_json_dump(ctx))\n",
    "    print(\"✘\" * 70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "RjShygBIwWRp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjShygBIwWRp",
    "outputId": "1c85d83e-5876-48d7-e8e3-f923e2829e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09T14:40:54.867837] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-a.agiliumtrade.ai shared server.\n",
      "[2025-10-09T14:40:54.871338] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-b.agiliumtrade.ai shared server.\n",
      "[2025-10-09T14:40:55.924610] london:0: MetaApi websocket client connected to the MetaApi server\n",
      "[2025-10-09T14:40:55.936744] london:1: MetaApi websocket client connected to the MetaApi server\n",
      "✅ Conectado y sincronizado con MetaApi. account_id=c7fd85f7-11c2-4c25-9369-e32f58f48dc7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HfxZvvtJeFh1",
   "metadata": {
    "id": "HfxZvvtJeFh1"
   },
   "source": [
    "# Real_Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Sx6TbzBOBzZl",
   "metadata": {
    "id": "Sx6TbzBOBzZl"
   },
   "outputs": [],
   "source": [
    "SYMBOL = \"XAUUSD\"\n",
    "FILE_PATH = 'xauusd_data.csv'\n",
    "\n",
    "FETCH_INTERVAL   = 60\n",
    "time_frame_data  = \"1m\"\n",
    "CANDEL_NUMBER    = 900\n",
    "\n",
    "LOT     = 0.5\n",
    "COMMENT = \"Kalman\"\n",
    "\n",
    "length_1 = 600\n",
    "length_2 = 520\n",
    "length_3 = 710\n",
    "length_4 = 1130\n",
    "\n",
    "smooth_1 = 3\n",
    "smooth_2 = 3\n",
    "smooth_3 = 3\n",
    "smooth_4 = 7\n",
    "\n",
    "INITIAL_SL         = -2\n",
    "FIRST_STEP_ATR     = 0.5\n",
    "GAP_FIRST_STEP_ATR = 2\n",
    "\n",
    "#REGION = \"new-york\"\n",
    "REGION = \"london\"\n",
    "\n",
    "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoiY29weWZhY3RvcnktYXBpIiwibWV0aG9kcyI6WyJjb3B5ZmFjdG9yeS1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoibXQtbWFuYWdlci1hcGkiLCJtZXRob2RzIjpbIm10LW1hbmFnZXItYXBpOnJlc3Q6ZGVhbGluZzoqOioiLCJtdC1tYW5hZ2VyLWFwaTpyZXN0OnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJiaWxsaW5nLWFwaSIsIm1ldGhvZHMiOlsiYmlsbGluZy1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTk3NjkyMTMsImV4cCI6MTc2NzU0NTIxM30.icWvqBmO9uUbHubK_Vr95aLr9fvHft_Qv-kH197w9V9JZfVeqoBY96Kf37sc2nI6sjoP_VoMshRRn7hFV1-rn5ve6-TR8wBxVGeX8ny7HcS70CJZzVtIrqjtWdxn4UB5dbTT5dUlNvGKjRLELFYPRcH-kP1YPXqwoS8EYTdh3KgKKJ-wr7B6vJnoPyL87ew80J-5r4yZYWl_c3lLOxoEJzUvVQ41mRwbuTLcVXZB3d9u0nfjP-Uufn43a25lizzvT4MmfP6jt0OL2Lh23M08LL25Zr_spCw7kz-KZAPM300AAIKWQ30Bh9weKGh8gFrUxxofRWhBmPnnGhvGuNk5q5xDoI17yv64t7WJ_VgPzPseK4A5SHM56Ifhh6CNZ-xgd1CyJzwA6oqP1TEVXrrWtkGM3ese2HvIS9sTlRw7sP9fBMCgYWecp98nk9D_3nawg4BxuPciwFyJuY8Afqbwfj1cUKMPsaDgWEkxDsdZ9jL1ZNJf2FXN_rRmyZX5YUw3RLifsU_66e2nsw2YqtN2E_HdugBvBm24KW2gKM0QeWihjfNtuI4Ve7MaaLhON5RfacOCAIhIYB47hZDccNTzvXznYmDhmInNcVNgohYig7F3cP8hETmsJd0TUdRgT80IXta4HLWQQnSCKxoszsBtv4k5US0PCM-5HZxEy7shxhk'\n",
    "ACCOUNT_ID     = 'c7fd85f7-11c2-4c25-9369-e32f58f48dc7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sKKJrsFBfOoT",
   "metadata": {
    "id": "sKKJrsFBfOoT"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARÁMETROS / DEFAULTS SEGUROS (no rompen si faltan globales)\n",
    "# ============================================================================\n",
    "try:\n",
    "    FILE_PATH  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    FILE_PATH = \"xauusd_data.csv\"\n",
    "\n",
    "try:\n",
    "    SYMBOL  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    SYMBOL = \"XAUUSD\"\n",
    "\n",
    "try:\n",
    "    time_frame_data  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    time_frame_data = \"1m\"\n",
    "\n",
    "try:\n",
    "    CANDEL_NUMBER  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    CANDEL_NUMBER = 100\n",
    "\n",
    "try:\n",
    "    INITIAL_SL  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    INITIAL_SL = -1.0  # múltiplos ATR (negativo para BUY)\n",
    "\n",
    "try:\n",
    "    FIRST_STEP_ATR  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    FIRST_STEP_ATR = 0.5\n",
    "\n",
    "try:\n",
    "    GAP_FIRST_STEP_ATR  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    GAP_FIRST_STEP_ATR = 2.0\n",
    "\n",
    "try:\n",
    "    META_API_TOKEN  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    META_API_TOKEN = \"\"  # pon tu token real\n",
    "\n",
    "try:\n",
    "    ACCOUNT_ID  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    ACCOUNT_ID = \"\"  # pon tu account id\n",
    "\n",
    "try:\n",
    "    REGION  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    REGION = \"new-york\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from typing import Any, Dict, Mapping, Sequence\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\", \"websockets\"):\n",
    "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
    "if \"MetaApi\" in globals() and MetaApi:\n",
    "    try:\n",
    "        MetaApi.enable_logging()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTRO KALMAN + INSTA\n",
    "# ============================================================================\n",
    "\n",
    "def kalman_line(source: pd.Series | Sequence[float], kalman_length: int, smooth: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Núcleo Kalman + EMA con semilla SMA (comportamiento de TradingView ta.ema).\n",
    "    Devuelve una Serie 'kal' indexada como 'source'.\n",
    "    - Warmup: NaN en las primeras 'smooth-1' barras (igual a Pine).\n",
    "    \"\"\"\n",
    "    src = pd.Series(source, dtype=\"float64\").copy()\n",
    "    n = len(src)\n",
    "    kal = pd.Series(np.nan, index=src.index, dtype=\"float64\", name=\"kal\")\n",
    "    if n == 0:\n",
    "        return kal\n",
    "\n",
    "    Lk = max(int(kalman_length), 1)\n",
    "    Ls = max(int(smooth), 1)\n",
    "\n",
    "    # Parámetros como en Pine\n",
    "    sqrt_term   = np.sqrt(Lk / 10000.0 * 2.0)\n",
    "    length_term = Lk / 10000.0\n",
    "\n",
    "    # Núcleo Kalman (kf_c) con la misma lógica nz() de Pine para los previos\n",
    "    kf_c = np.empty(n, dtype=\"float64\")\n",
    "    kf_c[:] = np.nan\n",
    "    velo = np.empty(n, dtype=\"float64\")\n",
    "    velo[:] = np.nan\n",
    "\n",
    "    # Bar 0 (equivalente a: dk=0, velo=0, kf_c = source)\n",
    "    kf_c[0] = float(src.iloc[0])\n",
    "    velo[0] = 0.0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        prev_kf   = kf_c[i-1] if np.isfinite(kf_c[i-1]) else float(src.iloc[i])\n",
    "        prev_velo = velo[i-1] if np.isfinite(velo[i-1]) else 0.0\n",
    "        dk = float(src.iloc[i]) - prev_kf\n",
    "        smooth_c = prev_kf + dk * sqrt_term\n",
    "        velo[i]  = prev_velo + length_term * dk\n",
    "        kf_c[i]  = smooth_c + velo[i]\n",
    "\n",
    "    kf = pd.Series(kf_c, index=src.index)\n",
    "\n",
    "    # ta.ema(kf, Ls) con semilla SMA y warmup NaN\n",
    "    if n >= Ls:\n",
    "        # Semilla SMA\n",
    "        sma0 = float(kf.iloc[:Ls].mean())\n",
    "        kal.iloc[Ls-1] = sma0\n",
    "        alpha = 2.0 / (Ls + 1.0)\n",
    "        for i in range(Ls, n):\n",
    "            kal.iloc[i] = alpha * kf.iloc[i] + (1.0 - alpha) * kal.iloc[i-1]\n",
    "    # Si n < Ls: todo NaN, como en Pine\n",
    "\n",
    "    return kal\n",
    "\n",
    "# ============================================================================\n",
    "# UTILIDADES DE COLUMNAS / CSV\n",
    "# ============================================================================\n",
    "\n",
    "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Crea/normaliza columnas clave con dtypes consistentes y\n",
    "    elimina columnas heredadas ('id', 'actionType') si existieran.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # elimina columnas heredadas\n",
    "    for col in (\"id\", \"actionType\"):\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    col_types = {\n",
    "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
    "        \"orderId\":       \"string\",\n",
    "        \"magic\":         \"Int64\",\n",
    "        \"symbol\":        \"string\",\n",
    "        \"openPrice\":     \"float64\",\n",
    "        \"comment\":       \"string\",\n",
    "        \"Type\":          \"string\",\n",
    "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
    "        \"Stop_Loss_atr\": \"float64\",\n",
    "        \"Stop_Loss_$\":   \"float64\",\n",
    "        \"Take_Profit_atr\": \"float64\",\n",
    "        \"Take_Profit_$\":   \"float64\",\n",
    "        \"Real_SL\":       \"float64\",\n",
    "        \"ATR\":           \"float64\",\n",
    "        \"atr_mult_high\": \"float64\",\n",
    "        \"atr_mult_low\":  \"float64\",\n",
    "        \"trade_size\":    \"float64\",\n",
    "        \"profits\":       \"float64\",\n",
    "        \"base_px\":       \"float64\",\n",
    "        \"atr_base\":      \"float64\",\n",
    "        \"source\":        \"Int64\",\n",
    "        \"time\":          \"datetime64[ns, UTC]\",\n",
    "        \"open\":          \"float64\",\n",
    "        \"high\":          \"float64\",\n",
    "        \"low\":           \"float64\",\n",
    "        \"close\":         \"float64\",\n",
    "        \"volume\":        \"float64\",\n",
    "        \"tickVolume\":    \"float64\",\n",
    "        \"spread\":        \"float64\",\n",
    "    }\n",
    "\n",
    "    for c, dtp in col_types.items():\n",
    "        if c not in df.columns:\n",
    "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
    "                df[c] = pd.NaT\n",
    "            elif dtp == \"Int64\":\n",
    "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
    "            elif dtp == \"string\":\n",
    "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
    "            else:\n",
    "                df[c] = np.nan\n",
    "\n",
    "    # normaliza sin romper datos existentes\n",
    "    for c, dtp in col_types.items():\n",
    "        try:\n",
    "            if dtp == \"string\":\n",
    "                df[c] = df[c].astype(\"string\")\n",
    "            elif dtp == \"Int64\":\n",
    "                df[c] = df[c].astype(\"Int64\")\n",
    "            elif isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
    "                df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "            # floats/num los dejamos tal cual para no forzar conversiones peligrosas\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
    "    \"\"\"\n",
    "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
    "    mode=\"last\": solo la última fila\n",
    "    mode=\"missing\": rellena donde esté NaT/NaN\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    _ensure_order_cols(df)\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
    "    if mode == \"last\":\n",
    "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
    "    else:\n",
    "        mask = df[\"System_time\"].isna()\n",
    "        if mask.any():\n",
    "            df.loc[mask, \"System_time\"] = now_utc\n",
    "\n",
    "\n",
    "def _fmt_dt_cols(df: pd.DataFrame, cols=(\"System_time\", \"time\", \"Entry_Date\")) -> None:\n",
    "    \"\"\"Formatea columnas datetime a string 'YYYY-mm-dd HH:MM:SS' sin tz.\"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
    "    \"\"\"\n",
    "    Reordena columnas y guarda CSV con tiempos formateados.\n",
    "    Orden deseado:\n",
    "      • System_time antes de 'time'\n",
    "      • 'source' a la derecha de 'time' y antes de 'open'\n",
    "      • Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
    "      • Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    _ensure_order_cols(df)\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
    "    _fmt_dt_cols(df_out)\n",
    "\n",
    "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
    "        if \"Entry_Date\" not in cols:\n",
    "            return cols\n",
    "        cols = cols.copy()\n",
    "        cols.remove(\"Entry_Date\")\n",
    "        if \"Stop_Loss_atr\" in cols:\n",
    "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
    "        elif \"Type\" in cols:\n",
    "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
    "        else:\n",
    "            cols.append(\"Entry_Date\")\n",
    "        return cols\n",
    "\n",
    "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
    "        cols = cols.copy()\n",
    "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
    "            if c in cols:\n",
    "                cols.remove(c)\n",
    "        if \"Stop_Loss_$\" in cols:\n",
    "            i = cols.index(\"Stop_Loss_$\") + 1\n",
    "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
    "                if c in df_out.columns:\n",
    "                    cols.insert(i, c)\n",
    "                    i += 1\n",
    "        else:\n",
    "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
    "                if c in df_out.columns and c not in cols:\n",
    "                    cols.append(c)\n",
    "        return cols\n",
    "\n",
    "    def _reorder_source(cols: list[str]) -> list[str]:\n",
    "        cols = cols.copy()\n",
    "        if \"source\" in cols:\n",
    "            cols.remove(\"source\")\n",
    "        if \"time\" in cols:\n",
    "            i = cols.index(\"time\") + 1\n",
    "            cols.insert(i, \"source\")\n",
    "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
    "                cols.remove(\"source\")\n",
    "                cols.insert(cols.index(\"open\"), \"source\")\n",
    "        else:\n",
    "            if \"open\" in cols:\n",
    "                cols.insert(cols.index(\"open\"), \"source\")\n",
    "            else:\n",
    "                cols.append(\"source\")\n",
    "        return cols\n",
    "\n",
    "    cols = [c for c in df_out.columns if c not in (\"id\", \"brokerTime\", \"actionType\")]\n",
    "    if \"time\" in cols:\n",
    "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
    "        i = cols_wo_sys.index(\"time\")\n",
    "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
    "    else:\n",
    "        ordered = cols\n",
    "\n",
    "    ordered = _reorder_source(ordered)\n",
    "    ordered = _reorder_for_entry_date(ordered)\n",
    "    ordered = _reorder_stop_cols(ordered)\n",
    "\n",
    "    # Filtra por columnas existentes para evitar ValueError en to_csv\n",
    "    ordered = [c for c in ordered if c in df_out.columns]\n",
    "    df_out.to_csv(path, index=False, columns=ordered)\n",
    "\n",
    "\n",
    "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
    "    \"\"\"\n",
    "    Migra CSV existente:\n",
    "      • Elimina columnas heredadas.\n",
    "      • Asegura columnas nuevas: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
    "      • Formatea tiempos.\n",
    "      • Reordena columnas al formato actual.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    ensure_cols = {\n",
    "        \"System_time\": pd.NaT,\n",
    "        \"profits\":     np.nan,\n",
    "        \"trade_size\":  np.nan,\n",
    "        \"base_px\":     np.nan,\n",
    "        \"atr_base\":    np.nan,\n",
    "        \"source\":      pd.NA,\n",
    "        \"Real_SL\":     np.nan,\n",
    "    }\n",
    "    for c, default in ensure_cols.items():\n",
    "        if c not in df.columns:\n",
    "            df[c] = default\n",
    "\n",
    "    _fmt_dt_cols(df)\n",
    "\n",
    "    # Reusar lógica de save_csv para reordenar\n",
    "    save_csv(df, path=path)\n",
    "\n",
    "\n",
    "def _load_csv(path: str = FILE_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware y elimina columnas heredadas.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        dtype={\n",
    "            \"orderId\": \"string\",\n",
    "            \"symbol\":  \"string\",\n",
    "            \"comment\": \"string\",\n",
    "            \"Type\":    \"string\",\n",
    "        }\n",
    "    )\n",
    "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "    _ensure_order_cols(df)\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# METAAPI: CONEXIÓN Y DATOS\n",
    "# ============================================================================\n",
    "\n",
    "import asyncio, datetime as dt\n",
    "\n",
    "async def connect_metaapi(token: str, account_id: str, *, rpc_timeout=60, retries=3):\n",
    "    \"\"\"\n",
    "    Devuelve (account) con RPC intentado. Si la sincronización falla (DNS / timeout),\n",
    "    seguimos en modo REST con el mismo 'account' (las llamadas RPC tendrán fallback).\n",
    "    \"\"\"\n",
    "    if \"MetaApi\" not in globals() or MetaApi is None:\n",
    "        raise RuntimeError(\"metaapi_cloud_sdk no está disponible en el entorno.\")\n",
    "\n",
    "    api = MetaApi(token)\n",
    "    account = await api.metatrader_account_api.get_account(account_id)\n",
    "\n",
    "    # Asegurar despliegue/conexión de la cuenta (no falla si el SDK no expone algo)\n",
    "    try:\n",
    "        await account.reload()\n",
    "        if getattr(account, \"state\", \"\").upper() != \"DEPLOYED\":\n",
    "            await account.deploy()\n",
    "            if hasattr(account, \"wait_deployed\"):\n",
    "                await account.wait_deployed()\n",
    "        if hasattr(account, \"wait_connected\"):\n",
    "            await account.wait_connected()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    conn = account.get_rpc_connection()\n",
    "\n",
    "    # Intentos de conectar/sincronizar RPC con backoff\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            await conn.connect()\n",
    "            try:\n",
    "                # según versión del SDK, wait_synchronized puede o no aceptar timeout\n",
    "                await asyncio.wait_for(conn.wait_synchronized(), timeout=rpc_timeout)\n",
    "            except TypeError:\n",
    "                await conn.wait_synchronized()\n",
    "            except asyncio.TimeoutError:\n",
    "                raise TimeoutError(\"RPC wait_synchronized timeout\")\n",
    "            return account  # RPC OK\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                logging.warning(\"RPC no sincronizó (%s). Continuando con REST-only; habrá fallbacks.\", e)\n",
    "                return account\n",
    "            await asyncio.sleep(min(5 * attempt, 15))\n",
    "\n",
    "    return account\n",
    "\n",
    "\n",
    "async def get_current_candle_snapshot(account,\n",
    "                                      rpc_conn,\n",
    "                                      symbol: str = SYMBOL,\n",
    "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve 1 fila con la vela más reciente (puede ser la vela en curso).\n",
    "    \"\"\"\n",
    "    def _to_row(c: dict) -> dict:\n",
    "        return {\n",
    "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
    "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
    "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
    "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
    "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
    "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
    "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
    "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
    "        }\n",
    "\n",
    "    # 1) RPC\n",
    "    try:\n",
    "        try:\n",
    "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe, limit=1)\n",
    "        except TypeError:\n",
    "            to_ts = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
    "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe,\n",
    "                                                 start_time=from_ts, end_time=to_ts)\n",
    "        if candles:\n",
    "            return pd.DataFrame([_to_row(candles[-1])])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Histórico como respaldo\n",
    "    try:\n",
    "        candles = await account.get_historical_candles(symbol=symbol, timeframe=timeframe, start_time=None, limit=1)\n",
    "        if candles:\n",
    "            return pd.DataFrame([_to_row(candles[-1])])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
    "\n",
    "\n",
    "async def get_candles_5m(account,\n",
    "                         start: dt.datetime | None,\n",
    "                         limit: int = CANDEL_NUMBER,\n",
    "                         *,\n",
    "                         retries: int = 5,\n",
    "                         retry_delay: float = 5.0) -> pd.DataFrame:\n",
    "    \"\"\"Descarga velas históricas (usa time_frame_data configurado) con reintentos.\"\"\"\n",
    "    columns = [\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"]\n",
    "    last_err: Exception | None = None\n",
    "    max_retries = max(int(retries), 1)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            candles = await account.get_historical_candles(symbol=SYMBOL,\n",
    "                                                            timeframe=time_frame_data,\n",
    "                                                            start_time=start,\n",
    "                                                            limit=limit)\n",
    "            rows = [{\n",
    "                \"time\": pd.to_datetime(c.get(\"time\"), utc=True),\n",
    "                \"open\": c.get(\"open\"),\n",
    "                \"high\": c.get(\"high\"),\n",
    "                \"low\": c.get(\"low\"),\n",
    "                \"close\": c.get(\"close\"),\n",
    "                \"volume\": c.get(\"volume\"),\n",
    "                \"tickVolume\": c.get(\"tickVolume\"),\n",
    "                \"spread\": c.get(\"spread\")\n",
    "            } for c in (candles or [])]\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "        except Exception as exc:\n",
    "            last_err = exc\n",
    "            logging.warning(\"Fallo al descargar velas (intento %s/%s): %s\", attempt, max_retries, exc)\n",
    "            try:\n",
    "                await account.reload()\n",
    "                if hasattr(account, \"wait_connected\"):\n",
    "                    await account.wait_connected()\n",
    "            except Exception:\n",
    "                pass\n",
    "            if attempt < max_retries:\n",
    "                await asyncio.sleep(min(retry_delay * attempt, 30))\n",
    "\n",
    "    if last_err is not None:\n",
    "        logging.error(\"No se pudieron obtener velas tras %s intentos: %s\", max_retries, last_err)\n",
    "        raise last_err\n",
    "\n",
    "    return pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TIMEFRAME SYNC\n",
    "# ============================================================================\n",
    "def seconds_until_next_tf(tf: str = \"5m\", *, offset_sec: int = 3) -> float:\n",
    "    \"\"\"\n",
    "    Segundos hasta el próximo cierre de vela del timeframe tf (+offset).\n",
    "    Soporta sufijos 'm','h','d'. Ej: '1m','5m','15m','1h','4h','1d'.\n",
    "    \"\"\"\n",
    "    tf = (tf or \"5m\").strip().lower()\n",
    "    if tf.endswith(\"mn\"):\n",
    "        tf = tf[:-2] + \"m\"\n",
    "    try:\n",
    "        if tf.endswith(\"m\"):\n",
    "            step = int(tf[:-1])\n",
    "            delta = dt.timedelta(minutes=max(step, 1))\n",
    "        elif tf.endswith(\"h\"):\n",
    "            step = int(tf[:-1])\n",
    "            delta = dt.timedelta(hours=max(step, 1))\n",
    "        elif tf.endswith(\"d\"):\n",
    "            step = int(tf[:-1])\n",
    "            delta = dt.timedelta(days=max(step, 1))\n",
    "        else:\n",
    "            delta = dt.timedelta(minutes=1)\n",
    "    except Exception:\n",
    "        delta = dt.timedelta(minutes=1)\n",
    "\n",
    "    now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "    epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
    "    secs = int((now - epoch).total_seconds())\n",
    "    stepS = int(delta.total_seconds())\n",
    "\n",
    "    next_boundary = ((secs // stepS) + 1) * stepS\n",
    "    target = epoch + dt.timedelta(seconds=next_boundary + max(offset_sec, 0))\n",
    "    wait_s = (target - now).total_seconds()\n",
    "    return max(wait_s, 0.5)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SQUEEZE MOMENTUM (LazyBear port)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def _true_range(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n",
    "    \"\"\"True Range como en Pine: max( high-low, abs(high-prev_close), abs(low-prev_close) ).\"\"\"\n",
    "    prev_close = close.shift(1)\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low - prev_close).abs()\n",
    "    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "\n",
    "def _rolling_stdev(s: pd.Series, window: int, ddof: int = 1) -> pd.Series:\n",
    "    \"\"\"Desviación estándar por ventana. En Pine, stdev es muestral; usa ddof=1 por defecto.\"\"\"\n",
    "    return s.rolling(window, min_periods=window).std(ddof=ddof)\n",
    "\n",
    "\n",
    "def _rolling_linreg_endpoint(y: pd.Series, length: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Emula ta.linreg(src, length, offset=0) de Pine:\n",
    "    y_hat en x = length-1 (el extremo de la ventana) para cada ventana deslizante.\n",
    "    \"\"\"\n",
    "    if length <= 1:\n",
    "        return y.copy()\n",
    "\n",
    "    x = np.arange(length, dtype=float)\n",
    "    mean_x = x.mean()\n",
    "    denom = ((x - mean_x) ** 2).sum()\n",
    "\n",
    "    def linreg_last(arr: np.ndarray) -> float:\n",
    "        yy = arr.astype(float)\n",
    "        mean_y = yy.mean()\n",
    "        slope = ((x - mean_x) * (yy - mean_y)).sum() / denom\n",
    "        intercept = mean_y - slope * mean_x\n",
    "        return intercept + slope * (length - 1)\n",
    "\n",
    "    return y.rolling(length, min_periods=length).apply(linreg_last, raw=True)\n",
    "\n",
    "\n",
    "def squeeze_momentum_lb(\n",
    "    df: pd.DataFrame,\n",
    "    length: int = 20,\n",
    "    mult: float = 2.0,\n",
    "    length_kc: int = 20,\n",
    "    mult_kc: float = 1.5,\n",
    "    use_true_range: bool = True,\n",
    "    ddof_stdev: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Puerto a Python del indicador \"Squeeze Momentum [LazyBear]\" (Pine).\n",
    "    Espera un DataFrame con columnas: 'high', 'low', 'close'. (open no es necesario)\n",
    "\n",
    "    Devuelve un DataFrame con:\n",
    "      - basis, upperBB, lowerBB\n",
    "      - maKC, range, rangeMA, upperKC, lowerKC\n",
    "      - sqzOn, sqzOff, noSqz\n",
    "      - val (serie principal tipo histograma en Pine)\n",
    "      - val_up, val_up_accel, val_down, val_down_accel\n",
    "      - zero_color_noSqz, zero_color_sqzOn, zero_color_else\n",
    "    \"\"\"\n",
    "    close = df[\"close\"].astype(float)\n",
    "    high = df[\"high\"].astype(float)\n",
    "    low = df[\"low\"].astype(float)\n",
    "\n",
    "    basis = close.rolling(length, min_periods=length).mean()\n",
    "    dev = mult_kc * _rolling_stdev(close, length, ddof=ddof_stdev)\n",
    "    upperBB = basis + dev\n",
    "    lowerBB = basis - dev\n",
    "\n",
    "    maKC = close.rolling(length_kc, min_periods=length_kc).mean()\n",
    "    if use_true_range:\n",
    "        rng = _true_range(high, low, close)\n",
    "    else:\n",
    "        rng = high - low\n",
    "    rangeMA = rng.rolling(length_kc, min_periods=length_kc).mean()\n",
    "    upperKC = maKC + rangeMA * mult_kc\n",
    "    lowerKC = maKC - rangeMA * mult_kc\n",
    "\n",
    "    sqzOn = (lowerBB > lowerKC) & (upperBB < upperKC)\n",
    "    sqzOff = (lowerBB < lowerKC) & (upperBB > upperKC)\n",
    "    noSqz = (~sqzOn) & (~sqzOff)\n",
    "\n",
    "    highest_high = high.rolling(length_kc, min_periods=length_kc).max()\n",
    "    lowest_low = low.rolling(length_kc, min_periods=length_kc).min()\n",
    "    mid_hl = (highest_high + lowest_low) / 2.0\n",
    "    sma_close_kc = close.rolling(length_kc, min_periods=length_kc).mean()\n",
    "\n",
    "    baseline = (mid_hl + sma_close_kc) / 2.0\n",
    "    src_adj = close - baseline\n",
    "\n",
    "    val = _rolling_linreg_endpoint(src_adj, length_kc)\n",
    "\n",
    "    val_prev = val.shift(1).fillna(0.0)\n",
    "\n",
    "    val_pos = val > 0\n",
    "    val_increasing = val > val_prev\n",
    "    val_decreasing = val < val_prev\n",
    "\n",
    "    val_up_accel = val_pos & val_increasing\n",
    "    val_up = val_pos & val_decreasing\n",
    "    val_down = ~val_pos & val_decreasing\n",
    "    val_down_accel = ~val_pos & val_increasing\n",
    "\n",
    "    zero_color_noSqz = noSqz\n",
    "    zero_color_sqzOn = sqzOn\n",
    "    zero_color_else = (~noSqz) & (~sqzOn)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"basis\": basis,\n",
    "        \"upperBB\": upperBB,\n",
    "        \"lowerBB\": lowerBB,\n",
    "        \"maKC\": maKC,\n",
    "        \"range\": rng,\n",
    "        \"rangeMA\": rangeMA,\n",
    "        \"upperKC\": upperKC,\n",
    "        \"lowerKC\": lowerKC,\n",
    "        \"sqzOn\": sqzOn,\n",
    "        \"sqzOff\": sqzOff,\n",
    "        \"noSqz\": noSqz,\n",
    "        \"val\": val,\n",
    "        \"val_up_accel\": val_up_accel,\n",
    "        \"val_up\": val_up,\n",
    "        \"val_down\": val_down,\n",
    "        \"val_down_accel\": val_down_accel,\n",
    "        \"zero_color_noSqz\": zero_color_noSqz,\n",
    "        \"zero_color_sqzOn\": zero_color_sqzOn,\n",
    "        \"zero_color_else\": zero_color_else,\n",
    "    }, index=df.index)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SEÑALES / KALMAN\n",
    "# ============================================================================\n",
    "def generate_trade_signals(\n",
    "    df: pd.DataFrame,\n",
    "    length_1: int,\n",
    "    length_2: int,\n",
    "    length_3: int,\n",
    "    length_4: int,\n",
    "    smooth_1: int,\n",
    "    smooth_2: int,\n",
    "    smooth_3: int,\n",
    "    smooth_4: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula 4 líneas de Kalman sobre 'close' y crea:\n",
    "      • kal_1, kal_2, kal_3, kal_4\n",
    "      • Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
    "      • Close_Trade: -1 si kal_4 < kal_4.shift()  → cierra BUY\n",
    "                     +1 si kal_4 > kal_4.shift()  → cierra SELL\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    if \"close\" not in df.columns:\n",
    "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
    "\n",
    "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").ffill()\n",
    "\n",
    "    def _clamp_int(x, mn=1):\n",
    "        try:\n",
    "            x = int(x)\n",
    "        except Exception:\n",
    "            x = mn\n",
    "        return max(x, mn)\n",
    "\n",
    "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
    "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
    "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
    "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
    "\n",
    "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
    "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
    "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
    "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
    "\n",
    "    if not {\"high\", \"low\"}.issubset(df.columns):\n",
    "        raise ValueError(\"generate_trade_signals: faltan columnas 'high'/'low'.\")\n",
    "    sqz_input = df[[\"high\", \"low\", \"close\"]].copy()\n",
    "    sqz_res = squeeze_momentum_lb(\n",
    "        sqz_input,\n",
    "        length=20,\n",
    "        mult=2.0,\n",
    "        length_kc=20,\n",
    "        mult_kc=1.5,\n",
    "        use_true_range=True,\n",
    "    )\n",
    "    df[\"sqz_val\"] = pd.to_numeric(sqz_res[\"val\"], errors=\"coerce\")\n",
    "\n",
    "    k1_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1)\n",
    "    k2_up = df[\"kal_2\"] > df[\"kal_2\"].shift(1)\n",
    "    k3_up = df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
    "    k1_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1)\n",
    "    k2_dn = df[\"kal_2\"] < df[\"kal_2\"].shift(1)\n",
    "    k3_dn = df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
    "\n",
    "    bull = k1_up & k2_up & k3_up\n",
    "    bear = k1_dn & k2_dn & k3_dn\n",
    "    aux = np.where(bull, 1.0, np.where(bear, -1.0, np.nan))\n",
    "    val = df[\"sqz_val\"]\n",
    "    allowed_dir = np.where(val > 0, 1.0, np.where(val < 0, -1.0, np.nan))\n",
    "    aux_gated = np.where(np.isnan(allowed_dir), aux, np.nan)\n",
    "    aux_gated = np.where(allowed_dir == 1.0, np.where(aux == 1.0, 1.0, np.nan), aux_gated)\n",
    "    aux_gated = np.where(allowed_dir == -1.0, np.where(aux == -1.0, -1.0, np.nan), aux_gated)\n",
    "    aux_series = pd.Series(aux_gated, index=df.index, dtype=\"float64\")\n",
    "    df[\"Open_Trade\"] = aux_series.where(aux_series.ne(aux_series.shift(1)), np.nan)\n",
    "\n",
    "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)\n",
    "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)\n",
    "    close_raw = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
    "    close_sr = pd.Series(close_raw)\n",
    "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REST HELPERS (MetaApi REST)\n",
    "# ============================================================================\n",
    "def _rest_place_order(auth_token: str,\n",
    "                      account_id: str,\n",
    "                      region: str,\n",
    "                      symbol: str,\n",
    "                      side: str,\n",
    "                      volume: float,\n",
    "                      comment: str = \"Kal\",\n",
    "                      magic: int | None = None,\n",
    "                      stop_loss: float | None = None,\n",
    "                      take_profit: float | None = None,\n",
    "                      timeout: int = 20):\n",
    "    side = side.upper().strip()\n",
    "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
    "    if side not in action_map:\n",
    "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
    "\n",
    "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"symbol\": symbol,\n",
    "        \"actionType\": action_map[side],\n",
    "        \"volume\": float(volume),\n",
    "        \"comment\": str(comment)\n",
    "    }\n",
    "    if magic is not None:\n",
    "        try:\n",
    "            payload[\"magic\"] = int(magic)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if stop_loss is not None:\n",
    "        try:\n",
    "            payload[\"stopLoss\"] = float(stop_loss)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if take_profit is not None:\n",
    "        try:\n",
    "            payload[\"takeProfit\"] = float(take_profit)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
    "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "\n",
    "\n",
    "def _rest_get_positions(auth_token: str,\n",
    "                        account_id: str,\n",
    "                        region: str,\n",
    "                        symbol: str | None = None,\n",
    "                        timeout: int = 15):\n",
    "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
    "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
    "    params = {}\n",
    "    if symbol:\n",
    "        params[\"symbol\"] = str(symbol)\n",
    "    try:\n",
    "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
    "    except Exception as e:\n",
    "        class _Dummy:\n",
    "            status_code = 0\n",
    "            def json(self): return {\"error\": str(e)}\n",
    "            text = str(e)\n",
    "        return _Dummy()\n",
    "\n",
    "\n",
    "def _rest_modify_position(auth_token: str,\n",
    "                          account_id: str,\n",
    "                          region: str,\n",
    "                          position_id: str,\n",
    "                          stop_loss: float | None = None,\n",
    "                          take_profit: float | None = None,\n",
    "                          timeout: int = 20):\n",
    "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
    "    payload: Dict[str, Any] = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
    "    if stop_loss is not None:\n",
    "        payload[\"stopLoss\"] = float(stop_loss)\n",
    "    if take_profit is not None:\n",
    "        payload[\"takeProfit\"] = float(take_profit)\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
    "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "\n",
    "# --- REST fallback to close a position --------------------------------------\n",
    "def _rest_close_position(auth_token: str,\n",
    "                         account_id: str,\n",
    "                         region: str,\n",
    "                         position_id: str,\n",
    "                         timeout: int = 20):\n",
    "    \"\"\"\n",
    "    Cierra una posición por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
    "    \"\"\"\n",
    "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
    "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
    "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "\n",
    "\n",
    "# --- Position helpers -------------------------------------------------------\n",
    "_MAGIC_COMMENT_RE = re.compile(r\"magic\\s*=\\s*(\\d+)\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _split_comment_and_magic(comment: str | None) -> tuple[str, int | None]:\n",
    "    text = str(comment or \"\")\n",
    "    clean = text.split(\"|\", 1)[0].strip()\n",
    "    match = _MAGIC_COMMENT_RE.search(text)\n",
    "    magic_val = int(match.group(1)) if match else None\n",
    "    return clean, magic_val\n",
    "\n",
    "\n",
    "def _coerce_int(value: Any) -> int | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(value)\n",
    "    except Exception:\n",
    "        pass\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    if text.isdigit() or (text.startswith(\"-\") and text[1:].isdigit()):\n",
    "        return int(text)\n",
    "    try:\n",
    "        num = float(text)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return int(num) if float(num).is_integer() else None\n",
    "\n",
    "\n",
    "def _position_id(position: Mapping[str, Any]) -> str:\n",
    "    for key in (\"id\", \"positionId\", \"position_id\"):\n",
    "        value = position.get(key)\n",
    "        if value not in (None, \"\"):\n",
    "            return str(value)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _position_magic_number(position: Mapping[str, Any]) -> int | None:\n",
    "    for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\", \"expertMagicId\", \"magicNumber\"):\n",
    "        value = position.get(key)\n",
    "        coerced = _coerce_int(value)\n",
    "        if coerced is not None:\n",
    "            return coerced\n",
    "    _, magic_from_comment = _split_comment_and_magic(position.get(\"comment\") or position.get(\"brokerComment\"))\n",
    "    return magic_from_comment\n",
    "\n",
    "\n",
    "def _position_magic_matches(position: Mapping[str, Any], magic: int | None) -> bool:\n",
    "    coerced = _coerce_int(magic)\n",
    "    if coerced is None:\n",
    "        return False\n",
    "    pos_magic = _position_magic_number(position)\n",
    "    if pos_magic is not None:\n",
    "        return pos_magic == coerced\n",
    "    comment = str(position.get(\"comment\") or \"\")\n",
    "    return f\"magic={coerced}\".lower() in comment.lower()\n",
    "\n",
    "\n",
    "def _position_symbol_matches(position: Mapping[str, Any], symbol: str | None) -> bool:\n",
    "    if not symbol:\n",
    "        return True\n",
    "    value = position.get(\"symbol\") or position.get(\"Symbol\")\n",
    "    return str(value or \"\").upper() == str(symbol).upper()\n",
    "\n",
    "\n",
    "def _position_side(position: Mapping[str, Any]) -> str:\n",
    "    value = position.get(\"type\")\n",
    "    if isinstance(value, str):\n",
    "        up = value.upper()\n",
    "        if \"BUY\" in up:\n",
    "            return \"BUY\"\n",
    "        if \"SELL\" in up:\n",
    "            return \"SELL\"\n",
    "    elif value == 0:\n",
    "        return \"BUY\"\n",
    "    elif value == 1:\n",
    "        return \"SELL\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
    "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
    "    positions = []\n",
    "    # RPC con timeout corto para evitar cuelgues cuando el subscribe falla\n",
    "    async def _rpc_try(fn, *args, **kwargs):\n",
    "        try:\n",
    "            return await asyncio.wait_for(fn(*args, **kwargs), timeout=4)\n",
    "        except asyncio.CancelledError:\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    if rpc_conn:\n",
    "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
    "        if not positions:\n",
    "            positions = await _rpc_try(rpc_conn.get_positions)\n",
    "\n",
    "    if not positions:\n",
    "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
    "        if getattr(r, \"status_code\", 0) == 200:\n",
    "            try:\n",
    "                positions = r.json() or []\n",
    "            except Exception:\n",
    "                positions = []\n",
    "    return positions\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# OPERATIVA: CLOSE / ORDER\n",
    "# ============================================================================\n",
    "\n",
    "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
    "async def close_order(df: pd.DataFrame,\n",
    "                      rpc_conn,\n",
    "                      symbol: str = SYMBOL,\n",
    "                      magic: int = 900001,\n",
    "                      close_col: str = \"Close_Trade\") -> None:\n",
    "    if df.empty or close_col not in df.columns:\n",
    "        return\n",
    "\n",
    "    sig = df[close_col].iloc[-1]\n",
    "    if not np.isfinite(sig):\n",
    "        return\n",
    "\n",
    "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
    "\n",
    "    try:\n",
    "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "    except Exception as e:\n",
    "        print(\"✘ No se pudieron leer posiciones:\", e)\n",
    "        return\n",
    "    if not positions:\n",
    "        return\n",
    "\n",
    "    async def _try_close_rpc(pid: str) -> bool:\n",
    "        try:\n",
    "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
    "            return True\n",
    "        except Exception:\n",
    "            try:\n",
    "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
    "                return True\n",
    "            except Exception:\n",
    "                return False\n",
    "\n",
    "    for position in positions:\n",
    "        if not _position_magic_matches(position, magic):\n",
    "            continue\n",
    "        pid = _position_id(position)\n",
    "        side = _position_side(position)\n",
    "        if not pid or side not in sides_to_close:\n",
    "            continue\n",
    "\n",
    "        ok = await _try_close_rpc(pid)\n",
    "        if not ok:\n",
    "            # REST fallback\n",
    "            resp = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
    "            ok = getattr(resp, \"status_code\", 0) == 200\n",
    "\n",
    "        if ok:\n",
    "            print(f\"✅ Cerrada {side} positionId={pid} (magic={magic})\")\n",
    "        else:\n",
    "            print(f\"✘ No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPERATIVA: ABRIR / CERRAR / SINCRONIZAR / SL DINÁMICO\n",
    "# ============================================================================\n",
    "async def open_trade(df: pd.DataFrame,\n",
    "                     rpc_conn,\n",
    "                     symbol: str = SYMBOL,\n",
    "                     lot: float = 1.0,\n",
    "                     comment: str = \"Kal\",\n",
    "                     magic: int = 900001):\n",
    "    \"\"\"\n",
    "    Abre mercado SOLO si NO hay posición con ese magic.\n",
    "    Al abrir, calcula y envía el SL inicial:\n",
    "       SL = close ± ATR * INITIAL_SL  (según BUY/SELL; INITIAL_SL suele ser negativo)\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty or \"Open_Trade\" not in df.columns:\n",
    "        return\n",
    "\n",
    "    _ensure_order_cols(df)\n",
    "\n",
    "    try:\n",
    "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "    except Exception:\n",
    "        positions = []\n",
    "    open_with_magic = [p for p in positions if _position_magic_matches(p, magic)]\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
    "    row = df.index[-1]\n",
    "\n",
    "    # Ya hay posición con este magic → sincroniza y sale\n",
    "    if open_with_magic:\n",
    "        pos = open_with_magic[0]\n",
    "        df.at[row, \"System_time\"] = now_utc\n",
    "\n",
    "        clean_cmt, mag_from_cmt = _split_comment_and_magic(pos.get(\"comment\") or pos.get(\"brokerComment\"))\n",
    "        pos_magic = _position_magic_number(pos)\n",
    "        if pos_magic is not None:\n",
    "            df.at[row, \"magic\"] = int(pos_magic)\n",
    "        elif mag_from_cmt is not None:\n",
    "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
    "        else:\n",
    "            df.at[row, \"magic\"] = int(magic)\n",
    "\n",
    "        df.at[row, \"symbol\"] = str(pos.get(\"symbol\") or symbol)\n",
    "        df.at[row, \"openPrice\"] = float(pos.get(\"openPrice\") or pos.get(\"price\") or np.nan)\n",
    "        df.at[row, \"comment\"] = clean_cmt or str(comment)\n",
    "\n",
    "        volume_val = pos.get(\"volume\") or pos.get(\"lots\")\n",
    "        if volume_val is not None:\n",
    "            try:\n",
    "                df.at[row, \"trade_size\"] = float(volume_val)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        side = _position_side(pos)\n",
    "        if side:\n",
    "            df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
    "        if pd.isna(df.at[row, \"Entry_Date\"]):\n",
    "            df.at[row, \"Entry_Date\"] = now_utc\n",
    "\n",
    "        save_csv(df)\n",
    "        print(\"ℹ Position already open; synced last row and skipped new order.\")\n",
    "        return\n",
    "\n",
    "    # No hay posición → decidir lado por Open_Trade\n",
    "    sig = df[\"Open_Trade\"].iloc[-1]\n",
    "    if not np.isfinite(sig):\n",
    "        return\n",
    "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
    "    if side_req is None:\n",
    "        return\n",
    "\n",
    "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
    "    atr_val = float(df[\"ATR\"].iloc[-1]) if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
    "    atr_mult = 1.0\n",
    "\n",
    "    sl_to_send = tp_to_send = None\n",
    "    if np.isfinite(prev_close) and np.isfinite(atr_val):\n",
    "        dist = atr_val * atr_mult\n",
    "        if side_req == \"BUY\":\n",
    "            sl_to_send = prev_close - dist\n",
    "            tp_to_send = prev_close + dist\n",
    "        else:\n",
    "            sl_to_send = prev_close + dist\n",
    "            tp_to_send = prev_close - dist\n",
    "    loop = asyncio.get_running_loop()\n",
    "    resp = await loop.run_in_executor(\n",
    "        None,\n",
    "        lambda: _rest_place_order(\n",
    "            auth_token=META_API_TOKEN,\n",
    "            account_id=ACCOUNT_ID,\n",
    "            region=REGION,\n",
    "            symbol=symbol,\n",
    "            side=side_req,\n",
    "            volume=float(lot),\n",
    "            comment=str(comment),\n",
    "            magic=int(magic),\n",
    "            stop_loss=sl_to_send,\n",
    "            take_profit=tp_to_send,\n",
    "            timeout=20\n",
    "        )\n",
    "    )\n",
    "    if getattr(resp, \"status_code\", 0) != 200:\n",
    "        try: err = resp.json()\n",
    "        except Exception: err = {\"raw\": getattr(resp, \"text\", \"\")[:500]}\n",
    "        print(\"✘ Order failed\", getattr(resp, \"status_code\", None), json.dumps(err, indent=2, ensure_ascii=False))\n",
    "        return\n",
    "\n",
    "    data = resp.json()\n",
    "    order_id = str(data.get(\"orderId\") or \"\")\n",
    "    position_id = str(data.get(\"positionId\") or \"\")\n",
    "    if position_id and (sl_to_send is not None or tp_to_send is not None):\n",
    "        resp_mod = await loop.run_in_executor(\n",
    "            None,\n",
    "            lambda: _rest_modify_position(\n",
    "                auth_token=META_API_TOKEN,\n",
    "                account_id=ACCOUNT_ID,\n",
    "                region=REGION,\n",
    "                position_id=position_id,\n",
    "                stop_loss=sl_to_send,\n",
    "                take_profit=tp_to_send,\n",
    "                timeout=15\n",
    "            )\n",
    "        )\n",
    "        if getattr(resp_mod, \"status_code\", 0) != 200:\n",
    "            try:\n",
    "                err_mod = resp_mod.json()\n",
    "            except Exception:\n",
    "                err_mod = {\"raw\": getattr(resp_mod, \"text\", \"\")[:500]}\n",
    "            print(\"✘ Modify failed\", getattr(resp_mod, \"status_code\", None), json.dumps(err_mod, indent=2, ensure_ascii=False))\n",
    "\n",
    "    df.at[row, \"System_time\"] = now_utc\n",
    "    df.at[row, \"orderId\"]     = order_id\n",
    "    df.at[row, \"magic\"]       = int(magic)\n",
    "    df.at[row, \"symbol\"]      = symbol\n",
    "    df.at[row, \"comment\"]     = str(comment)\n",
    "    df.at[row, \"Entry_Date\"]  = now_utc\n",
    "    df.at[row, \"trade_size\"]  = float(lot)\n",
    "\n",
    "    if sl_to_send is not None and np.isfinite(sl_to_send):\n",
    "        df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
    "        df.at[row, \"Stop_Loss_atr\"] = float(atr_mult)\n",
    "    if tp_to_send is not None and np.isfinite(tp_to_send):\n",
    "        df.at[row, \"Take_Profit_$\"]   = float(tp_to_send)\n",
    "        df.at[row, \"Take_Profit_atr\"] = float(atr_mult)\n",
    "    # Recuperar openPrice y Type desde API\n",
    "    open_price, fetched_typ = np.nan, None\n",
    "    if rpc_conn:\n",
    "        try:\n",
    "            for _ in range(60):\n",
    "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "                match = None\n",
    "                for p in (pos_list or []):\n",
    "                    pid = _position_id(p)\n",
    "                    if pid == position_id or pid == order_id or _position_magic_matches(p, magic):\n",
    "                        match = p\n",
    "                        break\n",
    "                if match:\n",
    "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
    "                    if val is not None: open_price = float(val)\n",
    "                    side_val = _position_side(match)\n",
    "                    if side_val == \"BUY\":\n",
    "                        fetched_typ = \"Long\"\n",
    "                    elif side_val == \"SELL\":\n",
    "                        fetched_typ = \"Short\"\n",
    "                    break\n",
    "                await asyncio.sleep(0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not np.isfinite(open_price):\n",
    "        try: open_price = float(df.at[row, \"close\"])\n",
    "        except Exception: open_price = np.nan\n",
    "\n",
    "    df.at[row, \"openPrice\"] = open_price\n",
    "    df.at[row, \"Type\"] = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
    "\n",
    "    save_csv(df)\n",
    "    print(f\"✅ {side_req} placed | orderId={order_id} positionId={position_id} \"\n",
    "          f\"openPrice={open_price if np.isfinite(open_price) else 'NaN'} \"\n",
    "          f\"| SL sent={sl_to_send if sl_to_send is not None else 'None'} \"\n",
    "          f\"| TP sent={tp_to_send if tp_to_send is not None else 'None'} \"\n",
    "          f\"| Type={df.at[row,'Type']}\")\n",
    "\n",
    "\n",
    "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Por bloque de trade:\n",
    "      • FFill de metadatos\n",
    "      • base_px / atr_base desde la apertura del bloque\n",
    "      • atr_mult_high/low\n",
    "      • 'profits' solo si está NaN\n",
    "      • Propaga 'Real_SL' dentro del bloque con el último valor no nulo\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    for c in ('time', 'Entry_Date'):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
    "\n",
    "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "        else:\n",
    "            df[c].values[:] = np.nan\n",
    "\n",
    "    if 'profits' not in df.columns:\n",
    "        df['profits'] = np.nan\n",
    "    if 'base_px' not in df.columns:\n",
    "        df['base_px'] = np.nan\n",
    "    if 'atr_base' not in df.columns:\n",
    "        df['atr_base'] = np.nan\n",
    "    if 'Real_SL' not in df.columns:\n",
    "        df['Real_SL'] = np.nan\n",
    "\n",
    "    starts_mask = pd.Series(False, index=df.index)\n",
    "    if 'orderId' in df.columns:\n",
    "        oid = df['orderId']\n",
    "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
    "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
    "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
    "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
    "    if not starts_mask.any():\n",
    "        return df\n",
    "\n",
    "    groups = starts_mask.cumsum()\n",
    "    trade_ids = groups[starts_mask].unique()\n",
    "\n",
    "    meta_cols = [\"orderId\", \"magic\", \"symbol\", \"openPrice\", \"comment\", \"Type\", \"Entry_Date\", \"trade_size\"]\n",
    "\n",
    "    for gid in trade_ids:\n",
    "        mask = (groups == gid)\n",
    "        start_idx = df.index[mask][0]\n",
    "\n",
    "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
    "        try:\n",
    "            base_px = float(base_px)\n",
    "        except Exception:\n",
    "            base_px = np.nan\n",
    "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
    "            try:\n",
    "                base_px = float(df.at[start_idx, 'close'])\n",
    "            except Exception:\n",
    "                base_px = np.nan\n",
    "\n",
    "        atr_base = np.nan\n",
    "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
    "            try:\n",
    "                atr_base = float(df.at[start_idx, 'atr_base'])\n",
    "            except Exception:\n",
    "                atr_base = np.nan\n",
    "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
    "            try:\n",
    "                atr_base = float(df.at[start_idx, 'ATR'])\n",
    "            except Exception:\n",
    "                atr_base = np.nan\n",
    "\n",
    "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
    "\n",
    "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
    "\n",
    "        if np.isfinite(base_px):\n",
    "            df.loc[mask, 'base_px'] = base_px\n",
    "        if np.isfinite(atr_base):\n",
    "            df.loc[mask, 'atr_base'] = atr_base\n",
    "\n",
    "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long', 'Short'):\n",
    "            if typ == 'Long':\n",
    "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
    "                df.loc[mask, 'atr_mult_low'] = ((df.loc[mask, 'low'] - base_px) / atr_base).round(2)\n",
    "            else:\n",
    "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
    "                df.loc[mask, 'atr_mult_low'] = ((base_px - df.loc[mask, 'low']) / atr_base).round(2)\n",
    "\n",
    "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
    "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long', 'Short'):\n",
    "            m_nan = mask & df['profits'].isna()\n",
    "            if m_nan.any():\n",
    "                if typ == 'Long':\n",
    "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
    "                else:\n",
    "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
    "\n",
    "    # Propaga Real_SL por bloque\n",
    "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
    "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
    "    else:\n",
    "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
    "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
    "\n",
    "    if starts.any():\n",
    "        grp = starts.cumsum()\n",
    "        for gid in grp[starts].unique():\n",
    "            m = (grp == gid)\n",
    "            ser = df.loc[m, 'Real_SL']\n",
    "            if ser.notna().any():\n",
    "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
    "                df.loc[m, 'Real_SL'] = val\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tick_dyn_atr(df: pd.DataFrame,\n",
    "                 initial_atr: float = INITIAL_SL,\n",
    "                 first_step_atr: float = FIRST_STEP_ATR,\n",
    "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dinámica de stop en múltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
    "      • Escribe 'tick_dyn_atr' (múltiplos)\n",
    "      • Calcula y actualiza 'Stop_Loss_$' y 'Stop_Loss_atr'\n",
    "    \"\"\"\n",
    "    col_name = 'tick_dyn_atr'\n",
    "    if col_name not in df.columns:\n",
    "        df[col_name] = np.nan\n",
    "    if 'Stop_Loss_$' not in df.columns:\n",
    "        df['Stop_Loss_$'] = np.nan\n",
    "    if 'Stop_Loss_atr' not in df.columns:\n",
    "        df['Stop_Loss_atr'] = np.nan\n",
    "\n",
    "    in_trade = False\n",
    "    trade_active = False\n",
    "    broken = False\n",
    "    sl_val = float(initial_atr)\n",
    "    next_threshold = float(first_step_atr)\n",
    "    prev_sl = float(initial_atr)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        new_open = (\n",
    "            (('orderId' in df.columns) and pd.notna(row.get('orderId'))) or\n",
    "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
    "            (pd.notna(row.get('Entry_Date')))\n",
    "        )\n",
    "\n",
    "        if new_open and not in_trade:\n",
    "            in_trade = True\n",
    "            trade_active = True\n",
    "            broken = False\n",
    "            sl_val = float(initial_atr)\n",
    "            next_threshold = float(first_step_atr)\n",
    "            prev_sl = sl_val\n",
    "            entry_dt = row.get('time')\n",
    "            if entry_dt is not None:\n",
    "                df.at[idx, 'Entry_Date'] = entry_dt\n",
    "\n",
    "        if in_trade:\n",
    "            m_high = row.get('atr_mult_high', np.nan)\n",
    "            m_low = row.get('atr_mult_low', np.nan)\n",
    "            best_pnl = np.nanmax([m_high, m_low])\n",
    "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
    "\n",
    "            if trade_active and not broken:\n",
    "                while best_pnl >= next_threshold:\n",
    "                    sl_val += float(gap_first_step_atr)\n",
    "                    next_threshold += float(gap_first_step_atr)\n",
    "\n",
    "                below_prev = (\n",
    "                    (np.isfinite(m_high) and float(m_high) < prev_sl) or\n",
    "                    (np.isfinite(m_low) and float(m_low) < prev_sl)\n",
    "                )\n",
    "                if below_prev:\n",
    "                    broken = True\n",
    "                    trade_active = False\n",
    "                    in_trade = False\n",
    "\n",
    "            df.at[idx, col_name] = np.nan if broken else sl_val\n",
    "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
    "            prev_sl = sl_val\n",
    "\n",
    "    # Convierte múltiplos a precio\n",
    "    try:\n",
    "        atr_mult = df[col_name].astype(float)\n",
    "        base = df['base_px'].astype(float)\n",
    "        atr = df['atr_base'].astype(float)\n",
    "        typ = df['Type'].astype('string')\n",
    "\n",
    "        stop_price = np.where(\n",
    "            (typ == 'Long') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
    "            base + atr * atr_mult,\n",
    "            np.where(\n",
    "                (typ == 'Short') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
    "                base - atr * atr_mult,\n",
    "                np.nan\n",
    "            )\n",
    "        )\n",
    "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
    "                                 rpc_conn,\n",
    "                                 symbol: str = SYMBOL,\n",
    "                                 magic: int = 900001,\n",
    "                                 tol: float = 0.01) -> None:\n",
    "    \"\"\"\n",
    "    Copia desde la posición viva los campos de mercado al DF.\n",
    "    'profits' solo se escribe en la ÚLTIMA FILA para no sobreescribir histórico.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "\n",
    "    _ensure_order_cols(df)\n",
    "\n",
    "    try:\n",
    "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "    except Exception:\n",
    "        positions = []\n",
    "\n",
    "    if not positions:\n",
    "        return\n",
    "\n",
    "    pos = next((p for p in positions if _position_magic_matches(p, magic) and _position_symbol_matches(p, symbol)), None)\n",
    "    if not pos:\n",
    "        pos = next((p for p in positions if _position_magic_matches(p, magic)), None)\n",
    "    if not pos:\n",
    "        return\n",
    "\n",
    "    order_id = _position_id(pos)\n",
    "    magic_val = _position_magic_number(pos)\n",
    "    symbol_val = pos.get(\"symbol\")\n",
    "    open_price = pos.get(\"openPrice\") or pos.get(\"price\")\n",
    "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\")\n",
    "    stop_loss = pos.get(\"stopLoss\")\n",
    "    volume_val = pos.get(\"volume\") or pos.get(\"lots\")\n",
    "    profit_val = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
    "                  else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
    "\n",
    "    side = _position_side(pos)\n",
    "    typ = \"Long\" if side == \"BUY\" else (\"Short\" if side == \"SELL\" else None)\n",
    "\n",
    "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
    "    clean_cmt, mag_from_cmt = _split_comment_and_magic(comment_raw)\n",
    "\n",
    "    block_mask = pd.Series(False, index=df.index)\n",
    "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
    "        block_mask = (df[\"orderId\"] == order_id)\n",
    "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
    "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
    "        starts = ed.notna() & (ed != ed.shift(1))\n",
    "        if starts.any():\n",
    "            last_start = df.index[starts].max()\n",
    "            block_mask = (df.index >= last_start)\n",
    "        else:\n",
    "            block_mask = ed.notna() & (ed >= entry_dt)\n",
    "    if not block_mask.any():\n",
    "        block_mask.iloc[-1] = True\n",
    "\n",
    "    try:\n",
    "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
    "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
    "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
    "        elif mag_from_cmt is not None:\n",
    "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
    "        else:\n",
    "            df.loc[block_mask, \"magic\"] = int(magic)\n",
    "\n",
    "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
    "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
    "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
    "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
    "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
    "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
    "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    last_idx = df.index[-1]\n",
    "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
    "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
    "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
    "    elif mag_from_cmt is not None:\n",
    "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
    "    else:\n",
    "        df.at[last_idx, \"magic\"] = int(magic)\n",
    "\n",
    "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
    "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
    "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
    "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
    "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
    "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
    "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
    "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
    "\n",
    "\n",
    "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
    "    \"\"\"Devuelve (prev, last) con los dos últimos valores no-NaN distintos.\"\"\"\n",
    "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    last = float(s.iloc[-1])\n",
    "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
    "    return (prev, last)\n",
    "\n",
    "\n",
    "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
    "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "    if not positions:\n",
    "        return None\n",
    "\n",
    "    for position in positions:\n",
    "        if _position_symbol_matches(position, symbol) and _position_magic_matches(position, magic):\n",
    "            return position\n",
    "    for position in positions:\n",
    "        if _position_magic_matches(position, magic):\n",
    "            return position\n",
    "    return None\n",
    "\n",
    "\n",
    "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
    "                                     rpc_conn,\n",
    "                                     *,\n",
    "                                     symbol: str,\n",
    "                                     magic: int,\n",
    "                                     auth_token: str,\n",
    "                                     account_id: str,\n",
    "                                     region: str,\n",
    "                                     tol: float = 0.0) -> dict:\n",
    "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
    "    if not np.isfinite(last_sl):\n",
    "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
    "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vacío\"}\n",
    "\n",
    "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
    "    if not changed:\n",
    "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
    "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
    "\n",
    "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
    "\n",
    "    if not pos:\n",
    "        last_oid = None\n",
    "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
    "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
    "        if last_oid:\n",
    "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
    "            for candidate in positions:\n",
    "                if _position_id(candidate) == last_oid:\n",
    "                    pos = candidate\n",
    "                    break\n",
    "\n",
    "    if not pos:\n",
    "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
    "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posición con ese magic\"}\n",
    "\n",
    "    position_id = _position_id(pos)\n",
    "    if not position_id:\n",
    "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
    "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vacío\"}\n",
    "\n",
    "    loop = asyncio.get_running_loop()\n",
    "    resp = await loop.run_in_executor(\n",
    "        None,\n",
    "        lambda: _rest_modify_position(\n",
    "            auth_token=auth_token,\n",
    "            account_id=account_id,\n",
    "            region=region,\n",
    "            position_id=position_id,\n",
    "            stop_loss=float(last_sl),\n",
    "            timeout=15\n",
    "        )\n",
    "    )\n",
    "    ok = getattr(resp, \"status_code\", 0) == 200\n",
    "    err = None\n",
    "    if not ok:\n",
    "        try:\n",
    "            err = json.dumps(resp.json())[:300]\n",
    "        except Exception:\n",
    "            err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
    "\n",
    "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
    "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cvhz3M-fTIw",
   "metadata": {
    "id": "5cvhz3M-fTIw"
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Bucle principal:\n",
    "      • Crea/migra el CSV inicial (ATR interno, señales Kalman).\n",
    "      • Cada 5 minutos procesa la última vela cerrada y, si corresponde,\n",
    "        abre una operación con stop-loss y take-profit.\n",
    "    \"\"\"\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    # 0) Conexión MetaApi / RPC\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
    "    rpc_conn = account.get_rpc_connection()\n",
    "    await rpc_conn.connect()\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    # Parámetros (con defaults tolerantes a faltantes globales)\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    MAGIC    = 900001\n",
    "    LENGTHS  = (300, 520, 710, 1130)\n",
    "    SMOOTHS  = (3, 3, 3, 7)\n",
    "    LOT_     = globals().get(\"LOT\", 0.1)\n",
    "    COMMENT_ = globals().get(\"COMMENT\", \"Kal\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    # Helpers locales\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    import datetime as dt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    def _parse_tf_to_delta(tf: str) -> dt.timedelta:\n",
    "        \"\"\"'1m','5m','15m','1h','4h','1d' → timedelta (fallback 1m).\"\"\"\n",
    "        tf = (tf or \"1m\").strip().lower()\n",
    "        if tf.endswith(\"mn\"):\n",
    "            tf = tf[:-2] + \"m\"\n",
    "        try:\n",
    "            if tf.endswith(\"m\"):\n",
    "                return dt.timedelta(minutes=max(int(tf[:-1]), 1))\n",
    "            if tf.endswith(\"h\"):\n",
    "                return dt.timedelta(hours=max(int(tf[:-1]), 1))\n",
    "            if tf.endswith(\"d\"):\n",
    "                return dt.timedelta(days=max(int(tf[:-1]), 1))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return dt.timedelta(minutes=1)\n",
    "\n",
    "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
    "        \"\"\"Floor de ts a múltiplo exacto del timeframe (UTC).\"\"\"\n",
    "        if ts.tzinfo is None:\n",
    "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
    "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
    "        secs  = int((ts - epoch).total_seconds())\n",
    "        step  = int(delta.total_seconds()) or 60\n",
    "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
    "\n",
    "    def _calc_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "        \"\"\"\n",
    "        ATR estilo Wilder: TR = max(H-L, |H-C1|, |L-C1|), ATR = RMA(TR, period).\n",
    "        Usa ewm(alpha=1/period) como aproximación de RMA.\n",
    "        \"\"\"\n",
    "        h, l, c = df[\"high\"].astype(float), df[\"low\"].astype(float), df[\"close\"].astype(float)\n",
    "        c1 = c.shift(1)\n",
    "        tr = np.maximum.reduce([\n",
    "            (h - l).to_numpy(),\n",
    "            (h - c1).abs().to_numpy(),\n",
    "            (l - c1).abs().to_numpy()\n",
    "        ])\n",
    "        atr = pd.Series(tr, index=df.index).ewm(alpha=1/period, adjust=False).mean()\n",
    "        return atr.round(4)\n",
    "\n",
    "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
    "        \"\"\"True si existe posición con ese magic (prefiere helper global si existe).\"\"\"\n",
    "        try:\n",
    "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
    "            return pos is not None\n",
    "        except Exception:\n",
    "            positions = []\n",
    "            try:\n",
    "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
    "            except Exception:\n",
    "                positions = []\n",
    "            if not positions:\n",
    "                r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
    "                if getattr(r, \"status_code\", 0) == 200:\n",
    "                    try:\n",
    "                        positions = r.json() or []\n",
    "                    except Exception:\n",
    "                        positions = []\n",
    "            if not positions:\n",
    "                return False\n",
    "            for p in positions:\n",
    "                pm = p.get(\"magic\")\n",
    "                cmt = str(p.get(\"comment\") or \"\")\n",
    "                ok = False\n",
    "                if pm is not None:\n",
    "                    try: ok = int(pm) == int(magic)\n",
    "                    except Exception: ok = False\n",
    "                if (not ok) and f\"magic={magic}\" in cmt:\n",
    "                    ok = True\n",
    "                if ok:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    async def _get_api_type(rpc_conn, symbol: str, magic: int):\n",
    "        \"\"\"'Long' / 'Short' / None usando get_pos_with_magic si existe.\"\"\"\n",
    "        side = None\n",
    "        try:\n",
    "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
    "            if not pos:\n",
    "                return None\n",
    "            t = pos.get(\"type\")\n",
    "            if isinstance(t, str):\n",
    "                tu = t.upper()\n",
    "                side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
    "            elif t == 0:\n",
    "                side = \"BUY\"\n",
    "            elif t == 1:\n",
    "                side = \"SELL\"\n",
    "        except Exception:\n",
    "            try:\n",
    "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
    "            except Exception:\n",
    "                positions = []\n",
    "            for p in positions:\n",
    "                pm = p.get(\"magic\")\n",
    "                cmt = str(p.get(\"comment\") or \"\")\n",
    "                ok = False\n",
    "                if pm is not None:\n",
    "                    try: ok = int(pm) == int(magic)\n",
    "                    except Exception: ok = False\n",
    "                if (not ok) and f\"magic={magic}\" in cmt:\n",
    "                    ok = True\n",
    "                if not ok:\n",
    "                    continue\n",
    "                t = p.get(\"type\")\n",
    "                if isinstance(t, str):\n",
    "                    tu = t.upper()\n",
    "                    side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
    "                elif t == 0:\n",
    "                    side = \"BUY\"\n",
    "                elif t == 1:\n",
    "                    side = \"SELL\"\n",
    "                break\n",
    "        if side is None:\n",
    "            return None\n",
    "        return \"Long\" if side == \"BUY\" else \"Short\"\n",
    "\n",
    "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: str | None) -> None:\n",
    "        \"\"\"Escribe 'Type' en el bloque activo o en la última fila si no se detecta bloque.\"\"\"\n",
    "        if not api_type or df_all.empty:\n",
    "            return\n",
    "        last_oid = df_all.get(\"orderId\")\n",
    "        if last_oid is not None and last_oid.notna().any():\n",
    "            last_oid_val = last_oid.dropna().iloc[-1]\n",
    "            mask = (df_all[\"orderId\"] == last_oid_val)\n",
    "        else:\n",
    "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
    "            starts = ed.notna() & (ed != ed.shift(1))\n",
    "            if starts.any():\n",
    "                start_idx = df_all.index[starts].max()\n",
    "                mask = (df_all.index >= start_idx)\n",
    "            else:\n",
    "                mask = pd.Series(False, index=df_all.index)\n",
    "        if mask.any():\n",
    "            df_all.loc[mask, \"Type\"] = api_type\n",
    "        else:\n",
    "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    # 1) Crear/migrar archivo inicial\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        df = await get_candles_5m(account, start=None, limit=CANDEL_NUMBER)\n",
    "        if len(df) >= 14:\n",
    "            df[\"ATR\"] = _calc_atr(df, 14)\n",
    "        l1, l2, l3, l4 = LENGTHS\n",
    "        s1, s2, s3, s4 = SMOOTHS\n",
    "        generate_trade_signals(df, l1, l2, l3, l4, s1, s2, s3, s4)\n",
    "        _ensure_order_cols(df)\n",
    "        stamp_system_time(df, \"last\")\n",
    "        save_csv(df)\n",
    "        print(f\"✔ Archivo inicial creado con {len(df)} velas\")\n",
    "    else:\n",
    "        migrate_csv_if_needed(FILE_PATH)\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    # 2) Loop principal\n",
    "    # ───────────────────────────────────────────────────────────────────\n",
    "    async def _wait_for_closed_candle(prev_bar: dt.datetime,\n",
    "                                      last_known_time: pd.Timestamp | None,\n",
    "                                      delta: dt.timedelta) -> pd.DataFrame:\n",
    "        \"\"\"Obtiene la última vela cerrada <= prev_bar reintentando durante un margen de seguridad.\"\"\"\n",
    "        wait_limit = max(dt.timedelta(seconds=45), delta)\n",
    "        wait_limit = min(wait_limit, dt.timedelta(minutes=5))\n",
    "        deadline = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc) + wait_limit\n",
    "        poll_sleep = max(3, min(20, int(delta.total_seconds() // 6) or 1))\n",
    "        warned_at: dt.datetime | None = None\n",
    "        latest_candidate: pd.DataFrame | None = None\n",
    "        last_snapshot = pd.DataFrame()\n",
    "        while True:\n",
    "            fresh = await get_candles_5m(account, start=None, limit=50)\n",
    "            fresh = (fresh[fresh[\"time\"] <= prev_bar]\n",
    "                     .drop_duplicates(\"time\")\n",
    "                     .sort_values(\"time\"))\n",
    "            last_snapshot = fresh.copy()\n",
    "            if not fresh.empty:\n",
    "                latest_candidate = fresh.iloc[[-1]].copy()\n",
    "                latest_time = pd.to_datetime(latest_candidate[\"time\"], utc=True, errors=\"coerce\")\n",
    "                latest_time = latest_time.iloc[-1] if not latest_time.empty else None\n",
    "                if last_known_time is None or (latest_time is not None and latest_time > last_known_time):\n",
    "                    return latest_candidate\n",
    "                if (last_known_time is not None\n",
    "                        and latest_time is not None and latest_time <= last_known_time):\n",
    "                    now_warn = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "                    if warned_at is None or (now_warn - warned_at).total_seconds() >= 60:\n",
    "                        ts_txt = prev_bar.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"⚠️ Vela cerrada {ts_txt} aún no publicada; reintentando...\", flush=True)\n",
    "                        warned_at = now_warn\n",
    "            if dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc) >= deadline:\n",
    "                if latest_candidate is not None:\n",
    "                    return latest_candidate\n",
    "                return last_snapshot\n",
    "            await asyncio.sleep(poll_sleep)\n",
    "\n",
    "    reconnect_backoff = 5.0\n",
    "    reconnect_backoff_max = 60.0\n",
    "\n",
    "    last_no_new_warning_at: dt.datetime | None = None\n",
    "    last_no_new_warning_bar: dt.datetime | None = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            await asyncio.sleep(seconds_until_next_tf(time_frame_data, offset_sec=3))\n",
    "\n",
    "            now_utc  = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "            delta    = _parse_tf_to_delta(time_frame_data)\n",
    "            this_bar = _floor_to_frame(now_utc, delta)\n",
    "            prev_bar = this_bar - delta  # última vela CERRADA\n",
    "\n",
    "            df_all = _load_csv()\n",
    "            last_known_time = None\n",
    "            if not df_all.empty and \"time\" in df_all.columns:\n",
    "                known_times = pd.to_datetime(df_all[\"time\"], utc=True, errors=\"coerce\").dropna()\n",
    "                if not known_times.empty:\n",
    "                    last_known_time = known_times.iloc[-1]\n",
    "\n",
    "            df_new = await _wait_for_closed_candle(prev_bar, last_known_time, delta)\n",
    "\n",
    "            prev_bar_txt = prev_bar.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            latest_new_time = None\n",
    "            if not df_new.empty and \"time\" in df_new.columns:\n",
    "                latest_times = pd.to_datetime(df_new[\"time\"], utc=True, errors=\"coerce\").dropna()\n",
    "                if not latest_times.empty:\n",
    "                    latest_new_time = latest_times.iloc[-1]\n",
    "            if last_known_time is not None and (latest_new_time is None or latest_new_time <= last_known_time):\n",
    "                now_warn = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "                should_warn = True\n",
    "                if last_no_new_warning_at is not None:\n",
    "                    elapsed = (now_warn - last_no_new_warning_at).total_seconds()\n",
    "                    same_bar = (last_no_new_warning_bar == prev_bar)\n",
    "                    if same_bar and elapsed < 60:\n",
    "                        should_warn = False\n",
    "                if should_warn:\n",
    "                    print(f\"⚠️ Sin nueva vela cerrada para {prev_bar_txt} UTC; se reintentará en el siguiente ciclo.\", flush=True)\n",
    "                    last_no_new_warning_at = now_warn\n",
    "                    last_no_new_warning_bar = prev_bar\n",
    "\n",
    "            existing_times = (\n",
    "                set(pd.to_datetime(df_all[\"time\"], utc=True))\n",
    "                if (not df_all.empty and \"time\" in df_all.columns)\n",
    "                else set()\n",
    "            )\n",
    "\n",
    "            if df_all.empty:\n",
    "                df_all = df_new.copy()\n",
    "            else:\n",
    "                df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
    "                          .drop_duplicates(\"time\")\n",
    "                          .sort_values(\"time\")\n",
    "                          .reset_index(drop=True))\n",
    "\n",
    "            if len(df_all) >= 14:\n",
    "                df_all[\"ATR\"] = _calc_atr(df_all, 14)\n",
    "\n",
    "            l1, l2, l3, l4 = LENGTHS\n",
    "            s1, s2, s3, s4 = SMOOTHS\n",
    "            generate_trade_signals(df_all, l1, l2, l3, l4, s1, s2, s3, s4)\n",
    "            _ensure_order_cols(df_all)\n",
    "\n",
    "            if not df_new.empty:\n",
    "                new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
    "                if new_times:\n",
    "                    df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
    "\n",
    "            await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT_, comment=COMMENT_, magic=MAGIC)\n",
    "\n",
    "            api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
    "            _sync_type_in_df(df_all, api_type)\n",
    "            await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
    "\n",
    "            stamp_system_time(df_all, \"last\")\n",
    "            save_csv(df_all)\n",
    "            print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"), \"| actualización (ciclo \"+time_frame_data+\")\")\n",
    "\n",
    "            reconnect_backoff = 5.0\n",
    "        except asyncio.CancelledError:\n",
    "            raise\n",
    "        except Exception as loop_err:\n",
    "            logging.exception(\"Error en ciclo principal MetaApi: %s\", loop_err)\n",
    "            print(f\"⚠️ Error en ciclo principal: {loop_err}\", flush=True)\n",
    "            await asyncio.sleep(reconnect_backoff)\n",
    "            reconnect_backoff = min(reconnect_backoff * 2, reconnect_backoff_max)\n",
    "            try:\n",
    "                account = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
    "                rpc_conn = account.get_rpc_connection()\n",
    "                try:\n",
    "                    await rpc_conn.connect()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    await asyncio.wait_for(rpc_conn.wait_synchronized(), timeout=30)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                reconnect_backoff = 5.0\n",
    "                print(\"🔄 Reconexión MetaApi completada.\", flush=True)\n",
    "            except Exception as recon_err:\n",
    "                logging.exception(\"Fallo reconectando a MetaApi: %s\", recon_err)\n",
    "                print(f\"❌ Falló la reconexión a MetaApi: {recon_err}\", flush=True)\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W481PrVufZ8Z",
   "metadata": {
    "id": "W481PrVufZ8Z"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yI6v2q_xfRtg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yI6v2q_xfRtg",
    "outputId": "1648fe99-4abe-41c6-c45f-11ac7e8a0d60"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Archivo inicial creado con 900 velas\n",
      "⚠️ Vela cerrada 2025-10-09 14:41:00 aún no publicada; reintentando...\n",
      "⚠️ Vela cerrada 2025-10-09 14:41:00 aún no publicada; reintentando...\n",
      "⚠️ Sin nueva vela cerrada para 2025-10-09 14:41:00 UTC; se reintentará en el siguiente ciclo.\n",
      "14:43:11 | actualización (ciclo 1m)\n",
      "14:44:06 | actualización (ciclo 1m)\n",
      "14:45:06 | actualización (ciclo 1m)\n",
      "[2025-10-09T14:45:13.339665] london:0: MetaApi websocket client disconnected from the MetaApi server\n",
      "[2025-10-09T14:45:14.341644] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-a.agiliumtrade.ai shared server.\n",
      "[2025-10-09T14:45:16.574513] london:1: MetaApi websocket client disconnected from the MetaApi server\n",
      "[2025-10-09T14:45:17.575939] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-b.agiliumtrade.ai shared server.\n",
      "✅ BUY placed | orderId=321846799 positionId=321846799 openPrice=4014.07 | SL sent=4010.0015000000003 | TP sent=4018.0185 | Type=Long\n",
      "14:46:24 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "14:47:05 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "14:48:05 | actualización (ciclo 1m)\n",
      "14:49:05 | actualización (ciclo 1m)\n",
      "14:50:05 | actualización (ciclo 1m)\n",
      "14:51:06 | actualización (ciclo 1m)\n",
      "14:52:05 | actualización (ciclo 1m)\n",
      "14:53:05 | actualización (ciclo 1m)\n",
      "14:54:05 | actualización (ciclo 1m)\n",
      "14:55:05 | actualización (ciclo 1m)\n",
      "14:56:05 | actualización (ciclo 1m)\n",
      "14:57:05 | actualización (ciclo 1m)\n",
      "14:58:05 | actualización (ciclo 1m)\n",
      "14:59:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321865558 positionId=321865558 openPrice=4024.27 | SL sent=4027.2605 | TP sent=4021.0595 | Type=Short\n",
      "15:00:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:01:05 | actualización (ciclo 1m)\n",
      "15:02:06 | actualización (ciclo 1m)\n",
      "15:03:07 | actualización (ciclo 1m)\n",
      "15:04:05 | actualización (ciclo 1m)\n",
      "15:05:06 | actualización (ciclo 1m)\n",
      "15:06:05 | actualización (ciclo 1m)\n",
      "15:07:05 | actualización (ciclo 1m)\n",
      "15:08:05 | actualización (ciclo 1m)\n",
      "15:09:05 | actualización (ciclo 1m)\n",
      "15:10:05 | actualización (ciclo 1m)\n",
      "✅ BUY placed | orderId=321882422 positionId=321882422 openPrice=4026.55 | SL sent=4023.3943 | TP sent=4029.8257000000003 | Type=Long\n",
      "15:11:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:12:05 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:13:06 | actualización (ciclo 1m)\n",
      "15:14:05 | actualización (ciclo 1m)\n",
      "15:15:05 | actualización (ciclo 1m)\n",
      "15:16:05 | actualización (ciclo 1m)\n",
      "15:17:05 | actualización (ciclo 1m)\n",
      "15:18:05 | actualización (ciclo 1m)\n",
      "✅ BUY placed | orderId=321889061 positionId=321889061 openPrice=4026.71 | SL sent=4024.0713 | TP sent=4029.7287 | Type=Long\n",
      "15:19:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:20:05 | actualización (ciclo 1m)\n",
      "15:21:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321891552 positionId=321891552 openPrice=4025.54 | SL sent=4026.9141 | TP sent=4021.4058999999997 | Type=Short\n",
      "15:22:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:23:05 | actualización (ciclo 1m)\n",
      "15:24:06 | actualización (ciclo 1m)\n",
      "15:25:05 | actualización (ciclo 1m)\n",
      "15:26:05 | actualización (ciclo 1m)\n",
      "15:27:05 | actualización (ciclo 1m)\n",
      "15:28:05 | actualización (ciclo 1m)\n",
      "15:29:05 | actualización (ciclo 1m)\n",
      "15:30:05 | actualización (ciclo 1m)\n",
      "15:31:05 | actualización (ciclo 1m)\n",
      "15:32:05 | actualización (ciclo 1m)\n",
      "15:33:05 | actualización (ciclo 1m)\n",
      "15:34:05 | actualización (ciclo 1m)\n",
      "15:35:07 | actualización (ciclo 1m)\n",
      "15:36:05 | actualización (ciclo 1m)\n",
      "15:37:05 | actualización (ciclo 1m)\n",
      "15:38:05 | actualización (ciclo 1m)\n",
      "15:39:05 | actualización (ciclo 1m)\n",
      "15:40:05 | actualización (ciclo 1m)\n",
      "15:41:05 | actualización (ciclo 1m)\n",
      "15:42:05 | actualización (ciclo 1m)\n",
      "15:43:05 | actualización (ciclo 1m)\n",
      "15:44:05 | actualización (ciclo 1m)\n",
      "15:45:05 | actualización (ciclo 1m)\n",
      "✅ BUY placed | orderId=321918338 positionId=321918338 openPrice=4011.31 | SL sent=4009.1486 | TP sent=4015.5514 | Type=Long\n",
      "15:46:08 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "15:47:05 | actualización (ciclo 1m)\n",
      "15:48:05 | actualización (ciclo 1m)\n",
      "15:49:05 | actualización (ciclo 1m)\n",
      "15:50:05 | actualización (ciclo 1m)\n",
      "15:51:05 | actualización (ciclo 1m)\n",
      "15:52:05 | actualización (ciclo 1m)\n",
      "15:53:05 | actualización (ciclo 1m)\n",
      "15:54:05 | actualización (ciclo 1m)\n",
      "15:55:05 | actualización (ciclo 1m)\n",
      "15:56:05 | actualización (ciclo 1m)\n",
      "15:57:05 | actualización (ciclo 1m)\n",
      "15:58:05 | actualización (ciclo 1m)\n",
      "15:59:05 | actualización (ciclo 1m)\n",
      "16:00:05 | actualización (ciclo 1m)\n",
      "✅ BUY placed | orderId=321933001 positionId=321933001 openPrice=4018.02 | SL sent=4015.0247 | TP sent=4019.8752999999997 | Type=Long\n",
      "16:01:08 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "16:02:05 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "16:03:05 | actualización (ciclo 1m)\n",
      "16:04:05 | actualización (ciclo 1m)\n",
      "16:05:05 | actualización (ciclo 1m)\n",
      "16:06:05 | actualización (ciclo 1m)\n",
      "16:07:06 | actualización (ciclo 1m)\n",
      "16:08:05 | actualización (ciclo 1m)\n",
      "16:09:05 | actualización (ciclo 1m)\n",
      "16:10:05 | actualización (ciclo 1m)\n",
      "16:11:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321940159 positionId=321940159 openPrice=4019.23 | SL sent=4020.9406999999997 | TP sent=4017.0393 | Type=Short\n",
      "16:12:07 | actualización (ciclo 1m)\n",
      "16:13:05 | actualización (ciclo 1m)\n",
      "16:14:05 | actualización (ciclo 1m)\n",
      "16:15:05 | actualización (ciclo 1m)\n",
      "16:16:05 | actualización (ciclo 1m)\n",
      "16:17:05 | actualización (ciclo 1m)\n",
      "16:18:06 | actualización (ciclo 1m)\n",
      "16:19:05 | actualización (ciclo 1m)\n",
      "16:20:05 | actualización (ciclo 1m)\n",
      "16:21:05 | actualización (ciclo 1m)\n",
      "16:22:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321947557 positionId=321947557 openPrice=4014.24 | SL sent=4016.3422 | TP sent=4012.2378 | Type=Short\n",
      "16:23:07 | actualización (ciclo 1m)\n",
      "16:24:05 | actualización (ciclo 1m)\n",
      "16:25:05 | actualización (ciclo 1m)\n",
      "16:26:05 | actualización (ciclo 1m)\n",
      "16:27:05 | actualización (ciclo 1m)\n",
      "16:28:05 | actualización (ciclo 1m)\n",
      "16:29:06 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321951767 positionId=321951767 openPrice=4011.65 | SL sent=4014.3424 | TP sent=4010.1776000000004 | Type=Short\n",
      "16:30:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "16:31:05 | actualización (ciclo 1m)\n",
      "16:32:05 | actualización (ciclo 1m)\n",
      "16:33:05 | actualización (ciclo 1m)\n",
      "16:34:05 | actualización (ciclo 1m)\n",
      "16:35:05 | actualización (ciclo 1m)\n",
      "16:36:05 | actualización (ciclo 1m)\n",
      "16:37:05 | actualización (ciclo 1m)\n",
      "16:38:05 | actualización (ciclo 1m)\n",
      "16:39:05 | actualización (ciclo 1m)\n",
      "16:40:06 | actualización (ciclo 1m)\n",
      "16:41:05 | actualización (ciclo 1m)\n",
      "16:42:05 | actualización (ciclo 1m)\n",
      "16:43:05 | actualización (ciclo 1m)\n",
      "16:44:05 | actualización (ciclo 1m)\n",
      "16:45:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321976692 positionId=321976692 openPrice=3978.22 | SL sent=3984.397 | TP sent=3975.383 | Type=Short\n",
      "16:46:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "16:47:05 | actualización (ciclo 1m)\n",
      "16:48:05 | actualización (ciclo 1m)\n",
      "16:49:05 | actualización (ciclo 1m)\n",
      "✅ BUY placed | orderId=321981122 positionId=321981122 openPrice=3982.92 | SL sent=3979.2922000000003 | TP sent=3988.3878 | Type=Long\n",
      "16:50:08 | actualización (ciclo 1m)\n",
      "16:51:06 | actualización (ciclo 1m)\n",
      "16:52:05 | actualización (ciclo 1m)\n",
      "✅ SELL placed | orderId=321984452 positionId=321984452 openPrice=3977.46 | SL sent=3982.1524000000004 | TP sent=3972.9476 | Type=Short\n",
      "16:53:07 | actualización (ciclo 1m)\n",
      "ℹ Position already open; synced last row and skipped new order.\n",
      "16:54:05 | actualización (ciclo 1m)\n",
      "16:55:05 | actualización (ciclo 1m)\n",
      "16:56:05 | actualización (ciclo 1m)\n",
      "16:57:05 | actualización (ciclo 1m)\n",
      "16:58:05 | actualización (ciclo 1m)\n",
      "16:59:05 | actualización (ciclo 1m)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# EJECUCIÓN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05e894",
   "metadata": {
    "id": "8c05e894"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the last 200 rows for plotting\n",
    "df_plot = df.iloc[500:751].copy()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_plot['time'], df_plot['close'], label='Close', color='black', linewidth=1)\n",
    "plt.plot(df_plot['time'], df_plot['kal_1'], label='Kal_1', color='blue', linestyle='--')\n",
    "plt.plot(df_plot['time'], df_plot['kal_2'], label='Kal_2', color='red', linestyle='--')\n",
    "plt.plot(df_plot['time'], df_plot['kal_3'], label='Kal_3', color='green', linestyle='--')\n",
    "\n",
    "# Add vertical lines for trade signals\n",
    "for index, row in df_plot.iterrows():\n",
    "    if row['Open_Trade'] == 1:\n",
    "        plt.axvline(row['time'], color='blue', linestyle='-', linewidth=1, alpha=0.7, label='Buy Signal' if row['time'] == df_plot['time'].iloc[0] else \"\")\n",
    "    elif row['Open_Trade'] == -1:\n",
    "        plt.axvline(row['time'], color='red', linestyle='-', linewidth=1, alpha=0.7, label='Sell Signal' if row['time'] == df_plot['time'].iloc[0] else \"\")\n",
    "\n",
    "plt.title('Close Price and Kalman Lines with Trade Signals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
