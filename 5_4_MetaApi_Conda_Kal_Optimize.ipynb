{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaf03429",
      "metadata": {
        "id": "eaf03429"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nu7i62hlGthO",
      "metadata": {
        "id": "nu7i62hlGthO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0563f9fc-0909-46e5-cbc1-15678edbb6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from ta-lib) (1.3.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from ta-lib) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta-lib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Ej4IeD7EeF0t",
      "metadata": {
        "id": "Ej4IeD7EeF0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d771ea16-b2e3-486a-c578-a2e4e9ebb043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: metaapi-cloud-sdk in /usr/local/lib/python3.12/dist-packages (29.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.12.15)\n",
            "Requirement already satisfied: python-engineio<4.0.0,>=3.14.2 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (4.15.0)\n",
            "Requirement already satisfied: iso8601 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2025.2)\n",
            "Requirement already satisfied: python-socketio<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (0.28.1)\n",
            "Requirement already satisfied: metaapi-cloud-copyfactory-sdk<13.0.0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (12.0.0)\n",
            "Requirement already satisfied: metaapi-cloud-metastats-sdk<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (6.0.0)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade metaapi-cloud-sdk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DF5CD6j5nmdo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5CD6j5nmdo",
        "outputId": "b134c6d6-6192-4ca8-bae7-3f581193cb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.7\n",
            "ATR(ndarray high, ndarray low, ndarray close, int timeperiod=-0x80000000)\n",
            "\n",
            "ATR(high, low, close[, timeperiod=?])\n",
            "\n",
            "Average True Range (Volatility Indicators)\n",
            "\n",
            "Inputs:\n",
            "    prices: ['high', 'low', 'close']\n",
            "Parameters:\n",
            "    timeperiod: 14\n",
            "Outputs:\n",
            "    real\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function talib._ta_lib.ATR(high, low, close, timeperiod=-2147483648)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import talib as ta\n",
        "print(ta.__version__)  # Should print something like 0.4.28\n",
        "print(ta.ATR.__doc__)  # Confirm ATR function works\n",
        "ta.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime as dt\n",
        "import logging\n",
        "\n",
        "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
        "\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from metaapi_cloud_sdk import MetaApi\n",
        "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import json, re, traceback, requests\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "try:\n",
        "    from metaapi_cloud_sdk import MetaApi\n",
        "except Exception:\n",
        "    MetaApi = None  # permite importar el m√≥dulo incluso sin el SDK instalado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KFfn45ty82qv",
      "metadata": {
        "id": "KFfn45ty82qv"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5RqjoAtFwl",
        "outputId": "7acda3b9-e439-4c2b-af84-b1321541fcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Course Folder/Forex/BTCUSD/\n"
          ]
        }
      ],
      "source": [
        "process = 'Train'\n",
        "SYMBOL = 'BTCUSD'\n",
        "\n",
        "root_data = f'/content/drive/MyDrive/Course Folder/Forex/{SYMBOL}/'\n",
        "print(root_data)\n",
        "\n",
        "rolling_window = 100\n",
        "\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI'\n",
        "ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'\n",
        "\n",
        "LOT      = 1.0\n",
        "COMMENT  = \"Insta\"\n",
        "CANDLE_NUMBER = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dTlwjirFwRfj",
      "metadata": {
        "id": "dTlwjirFwRfj"
      },
      "outputs": [],
      "source": [
        "# cache simple en m√≥dulo\n",
        "__CONNECTION_CHECKED = False\n",
        "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
        "\n",
        "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Conecta v√≠a RPC y espera sincronizaci√≥n. Lanza excepci√≥n si no se logra.\n",
        "    Devuelve (account, rpc_conn).\n",
        "    \"\"\"\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # refrescamos para leer estado/connectionStatus\n",
        "    try:\n",
        "        await account.reload()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Conexi√≥n RPC + sincronizaci√≥n del terminal\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "    await rpc_conn.wait_synchronized()  # espera a que el terminal est√© listo\n",
        "\n",
        "    # Sonda r√°pida para confirmar conectividad real con el terminal\n",
        "    try:\n",
        "        _ = await rpc_conn.get_account_information()\n",
        "    except Exception:\n",
        "        # si falla la sonda, igual devolvemos la conexi√≥n (ya sincronizada)\n",
        "        pass\n",
        "\n",
        "    return account, rpc_conn\n",
        "\n",
        "def _run(coro):\n",
        "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "def check_connection_once(token: str, account_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida la conexi√≥n y sincronizaci√≥n SOLO la primera vez que se llama.\n",
        "    En llamadas posteriores no vuelve a conectar.\n",
        "    \"\"\"\n",
        "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
        "    if __CONNECTION_CHECKED:\n",
        "        print(\"‚ÑπÔ∏è Conexi√≥n ya validada en esta sesi√≥n; no se repite.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
        "        __ACCOUNT_CONN = (account, rpc_conn)\n",
        "        __CONNECTION_CHECKED = True\n",
        "        print(f\"‚úÖ Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
        "        return True\n",
        "    except TimeoutException as e:\n",
        "        print(f\"‚ùå Timeout esperando sincronizaci√≥n. ¬øLa cuenta est√° CONNECTED al broker? Detalle: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå No fue posible validar la conexi√≥n. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def _safe_json_dump(value):\n",
        "    try:\n",
        "        return json.dumps(value, indent=2, default=str)\n",
        "    except Exception:\n",
        "        return str(value)\n",
        "\n",
        "def print_order_error_details(ctx: dict, err: Exception):\n",
        "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
        "    print(\"\\n\" + \"‚úò\" * 70)\n",
        "    print(\"‚ùå Order failed\")\n",
        "    print(\"‚Ä¢ Exception type:\", type(err).__name__)\n",
        "    print(\"‚Ä¢ Message       :\", str(err))\n",
        "\n",
        "    # Known useful attributes often present on MetaApi exceptions\n",
        "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
        "        if hasattr(err, attr):\n",
        "            val = getattr(err, attr)\n",
        "            if val:\n",
        "                print(f\"‚Ä¢ {attr:12}: {_safe_json_dump(val)}\")\n",
        "\n",
        "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
        "    msg = str(err)\n",
        "    m = re.search(r\"\\{.*\\}\", msg)\n",
        "    if m:\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "            print(\"‚Ä¢ parsed_json  :\", _safe_json_dump(payload))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Stack (useful while debugging)\n",
        "    print(\"‚Ä¢ traceback    :\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Context of the attempt\n",
        "    print(\"‚Ä¢ context      :\", _safe_json_dump(ctx))\n",
        "    print(\"‚úò\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RjShygBIwWRp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjShygBIwWRp",
        "outputId": "15aa1716-58d0-4015-d5b1-91656f2e7f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08T16:42:52.736564] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-a.agiliumtrade.ai shared server.\n",
            "[2025-09-08T16:42:52.753626] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-b.agiliumtrade.ai shared server.\n",
            "[2025-09-08T16:42:53.930475] new-york:0: MetaApi websocket client connected to the MetaApi server\n",
            "[2025-09-08T16:42:53.955318] new-york:1: MetaApi websocket client connected to the MetaApi server\n",
            "‚úÖ Conectado y sincronizado con MetaApi. account_id=163d9a57-1f07-4e78-a6af-036efe867c1b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfxZvvtJeFh1",
      "metadata": {
        "id": "HfxZvvtJeFh1"
      },
      "source": [
        "# Real_Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Sx6TbzBOBzZl",
      "metadata": {
        "id": "Sx6TbzBOBzZl"
      },
      "outputs": [],
      "source": [
        "SYMBOL = \"BTCUSD\"\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "FETCH_INTERVAL   = 60        # opcional (si lo usas en otra parte)\n",
        "time_frame_data  = \"1m\"       # <<--- cambia de \"1m\" a \"5m\"\n",
        "CANDEL_NUMBER    = 900        # sin cambio; solo afecta cu√°ntas velas bajas inicialmente\n",
        "\n",
        "LOT     = 1.0\n",
        "COMMENT = \"Insta\"\n",
        "\n",
        "length_1 = 300\n",
        "length_2 = 410\n",
        "length_3 = 710\n",
        "length_4 = 870\n",
        "\n",
        "smooth_1 = 3\n",
        "smooth_2 = 3\n",
        "smooth_3 = 3\n",
        "smooth_4 = 5\n",
        "\n",
        "INITIAL_SL         = -2\n",
        "FIRST_STEP_ATR     = 0.5\n",
        "GAP_FIRST_STEP_ATR = 2\n",
        "\n",
        "REGION = \"new-york\"\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI'\n",
        "ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sKKJrsFBfOoT",
      "metadata": {
        "id": "sKKJrsFBfOoT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PAR√ÅMETROS / DEFAULTS SEGUROS (no rompen si faltan globales)\n",
        "# ============================================================================\n",
        "try:\n",
        "    FILE_PATH  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FILE_PATH = \"xauusd_data.csv\"\n",
        "\n",
        "try:\n",
        "    SYMBOL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    SYMBOL = \"BTCUSD\"\n",
        "\n",
        "try:\n",
        "    time_frame_data  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    time_frame_data = \"5m\"\n",
        "\n",
        "try:\n",
        "    CANDEL_NUMBER  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    CANDEL_NUMBER = 100\n",
        "\n",
        "try:\n",
        "    INITIAL_SL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    INITIAL_SL = -1.0  # m√∫ltiplos ATR (negativo para BUY)\n",
        "\n",
        "try:\n",
        "    FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FIRST_STEP_ATR = 0.5\n",
        "\n",
        "try:\n",
        "    GAP_FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    GAP_FIRST_STEP_ATR = 2.0\n",
        "\n",
        "try:\n",
        "    META_API_TOKEN  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    META_API_TOKEN = \"\"  # pon tu token real\n",
        "\n",
        "try:\n",
        "    ACCOUNT_ID  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    ACCOUNT_ID = \"\"  # pon tu account id\n",
        "\n",
        "try:\n",
        "    REGION  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    REGION = \"new-york\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOGGING\n",
        "# ============================================================================\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\", \"websockets\"):\n",
        "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
        "if \"MetaApi\" in globals() and MetaApi:\n",
        "    try:\n",
        "        MetaApi.enable_logging()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FILTRO KALMAN + INSTA\n",
        "# ============================================================================\n",
        "def kalman_line(source: pd.Series | Sequence[float], kalman_length: int, smooth: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Filtro Kalman (versi√≥n simple) + suavizado EWM.\n",
        "    Retorna una Serie alineada al √≠ndice de 'source' si es Series.\n",
        "    \"\"\"\n",
        "    src = pd.Series(source)\n",
        "    n = len(src)\n",
        "    if n == 0:\n",
        "        return src\n",
        "\n",
        "    kf_c = np.empty(n, dtype=float)\n",
        "    velo_c = np.zeros(n, dtype=float)\n",
        "\n",
        "    sqrt_term = np.sqrt(max(kalman_length, 1) / 10000.0 * 2.0)\n",
        "    length_term = max(kalman_length, 1) / 10000.0\n",
        "\n",
        "    kf_c[0] = float(src.iloc[0])\n",
        "    velo_c[0] = 0.0\n",
        "\n",
        "    for i in range(1, n):\n",
        "        prev_kf = kf_c[i - 1] if np.isfinite(kf_c[i - 1]) else float(src.iloc[i])\n",
        "        dk = float(src.iloc[i]) - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term\n",
        "        velo_c[i] = velo_c[i - 1] + length_term * dk\n",
        "        kf_c[i] = smooth_c + velo_c[i]\n",
        "\n",
        "    kf_c_series = pd.Series(kf_c, index=src.index)\n",
        "    kal = kf_c_series.ewm(span=max(int(smooth), 1), adjust=False).mean()\n",
        "    return kal\n",
        "\n",
        "\n",
        "def insta(src: Sequence[float], a: float) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implementaci√≥n de filtro 'insta' tipo Pine (lag e instant√°neo).\n",
        "    Devuelve (lag, it).\n",
        "    \"\"\"\n",
        "    src = np.asarray(src, dtype=float)\n",
        "    n = src.size\n",
        "    it = np.full(n, np.nan)\n",
        "    lag = np.full(n, np.nan)\n",
        "\n",
        "    def get(arr, i):\n",
        "        return arr[i] if 0 <= i < n else np.nan\n",
        "\n",
        "    def nz(x, y):\n",
        "        return x if np.isfinite(x) else y\n",
        "\n",
        "    for i in range(n):\n",
        "        s0, s1, s2 = get(src, i), get(src, i - 1), get(src, i - 2)\n",
        "        fallback = (s0 + 2 * s1 + s2) / 4 if all(map(np.isfinite, [s0, s1, s2])) else np.nan\n",
        "\n",
        "        it_prev1, it_prev2 = get(it, i - 1), get(it, i - 2)\n",
        "\n",
        "        term1 = (a - (a * a) / 4.0) * (s0 if np.isfinite(s0) else 0.0)\n",
        "        term2 = 0.5 * a * a * (s1 if np.isfinite(s1) else 0.0)\n",
        "        term3 = -(a - 0.75 * a * a) * (s2 if np.isfinite(s2) else 0.0)\n",
        "        term4 = 2 * (1 - a) * nz(it_prev1, fallback)\n",
        "        term5 = (1 - a) * (1 - a) * nz(it_prev2, fallback)\n",
        "\n",
        "        it[i] = term1 + term2 + term3 + term4 - term5\n",
        "\n",
        "        it_prev2_for_lag = get(it, i - 2)\n",
        "        lag[i] = 2 * it[i] - it_prev2_for_lag if np.isfinite(it_prev2_for_lag) else np.nan\n",
        "\n",
        "    return lag, it\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UTILIDADES DE COLUMNAS / CSV\n",
        "# ============================================================================\n",
        "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Crea/normaliza columnas clave con dtypes consistentes y\n",
        "    elimina columnas heredadas ('id', 'actionType') si existieran.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # elimina columnas heredadas\n",
        "    for col in (\"id\", \"actionType\"):\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    col_types = {\n",
        "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
        "        \"orderId\":       \"string\",\n",
        "        \"magic\":         \"Int64\",\n",
        "        \"symbol\":        \"string\",\n",
        "        \"openPrice\":     \"float64\",\n",
        "        \"comment\":       \"string\",\n",
        "        \"Type\":          \"string\",\n",
        "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
        "        \"Stop_Loss_atr\": \"float64\",\n",
        "        \"Stop_Loss_$\":   \"float64\",\n",
        "        \"Take_Profit_atr\": \"float64\",\n",
        "        \"Take_Profit_$\":   \"float64\",\n",
        "        \"Real_SL\":       \"float64\",\n",
        "        \"ATR\":           \"float64\",\n",
        "        \"atr_mult_high\": \"float64\",\n",
        "        \"atr_mult_low\":  \"float64\",\n",
        "        \"trade_size\":    \"float64\",\n",
        "        \"profits\":       \"float64\",\n",
        "        \"base_px\":       \"float64\",\n",
        "        \"atr_base\":      \"float64\",\n",
        "        \"source\":        \"Int64\",\n",
        "        \"time\":          \"datetime64[ns, UTC]\",\n",
        "        \"open\":          \"float64\",\n",
        "        \"high\":          \"float64\",\n",
        "        \"low\":           \"float64\",\n",
        "        \"close\":         \"float64\",\n",
        "        \"volume\":        \"float64\",\n",
        "        \"tickVolume\":    \"float64\",\n",
        "        \"spread\":        \"float64\",\n",
        "    }\n",
        "\n",
        "    for c, dtp in col_types.items():\n",
        "        if c not in df.columns:\n",
        "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.NaT\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
        "            elif dtp == \"string\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
        "            else:\n",
        "                df[c] = np.nan\n",
        "\n",
        "    # normaliza sin romper datos existentes\n",
        "    for c, dtp in col_types.items():\n",
        "        try:\n",
        "            if dtp == \"string\":\n",
        "                df[c] = df[c].astype(\"string\")\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = df[c].astype(\"Int64\")\n",
        "            elif isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "            # floats/num los dejamos tal cual para no forzar conversiones peligrosas\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la √∫ltima fila\n",
        "    mode=\"missing\": rellena donde est√© NaT/NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "\n",
        "def _fmt_dt_cols(df: pd.DataFrame, cols=(\"System_time\", \"time\", \"Entry_Date\")) -> None:\n",
        "    \"\"\"Formatea columnas datetime a string 'YYYY-mm-dd HH:MM:SS' sin tz.\"\"\"\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Reordena columnas y guarda CSV con tiempos formateados.\n",
        "    Orden deseado:\n",
        "      ‚Ä¢ System_time antes de 'time'\n",
        "      ‚Ä¢ 'source' a la derecha de 'time' y antes de 'open'\n",
        "      ‚Ä¢ Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
        "      ‚Ä¢ Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    df_out = df.copy()\n",
        "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "    _fmt_dt_cols(df_out)\n",
        "\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols:\n",
        "            return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns:\n",
        "                    cols.insert(i, c)\n",
        "                    i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns and c not in cols:\n",
        "                    cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols:\n",
        "            cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols:\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else:\n",
        "                cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    cols = [c for c in df_out.columns if c not in (\"id\", \"brokerTime\", \"actionType\")]\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "\n",
        "    # Filtra por columnas existentes para evitar ValueError en to_csv\n",
        "    ordered = [c for c in ordered if c in df_out.columns]\n",
        "    df_out.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "\n",
        "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Migra CSV existente:\n",
        "      ‚Ä¢ Elimina columnas heredadas.\n",
        "      ‚Ä¢ Asegura columnas nuevas: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
        "      ‚Ä¢ Formatea tiempos.\n",
        "      ‚Ä¢ Reordena columnas al formato actual.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    ensure_cols = {\n",
        "        \"System_time\": pd.NaT,\n",
        "        \"profits\":     np.nan,\n",
        "        \"trade_size\":  np.nan,\n",
        "        \"base_px\":     np.nan,\n",
        "        \"atr_base\":    np.nan,\n",
        "        \"source\":      pd.NA,\n",
        "        \"Real_SL\":     np.nan,\n",
        "    }\n",
        "    for c, default in ensure_cols.items():\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    _fmt_dt_cols(df)\n",
        "\n",
        "    # Reusar l√≥gica de save_csv para reordenar\n",
        "    save_csv(df, path=path)\n",
        "\n",
        "\n",
        "def _load_csv(path: str = FILE_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware y elimina columnas heredadas.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        dtype={\n",
        "            \"orderId\": \"string\",\n",
        "            \"symbol\":  \"string\",\n",
        "            \"comment\": \"string\",\n",
        "            \"Type\":    \"string\",\n",
        "        }\n",
        "    )\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# METAAPI: CONEXI√ìN Y DATOS\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio, datetime as dt\n",
        "\n",
        "async def connect_metaapi(token: str, account_id: str, *, rpc_timeout=60, retries=3):\n",
        "    \"\"\"\n",
        "    Devuelve (account) con RPC intentado. Si la sincronizaci√≥n falla (DNS / timeout),\n",
        "    seguimos en modo REST con el mismo 'account' (las llamadas RPC tendr√°n fallback).\n",
        "    \"\"\"\n",
        "    if \"MetaApi\" not in globals() or MetaApi is None:\n",
        "        raise RuntimeError(\"metaapi_cloud_sdk no est√° disponible en el entorno.\")\n",
        "\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # Asegurar despliegue/conexi√≥n de la cuenta (no falla si el SDK no expone algo)\n",
        "    try:\n",
        "        await account.reload()\n",
        "        if getattr(account, \"state\", \"\").upper() != \"DEPLOYED\":\n",
        "            await account.deploy()\n",
        "            if hasattr(account, \"wait_deployed\"):\n",
        "                await account.wait_deployed()\n",
        "        if hasattr(account, \"wait_connected\"):\n",
        "            await account.wait_connected()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    conn = account.get_rpc_connection()\n",
        "\n",
        "    # Intentos de conectar/sincronizar RPC con backoff\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            await conn.connect()\n",
        "            try:\n",
        "                # seg√∫n versi√≥n del SDK, wait_synchronized puede o no aceptar timeout\n",
        "                await asyncio.wait_for(conn.wait_synchronized(), timeout=rpc_timeout)\n",
        "            except TypeError:\n",
        "                await conn.wait_synchronized()\n",
        "            except asyncio.TimeoutError:\n",
        "                raise TimeoutError(\"RPC wait_synchronized timeout\")\n",
        "            return account  # RPC OK\n",
        "        except Exception as e:\n",
        "            if attempt == retries:\n",
        "                logging.warning(\"RPC no sincroniz√≥ (%s). Continuando con REST-only; habr√° fallbacks.\", e)\n",
        "                return account\n",
        "            await asyncio.sleep(min(5 * attempt, 15))\n",
        "\n",
        "    return account\n",
        "\n",
        "\n",
        "\n",
        "async def get_current_candle_snapshot(account,\n",
        "                                      rpc_conn,\n",
        "                                      symbol: str = SYMBOL,\n",
        "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve 1 fila con la vela m√°s reciente (puede ser la vela en curso).\n",
        "    \"\"\"\n",
        "    def _to_row(c: dict) -> dict:\n",
        "        return {\n",
        "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
        "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
        "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
        "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
        "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
        "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
        "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
        "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
        "        }\n",
        "\n",
        "    # 1) RPC\n",
        "    try:\n",
        "        try:\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe, limit=1)\n",
        "        except TypeError:\n",
        "            to_ts = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe,\n",
        "                                                 start_time=from_ts, end_time=to_ts)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Hist√≥rico como respaldo\n",
        "    try:\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe=timeframe, start_time=None, limit=1)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
        "\n",
        "\n",
        "async def get_candles_5m(account, start: dt.datetime | None, limit: int = CANDEL_NUMBER):\n",
        "    \"\"\"Descarga velas hist√≥ricas (usa time_frame_data configurado).\"\"\"\n",
        "    candles = await account.get_historical_candles(symbol=SYMBOL, timeframe=time_frame_data, start_time=start, limit=limit)\n",
        "    return pd.DataFrame([{\n",
        "        \"time\": pd.to_datetime(c[\"time\"], utc=True),\n",
        "        \"open\": c[\"open\"], \"high\": c[\"high\"], \"low\": c[\"low\"], \"close\": c[\"close\"],\n",
        "        \"volume\": c[\"volume\"], \"tickVolume\": c[\"tickVolume\"], \"spread\": c[\"spread\"]\n",
        "    } for c in candles])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TIMEFRAME SYNC\n",
        "# ============================================================================\n",
        "def seconds_until_next_tf(tf: str = \"5m\", *, offset_sec: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Segundos hasta el pr√≥ximo cierre de vela del timeframe tf (+offset).\n",
        "    Soporta sufijos 'm','h','d'. Ej: '1m','5m','15m','1h','4h','1d'.\n",
        "    \"\"\"\n",
        "    tf = (tf or \"5m\").strip().lower()\n",
        "    if tf.endswith(\"mn\"):\n",
        "        tf = tf[:-2] + \"m\"\n",
        "    try:\n",
        "        if tf.endswith(\"m\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(minutes=max(step, 1))\n",
        "        elif tf.endswith(\"h\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(hours=max(step, 1))\n",
        "        elif tf.endswith(\"d\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(days=max(step, 1))\n",
        "        else:\n",
        "            delta = dt.timedelta(minutes=1)\n",
        "    except Exception:\n",
        "        delta = dt.timedelta(minutes=1)\n",
        "\n",
        "    now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "    epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "    secs = int((now - epoch).total_seconds())\n",
        "    stepS = int(delta.total_seconds())\n",
        "\n",
        "    next_boundary = ((secs // stepS) + 1) * stepS\n",
        "    target = epoch + dt.timedelta(seconds=next_boundary + max(offset_sec, 0))\n",
        "    wait_s = (target - now).total_seconds()\n",
        "    return max(wait_s, 0.5)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SE√ëALES / KALMAN\n",
        "# ============================================================================\n",
        "def generate_trade_signals(\n",
        "    df: pd.DataFrame,\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula 4 l√≠neas de Kalman sobre 'close' y crea:\n",
        "      ‚Ä¢ kal_1, kal_2, kal_3, kal_4\n",
        "      ‚Ä¢ Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
        "      ‚Ä¢ Close_Trade: -1 si kal_4 < kal_4.shift()  ‚Üí cierra BUY\n",
        "                     +1 si kal_4 > kal_4.shift()  ‚Üí cierra SELL\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if \"close\" not in df.columns:\n",
        "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
        "\n",
        "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").ffill()\n",
        "\n",
        "    def _clamp_int(x, mn=1):\n",
        "        try:\n",
        "            x = int(x)\n",
        "        except Exception:\n",
        "            x = mn\n",
        "        return max(x, mn)\n",
        "\n",
        "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
        "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
        "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
        "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
        "\n",
        "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
        "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
        "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
        "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
        "\n",
        "    k1_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1)\n",
        "    k2_up = df[\"kal_2\"] > df[\"kal_2\"].shift(1)\n",
        "    k3_up = df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
        "    k1_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1)\n",
        "    k2_dn = df[\"kal_2\"] < df[\"kal_2\"].shift(1)\n",
        "    k3_dn = df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
        "\n",
        "    bull = k1_up & k2_up & k3_up\n",
        "    bear = k1_dn & k2_dn & k3_dn\n",
        "    aux = np.where(bull, 1, np.where(bear, -1, np.nan))\n",
        "    df[\"Open_Trade\"] = np.where(pd.Series(aux).shift(1) != aux, aux, np.nan)\n",
        "\n",
        "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)\n",
        "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)\n",
        "    close_raw = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
        "    close_sr = pd.Series(close_raw)\n",
        "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REST HELPERS (MetaApi REST)\n",
        "# ============================================================================\n",
        "def _rest_place_order(auth_token: str,\n",
        "                      account_id: str,\n",
        "                      region: str,\n",
        "                      symbol: str,\n",
        "                      side: str,\n",
        "                      volume: float,\n",
        "                      comment: str = \"Kal\",\n",
        "                      magic: int | None = None,\n",
        "                      stop_loss: float | None = None,\n",
        "                      take_profit: float | None = None,\n",
        "                      timeout: int = 20):\n",
        "    side = side.upper().strip()\n",
        "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
        "    if side not in action_map:\n",
        "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
        "\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\n",
        "        \"symbol\": symbol,\n",
        "        \"actionType\": action_map[side],\n",
        "        \"volume\": float(volume),\n",
        "        \"comment\": str(comment)\n",
        "    }\n",
        "    if magic is not None:\n",
        "        try:\n",
        "            payload[\"magic\"] = int(magic)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if stop_loss is not None:\n",
        "        try:\n",
        "            payload[\"stopLoss\"] = float(stop_loss)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if take_profit is not None:\n",
        "        try:\n",
        "            payload[\"takeProfit\"] = float(take_profit)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "def _rest_get_positions(auth_token: str,\n",
        "                        account_id: str,\n",
        "                        region: str,\n",
        "                        symbol: str | None = None,\n",
        "                        timeout: int = 15):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
        "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    params = {}\n",
        "    if symbol:\n",
        "        params[\"symbol\"] = str(symbol)\n",
        "    try:\n",
        "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        class _Dummy:\n",
        "            status_code = 0\n",
        "            def json(self): return {\"error\": str(e)}\n",
        "            text = str(e)\n",
        "        return _Dummy()\n",
        "\n",
        "\n",
        "def _rest_modify_position(auth_token: str,\n",
        "                          account_id: str,\n",
        "                          region: str,\n",
        "                          position_id: str,\n",
        "                          stop_loss: float | None = None,\n",
        "                          take_profit: float | None = None,\n",
        "                          timeout: int = 20):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
        "    if stop_loss is not None:\n",
        "        payload[\"stopLoss\"] = float(stop_loss)\n",
        "    if take_profit is not None:\n",
        "        payload[\"takeProfit\"] = float(take_profit)\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "# --- REST fallback to close a position --------------------------------------\n",
        "def _rest_close_position(auth_token: str,\n",
        "                         account_id: str,\n",
        "                         region: str,\n",
        "                         position_id: str,\n",
        "                         timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Cierra una posici√≥n por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    # RPC con timeout corto para evitar cuelgues cuando el subscribe falla\n",
        "    async def _rpc_try(fn, *args, **kwargs):\n",
        "        try:\n",
        "            return await asyncio.wait_for(fn(*args, **kwargs), timeout=4)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    if rpc_conn:\n",
        "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
        "        if not positions:\n",
        "            positions = await _rpc_try(rpc_conn.get_positions)\n",
        "\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: CLOSE / ORDER\n",
        "# ============================================================================\n",
        "\n",
        "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"‚úò No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt:  return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close_rpc(pid: str) -> bool:\n",
        "        try:\n",
        "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
        "                return True\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid  = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "\n",
        "        ok = await _try_close_rpc(pid)\n",
        "        if not ok:\n",
        "            # REST fallback\n",
        "            r = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
        "            ok = getattr(r, \"status_code\", 0) == 200\n",
        "\n",
        "        if ok:\n",
        "            print(f\"‚úÖ Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "        else:\n",
        "            print(f\"‚úò No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: ABRIR / CERRAR / SINCRONIZAR / SL DIN√ÅMICO\n",
        "# ============================================================================\n",
        "async def open_trade(df: pd.DataFrame,\n",
        "                     rpc_conn,\n",
        "                     symbol: str = SYMBOL,\n",
        "                     lot: float = 1.0,\n",
        "                     comment: str = \"Kal\",\n",
        "                     magic: int = 900001):\n",
        "    \"\"\"\n",
        "    Abre mercado SOLO si NO hay posici√≥n con ese magic.\n",
        "    Al abrir, calcula y env√≠a el SL inicial:\n",
        "       SL = close ¬± ATR * INITIAL_SL  (seg√∫n BUY/SELL; INITIAL_SL suele ser negativo)\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                return int(pm) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt: return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    open_with_magic = [p for p in positions if _has_magic(p)]\n",
        "\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    row = df.index[-1]\n",
        "\n",
        "    # Ya hay posici√≥n con este magic ‚Üí sincroniza y sale\n",
        "    if open_with_magic:\n",
        "        p = open_with_magic[0]\n",
        "        df.at[row, \"System_time\"] = now_utc\n",
        "        pm = p.get(\"magic\")\n",
        "        pc = str(p.get(\"comment\") or \"\")\n",
        "        clean_cmt, mag_from_cmt = _split_comment_magic(pc)\n",
        "\n",
        "        if pm is not None and str(pm).strip() != \"\":\n",
        "            try: df.at[row, \"magic\"] = int(pm)\n",
        "            except Exception: df.at[row, \"magic\"] = int(magic)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.at[row, \"magic\"] = int(magic)\n",
        "\n",
        "        df.at[row, \"symbol\"]    = str(p.get(\"symbol\") or symbol)\n",
        "        df.at[row, \"openPrice\"] = float(p.get(\"openPrice\") or p.get(\"price\") or np.nan)\n",
        "        df.at[row, \"comment\"]   = clean_cmt or str(comment)\n",
        "\n",
        "        vol = p.get(\"volume\") or p.get(\"lots\") or None\n",
        "        if vol is not None:\n",
        "            try: df.at[row, \"trade_size\"] = float(vol)\n",
        "            except Exception: pass\n",
        "\n",
        "        side = _side_of(p)\n",
        "        if side: df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
        "        if pd.isna(df.at[row, \"Entry_Date\"]): df.at[row, \"Entry_Date\"] = now_utc\n",
        "\n",
        "        save_csv(df)\n",
        "        print(\"‚Ñπ Position already open; synced last row and skipped new order.\")\n",
        "        return\n",
        "\n",
        "    # No hay posici√≥n ‚Üí decidir lado por Open_Trade\n",
        "    sig = df[\"Open_Trade\"].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
        "    if side_req is None:\n",
        "        return\n",
        "\n",
        "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
        "    atr_val = float(df[\"ATR\"].iloc[-1]) if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
        "    atr_mult = 1.0\n",
        "\n",
        "    sl_to_send = tp_to_send = None\n",
        "    if np.isfinite(prev_close) and np.isfinite(atr_val):\n",
        "        dist = atr_val * atr_mult\n",
        "        if side_req == \"BUY\":\n",
        "            sl_to_send = prev_close - dist\n",
        "            tp_to_send = prev_close + dist\n",
        "        else:\n",
        "            sl_to_send = prev_close + dist\n",
        "            tp_to_send = prev_close - dist\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_place_order(\n",
        "            auth_token=META_API_TOKEN,\n",
        "            account_id=ACCOUNT_ID,\n",
        "            region=REGION,\n",
        "            symbol=symbol,\n",
        "            side=side_req,\n",
        "            volume=float(lot),\n",
        "            comment=str(comment),\n",
        "            magic=int(magic),\n",
        "            stop_loss=sl_to_send,\n",
        "            take_profit=tp_to_send,\n",
        "            timeout=20\n",
        "        )\n",
        "    )\n",
        "    if getattr(resp, \"status_code\", 0) != 200:\n",
        "        try: err = resp.json()\n",
        "        except Exception: err = {\"raw\": getattr(resp, \"text\", \"\")[:500]}\n",
        "        print(\"‚úò Order failed\", getattr(resp, \"status_code\", None), json.dumps(err, indent=2, ensure_ascii=False))\n",
        "        return\n",
        "\n",
        "    data = resp.json()\n",
        "    order_id = str(data.get(\"orderId\") or \"\")\n",
        "    position_id = str(data.get(\"positionId\") or \"\")\n",
        "    if position_id and (sl_to_send is not None or tp_to_send is not None):\n",
        "        resp_mod = await loop.run_in_executor(\n",
        "            None,\n",
        "            lambda: _rest_modify_position(\n",
        "                auth_token=META_API_TOKEN,\n",
        "                account_id=ACCOUNT_ID,\n",
        "                region=REGION,\n",
        "                position_id=position_id,\n",
        "                stop_loss=sl_to_send,\n",
        "                take_profit=tp_to_send,\n",
        "                timeout=15\n",
        "            )\n",
        "        )\n",
        "        if getattr(resp_mod, \"status_code\", 0) != 200:\n",
        "            try:\n",
        "                err_mod = resp_mod.json()\n",
        "            except Exception:\n",
        "                err_mod = {\"raw\": getattr(resp_mod, \"text\", \"\")[:500]}\n",
        "            print(\"‚úò Modify failed\", getattr(resp_mod, \"status_code\", None), json.dumps(err_mod, indent=2, ensure_ascii=False))\n",
        "\n",
        "    df.at[row, \"System_time\"] = now_utc\n",
        "    df.at[row, \"orderId\"]     = order_id\n",
        "    df.at[row, \"magic\"]       = int(magic)\n",
        "    df.at[row, \"symbol\"]      = symbol\n",
        "    df.at[row, \"comment\"]     = str(comment)\n",
        "    df.at[row, \"Entry_Date\"]  = now_utc\n",
        "    df.at[row, \"trade_size\"]  = float(lot)\n",
        "\n",
        "    if sl_to_send is not None and np.isfinite(sl_to_send):\n",
        "        df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
        "        df.at[row, \"Stop_Loss_atr\"] = float(atr_mult)\n",
        "    if tp_to_send is not None and np.isfinite(tp_to_send):\n",
        "        df.at[row, \"Take_Profit_$\"]   = float(tp_to_send)\n",
        "        df.at[row, \"Take_Profit_atr\"] = float(atr_mult)\n",
        "    # Recuperar openPrice y Type desde API\n",
        "    open_price, fetched_typ = np.nan, None\n",
        "    if rpc_conn:\n",
        "        try:\n",
        "            for _ in range(60):\n",
        "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "                match = None\n",
        "                for p in (pos_list or []):\n",
        "                    pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                    if pid == position_id or pid == order_id or _has_magic(p):\n",
        "                        match = p; break\n",
        "                if match:\n",
        "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
        "                    if val is not None: open_price = float(val)\n",
        "                    t = match.get(\"type\")\n",
        "                    if isinstance(t, str):\n",
        "                        tu = t.upper(); fetched_typ = \"Long\" if \"BUY\" in tu else (\"Short\" if \"SELL\" in tu else None)\n",
        "                    elif t == 0:\n",
        "                        fetched_typ = \"Long\"\n",
        "                    elif t == 1:\n",
        "                        fetched_typ = \"Short\"\n",
        "                    break\n",
        "                await asyncio.sleep(0.5)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not np.isfinite(open_price):\n",
        "        try: open_price = float(df.at[row, \"close\"])\n",
        "        except Exception: open_price = np.nan\n",
        "\n",
        "    df.at[row, \"openPrice\"] = open_price\n",
        "    df.at[row, \"Type\"] = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
        "\n",
        "    save_csv(df)\n",
        "    print(f\"‚úÖ {side_req} placed | orderId={order_id} positionId={position_id} \"\n",
        "          f\"openPrice={open_price if np.isfinite(open_price) else 'NaN'} \"\n",
        "          f\"| SL sent={sl_to_send if sl_to_send is not None else 'None'} \"\n",
        "          f\"| TP sent={tp_to_send if tp_to_send is not None else 'None'} \"\n",
        "          f\"| Type={df.at[row,'Type']}\")\n",
        "\n",
        "\n",
        "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Por bloque de trade:\n",
        "      ‚Ä¢ FFill de metadatos\n",
        "      ‚Ä¢ base_px / atr_base desde la apertura del bloque\n",
        "      ‚Ä¢ atr_mult_high/low\n",
        "      ‚Ä¢ 'profits' solo si est√° NaN\n",
        "      ‚Ä¢ Propaga 'Real_SL' dentro del bloque con el √∫ltimo valor no nulo\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    for c in ('time', 'Entry_Date'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
        "\n",
        "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "        else:\n",
        "            df[c].values[:] = np.nan\n",
        "\n",
        "    if 'profits' not in df.columns:\n",
        "        df['profits'] = np.nan\n",
        "    if 'base_px' not in df.columns:\n",
        "        df['base_px'] = np.nan\n",
        "    if 'atr_base' not in df.columns:\n",
        "        df['atr_base'] = np.nan\n",
        "    if 'Real_SL' not in df.columns:\n",
        "        df['Real_SL'] = np.nan\n",
        "\n",
        "    starts_mask = pd.Series(False, index=df.index)\n",
        "    if 'orderId' in df.columns:\n",
        "        oid = df['orderId']\n",
        "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
        "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
        "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
        "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
        "    if not starts_mask.any():\n",
        "        return df\n",
        "\n",
        "    groups = starts_mask.cumsum()\n",
        "    trade_ids = groups[starts_mask].unique()\n",
        "\n",
        "    meta_cols = [\"orderId\", \"magic\", \"symbol\", \"openPrice\", \"comment\", \"Type\", \"Entry_Date\", \"trade_size\"]\n",
        "\n",
        "    for gid in trade_ids:\n",
        "        mask = (groups == gid)\n",
        "        start_idx = df.index[mask][0]\n",
        "\n",
        "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
        "        try:\n",
        "            base_px = float(base_px)\n",
        "        except Exception:\n",
        "            base_px = np.nan\n",
        "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
        "            try:\n",
        "                base_px = float(df.at[start_idx, 'close'])\n",
        "            except Exception:\n",
        "                base_px = np.nan\n",
        "\n",
        "        atr_base = np.nan\n",
        "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'atr_base'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'ATR'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "\n",
        "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
        "\n",
        "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
        "\n",
        "        if np.isfinite(base_px):\n",
        "            df.loc[mask, 'base_px'] = base_px\n",
        "        if np.isfinite(atr_base):\n",
        "            df.loc[mask, 'atr_base'] = atr_base\n",
        "\n",
        "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long', 'Short'):\n",
        "            if typ == 'Long':\n",
        "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((df.loc[mask, 'low'] - base_px) / atr_base).round(2)\n",
        "            else:\n",
        "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((base_px - df.loc[mask, 'low']) / atr_base).round(2)\n",
        "\n",
        "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
        "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long', 'Short'):\n",
        "            m_nan = mask & df['profits'].isna()\n",
        "            if m_nan.any():\n",
        "                if typ == 'Long':\n",
        "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
        "                else:\n",
        "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
        "\n",
        "    # Propaga Real_SL por bloque\n",
        "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
        "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
        "    else:\n",
        "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
        "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
        "\n",
        "    if starts.any():\n",
        "        grp = starts.cumsum()\n",
        "        for gid in grp[starts].unique():\n",
        "            m = (grp == gid)\n",
        "            ser = df.loc[m, 'Real_SL']\n",
        "            if ser.notna().any():\n",
        "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
        "                df.loc[m, 'Real_SL'] = val\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def tick_dyn_atr(df: pd.DataFrame,\n",
        "                 initial_atr: float = INITIAL_SL,\n",
        "                 first_step_atr: float = FIRST_STEP_ATR,\n",
        "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Din√°mica de stop en m√∫ltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
        "      ‚Ä¢ Escribe 'tick_dyn_atr' (m√∫ltiplos)\n",
        "      ‚Ä¢ Calcula y actualiza 'Stop_Loss_$' y 'Stop_Loss_atr'\n",
        "    \"\"\"\n",
        "    col_name = 'tick_dyn_atr'\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = np.nan\n",
        "    if 'Stop_Loss_$' not in df.columns:\n",
        "        df['Stop_Loss_$'] = np.nan\n",
        "    if 'Stop_Loss_atr' not in df.columns:\n",
        "        df['Stop_Loss_atr'] = np.nan\n",
        "\n",
        "    in_trade = False\n",
        "    trade_active = False\n",
        "    broken = False\n",
        "    sl_val = float(initial_atr)\n",
        "    next_threshold = float(first_step_atr)\n",
        "    prev_sl = float(initial_atr)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        new_open = (\n",
        "            (('orderId' in df.columns) and pd.notna(row.get('orderId'))) or\n",
        "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
        "            (pd.notna(row.get('Entry_Date')))\n",
        "        )\n",
        "\n",
        "        if new_open and not in_trade:\n",
        "            in_trade = True\n",
        "            trade_active = True\n",
        "            broken = False\n",
        "            sl_val = float(initial_atr)\n",
        "            next_threshold = float(first_step_atr)\n",
        "            prev_sl = sl_val\n",
        "            entry_dt = row.get('time')\n",
        "            if entry_dt is not None:\n",
        "                df.at[idx, 'Entry_Date'] = entry_dt\n",
        "\n",
        "        if in_trade:\n",
        "            m_high = row.get('atr_mult_high', np.nan)\n",
        "            m_low = row.get('atr_mult_low', np.nan)\n",
        "            best_pnl = np.nanmax([m_high, m_low])\n",
        "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
        "\n",
        "            if trade_active and not broken:\n",
        "                while best_pnl >= next_threshold:\n",
        "                    sl_val += float(gap_first_step_atr)\n",
        "                    next_threshold += float(gap_first_step_atr)\n",
        "\n",
        "                below_prev = (\n",
        "                    (np.isfinite(m_high) and float(m_high) < prev_sl) or\n",
        "                    (np.isfinite(m_low) and float(m_low) < prev_sl)\n",
        "                )\n",
        "                if below_prev:\n",
        "                    broken = True\n",
        "                    trade_active = False\n",
        "                    in_trade = False\n",
        "\n",
        "            df.at[idx, col_name] = np.nan if broken else sl_val\n",
        "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
        "            prev_sl = sl_val\n",
        "\n",
        "    # Convierte m√∫ltiplos a precio\n",
        "    try:\n",
        "        atr_mult = df[col_name].astype(float)\n",
        "        base = df['base_px'].astype(float)\n",
        "        atr = df['atr_base'].astype(float)\n",
        "        typ = df['Type'].astype('string')\n",
        "\n",
        "        stop_price = np.where(\n",
        "            (typ == 'Long') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "            base + atr * atr_mult,\n",
        "            np.where(\n",
        "                (typ == 'Short') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "                base - atr * atr_mult,\n",
        "                np.nan\n",
        "            )\n",
        "        )\n",
        "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    \"\"\"\n",
        "    Cierra posiciones seg√∫n la pendiente de kal_4 codificada en Close_Trade:\n",
        "      ‚Ä¢ Close_Trade == -1  ‚Üí cerrar BUY\n",
        "      ‚Ä¢ Close_Trade == +1  ‚Üí cerrar SELL\n",
        "    \"\"\"\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"‚úò No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close(pid: str) -> bool:\n",
        "        try:\n",
        "            await rpc_conn.close_position(pid)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await rpc_conn.close_position({\"positionId\": pid})\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"‚úò Fall√≥ cierre positionId={pid}: {e}\")\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "        ok = await _try_close(pid)\n",
        "        if ok:\n",
        "            print(f\"‚úÖ Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "\n",
        "\n",
        "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
        "                                 rpc_conn,\n",
        "                                 symbol: str = SYMBOL,\n",
        "                                 magic: int = 900001,\n",
        "                                 tol: float = 0.01) -> None:\n",
        "    \"\"\"\n",
        "    Copia desde la posici√≥n viva los campos de mercado al DF.\n",
        "    'profits' solo se escribe en la √öLTIMA FILA para no sobreescribir hist√≥rico.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt:\n",
        "            return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    pos = next((p for p in positions if _has_magic(p) and _sym_ok(p)), None)\n",
        "    if not pos:\n",
        "        pos = next((p for p in positions if _has_magic(p)), None)\n",
        "    if not pos:\n",
        "        return\n",
        "\n",
        "    order_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    magic_val = pos.get(\"magic\")\n",
        "    symbol_val = pos.get(\"symbol\")\n",
        "    open_price = pos.get(\"openPrice\") or pos.get(\"price\")\n",
        "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\") or None\n",
        "    stop_loss = pos.get(\"stopLoss\")\n",
        "    volume_val = pos.get(\"volume\") or pos.get(\"lots\")\n",
        "    profit_val = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
        "                  else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
        "\n",
        "    t = pos.get(\"type\")\n",
        "    typ = None\n",
        "    if isinstance(t, str):\n",
        "        tu = t.upper()\n",
        "        if \"BUY\" in tu:  typ = \"Long\"\n",
        "        if \"SELL\" in tu: typ = \"Short\"\n",
        "    elif t == 0:\n",
        "        typ = \"Long\"\n",
        "    elif t == 1:\n",
        "        typ = \"Short\"\n",
        "\n",
        "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
        "    clean_cmt, mag_from_cmt = _split_comment_magic(str(comment_raw or \"\"))\n",
        "\n",
        "    block_mask = pd.Series(False, index=df.index)\n",
        "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
        "        block_mask = (df[\"orderId\"] == order_id)\n",
        "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
        "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
        "        starts = ed.notna() & (ed != ed.shift(1))\n",
        "        if starts.any():\n",
        "            last_start = df.index[starts].max()\n",
        "            block_mask = (df.index >= last_start)\n",
        "        else:\n",
        "            block_mask = ed.notna() & (ed >= entry_dt)\n",
        "    if not block_mask.any():\n",
        "        block_mask.iloc[-1] = True\n",
        "\n",
        "    try:\n",
        "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
        "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.loc[block_mask, \"magic\"] = int(magic)\n",
        "\n",
        "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
        "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
        "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
        "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
        "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
        "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
        "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    last_idx = df.index[-1]\n",
        "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
        "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
        "    elif mag_from_cmt is not None:\n",
        "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
        "    else:\n",
        "        df.at[last_idx, \"magic\"] = int(magic)\n",
        "\n",
        "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
        "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
        "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
        "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
        "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
        "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
        "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
        "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
        "\n",
        "\n",
        "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
        "    \"\"\"Devuelve (prev, last) con los dos √∫ltimos valores no-NaN distintos.\"\"\"\n",
        "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        return (np.nan, np.nan)\n",
        "    last = float(s.iloc[-1])\n",
        "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
        "    return (prev, last)\n",
        "\n",
        "\n",
        "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
        "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\"):\n",
        "            if key in p and p[key] is not None:\n",
        "                try:\n",
        "                    if int(p[key]) == int(magic):\n",
        "                        return True\n",
        "                except Exception:\n",
        "                    if str(p[key]).strip() == str(magic):\n",
        "                        return True\n",
        "        if f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    for p in positions:\n",
        "        if _sym_ok(p) and _has_magic(p):\n",
        "            return p\n",
        "    for p in positions:\n",
        "        if _has_magic(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
        "                                     rpc_conn,\n",
        "                                     *,\n",
        "                                     symbol: str,\n",
        "                                     magic: int,\n",
        "                                     auth_token: str,\n",
        "                                     account_id: str,\n",
        "                                     region: str,\n",
        "                                     tol: float = 0.0) -> dict:\n",
        "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
        "    if not np.isfinite(last_sl):\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vac√≠o\"}\n",
        "\n",
        "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
        "    if not changed:\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
        "\n",
        "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "\n",
        "    if not pos:\n",
        "        last_oid = None\n",
        "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
        "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
        "        if last_oid:\n",
        "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "            for p in positions:\n",
        "                pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                if pid == last_oid:\n",
        "                    pos = p\n",
        "                    break\n",
        "\n",
        "    if not pos:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posici√≥n con ese magic\"}\n",
        "\n",
        "    position_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    if not position_id:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vac√≠o\"}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_modify_position(\n",
        "            auth_token=auth_token,\n",
        "            account_id=account_id,\n",
        "            region=region,\n",
        "            position_id=position_id,\n",
        "            stop_loss=float(last_sl),\n",
        "            timeout=15\n",
        "        )\n",
        "    )\n",
        "    ok = getattr(resp, \"status_code\", 0) == 200\n",
        "    err = None\n",
        "    if not ok:\n",
        "        try:\n",
        "            err = json.dumps(resp.json())[:300]\n",
        "        except Exception:\n",
        "            err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
        "\n",
        "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
        "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
        "\n",
        "\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    try:\n",
        "        positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    if not positions:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions() or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cvhz3M-fTIw",
      "metadata": {
        "id": "5cvhz3M-fTIw"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    \"\"\"\n",
        "    Bucle principal:\n",
        "      ‚Ä¢ Crea/migra el CSV inicial (ATR interno, se√±ales Kalman).\n",
        "      ‚Ä¢ Cada 5 minutos procesa la √∫ltima vela cerrada y, si corresponde,\n",
        "        abre una operaci√≥n con stop-loss y take-profit.\n",
        "    \"\"\"\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 0) Conexi√≥n MetaApi / RPC\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # Par√°metros (con defaults tolerantes a faltantes globales)\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    MAGIC    = 900001\n",
        "    LENGTHS  = (300, 410, 710, 870)\n",
        "    SMOOTHS  = (3, 3, 3, 5)\n",
        "    LOT_     = globals().get(\"LOT\", 1.0)\n",
        "    COMMENT_ = globals().get(\"COMMENT\", \"Kal\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # Helpers locales\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    import datetime as dt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    def _parse_tf_to_delta(tf: str) -> dt.timedelta:\n",
        "        \"\"\"'1m','5m','15m','1h','4h','1d' ‚Üí timedelta (fallback 1m).\"\"\"\n",
        "        tf = (tf or \"1m\").strip().lower()\n",
        "        if tf.endswith(\"mn\"):\n",
        "            tf = tf[:-2] + \"m\"\n",
        "        try:\n",
        "            if tf.endswith(\"m\"):\n",
        "                return dt.timedelta(minutes=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"h\"):\n",
        "                return dt.timedelta(hours=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"d\"):\n",
        "                return dt.timedelta(days=max(int(tf[:-1]), 1))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return dt.timedelta(minutes=1)\n",
        "\n",
        "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
        "        \"\"\"Floor de ts a m√∫ltiplo exacto del timeframe (UTC).\"\"\"\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
        "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        secs  = int((ts - epoch).total_seconds())\n",
        "        step  = int(delta.total_seconds()) or 60\n",
        "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
        "\n",
        "    def _calc_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
        "        \"\"\"\n",
        "        ATR estilo Wilder: TR = max(H-L, |H-C1|, |L-C1|), ATR = RMA(TR, period).\n",
        "        Usa ewm(alpha=1/period) como aproximaci√≥n de RMA.\n",
        "        \"\"\"\n",
        "        h, l, c = df[\"high\"].astype(float), df[\"low\"].astype(float), df[\"close\"].astype(float)\n",
        "        c1 = c.shift(1)\n",
        "        tr = np.maximum.reduce([\n",
        "            (h - l).to_numpy(),\n",
        "            (h - c1).abs().to_numpy(),\n",
        "            (l - c1).abs().to_numpy()\n",
        "        ])\n",
        "        atr = pd.Series(tr, index=df.index).ewm(alpha=1/period, adjust=False).mean()\n",
        "        return atr.round(4)\n",
        "\n",
        "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
        "        \"\"\"True si existe posici√≥n con ese magic (prefiere helper global si existe).\"\"\"\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            return pos is not None\n",
        "        except Exception:\n",
        "            positions = []\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            if not positions:\n",
        "                r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "                if getattr(r, \"status_code\", 0) == 200:\n",
        "                    try:\n",
        "                        positions = r.json() or []\n",
        "                    except Exception:\n",
        "                        positions = []\n",
        "            if not positions:\n",
        "                return False\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if ok:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "    async def _get_api_type(rpc_conn, symbol: str, magic: int):\n",
        "        \"\"\"'Long' / 'Short' / None usando get_pos_with_magic si existe.\"\"\"\n",
        "        side = None\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            if not pos:\n",
        "                return None\n",
        "            t = pos.get(\"type\")\n",
        "            if isinstance(t, str):\n",
        "                tu = t.upper()\n",
        "                side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "            elif t == 0:\n",
        "                side = \"BUY\"\n",
        "            elif t == 1:\n",
        "                side = \"SELL\"\n",
        "        except Exception:\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if not ok:\n",
        "                    continue\n",
        "                t = p.get(\"type\")\n",
        "                if isinstance(t, str):\n",
        "                    tu = t.upper()\n",
        "                    side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "                elif t == 0:\n",
        "                    side = \"BUY\"\n",
        "                elif t == 1:\n",
        "                    side = \"SELL\"\n",
        "                break\n",
        "        if side is None:\n",
        "            return None\n",
        "        return \"Long\" if side == \"BUY\" else \"Short\"\n",
        "\n",
        "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: str | None) -> None:\n",
        "        \"\"\"Escribe 'Type' en el bloque activo o en la √∫ltima fila si no se detecta bloque.\"\"\"\n",
        "        if not api_type or df_all.empty:\n",
        "            return\n",
        "        last_oid = df_all.get(\"orderId\")\n",
        "        if last_oid is not None and last_oid.notna().any():\n",
        "            last_oid_val = last_oid.dropna().iloc[-1]\n",
        "            mask = (df_all[\"orderId\"] == last_oid_val)\n",
        "        else:\n",
        "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
        "            starts = ed.notna() & (ed != ed.shift(1))\n",
        "            if starts.any():\n",
        "                start_idx = df_all.index[starts].max()\n",
        "                mask = (df_all.index >= start_idx)\n",
        "            else:\n",
        "                mask = pd.Series(False, index=df_all.index)\n",
        "        if mask.any():\n",
        "            df_all.loc[mask, \"Type\"] = api_type\n",
        "        else:\n",
        "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 1) Crear/migrar archivo inicial\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        df = await get_candles_5m(account, start=None, limit=CANDEL_NUMBER)\n",
        "        if len(df) >= 14:\n",
        "            df[\"ATR\"] = _calc_atr(df, 14)\n",
        "        l1, l2, l3, l4 = LENGTHS\n",
        "        s1, s2, s3, s4 = SMOOTHS\n",
        "        generate_trade_signals(df, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "        _ensure_order_cols(df)\n",
        "        stamp_system_time(df, \"last\")\n",
        "        save_csv(df)\n",
        "        print(f\"‚úî Archivo inicial creado con {len(df)} velas\")\n",
        "    else:\n",
        "        migrate_csv_if_needed(FILE_PATH)\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # 2) Loop principal\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    while True:\n",
        "        await asyncio.sleep(seconds_until_next_tf(time_frame_data, offset_sec=3))\n",
        "\n",
        "        now_utc  = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "        delta    = _parse_tf_to_delta(time_frame_data)\n",
        "        this_bar = _floor_to_frame(now_utc, delta)\n",
        "        prev_bar = this_bar - delta  # √∫ltima vela CERRADA\n",
        "\n",
        "        df_new = await get_candles_5m(account, start=None, limit=50)\n",
        "        df_new = (df_new[df_new[\"time\"] <= prev_bar]\n",
        "                  .drop_duplicates(\"time\")\n",
        "                  .sort_values(\"time\"))\n",
        "        if not df_new.empty:\n",
        "            df_new = df_new.iloc[[-1]]\n",
        "\n",
        "        df_all = _load_csv()\n",
        "        existing_times = set(pd.to_datetime(df_all[\"time\"], utc=True)) if (not df_all.empty and \"time\" in df_all.columns) else set()\n",
        "\n",
        "        if df_all.empty:\n",
        "            df_all = df_new.copy()\n",
        "        else:\n",
        "            df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
        "                      .drop_duplicates(\"time\")\n",
        "                      .sort_values(\"time\")\n",
        "                      .reset_index(drop=True))\n",
        "\n",
        "        if len(df_all) >= 14:\n",
        "            df_all[\"ATR\"] = _calc_atr(df_all, 14)\n",
        "\n",
        "        l1, l2, l3, l4 = LENGTHS\n",
        "        s1, s2, s3, s4 = SMOOTHS\n",
        "        generate_trade_signals(df_all, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "        _ensure_order_cols(df_all)\n",
        "\n",
        "        if not df_new.empty:\n",
        "            new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
        "            if new_times:\n",
        "                df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
        "\n",
        "        await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT_, comment=COMMENT_, magic=MAGIC)\n",
        "\n",
        "        api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
        "        _sync_type_in_df(df_all, api_type)\n",
        "        await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "        stamp_system_time(df_all, \"last\")\n",
        "        save_csv(df_all)\n",
        "        print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"), \"| actualizaci√≥n (ciclo \"+time_frame_data+\")\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W481PrVufZ8Z",
      "metadata": {
        "id": "W481PrVufZ8Z"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yI6v2q_xfRtg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI6v2q_xfRtg",
        "outputId": "a897cd2c-7dd6-441e-c56b-8ef656e0ddb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16:44:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215698030 positionId=1215698030 openPrice=112490.0 | SL sent=112426.8651 | TP sent=112573.6349 | Type=Long\n",
            "16:45:07 | actualizaci√≥n (ciclo 1m)\n",
            "16:46:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215700484 positionId=1215700484 openPrice=112372.5 | SL sent=112444.7924 | TP sent=112297.2076 | Type=Short\n",
            "16:47:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:48:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:49:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:50:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:51:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:52:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:53:05 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215707272 positionId=1215707272 openPrice=112444.88 | SL sent=112369.5733 | TP sent=112493.1867 | Type=Long\n",
            "16:54:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "16:55:04 | actualizaci√≥n (ciclo 1m)\n",
            "16:56:04 | actualizaci√≥n (ciclo 1m)\n",
            "16:57:04 | actualizaci√≥n (ciclo 1m)\n",
            "16:58:04 | actualizaci√≥n (ciclo 1m)\n",
            "16:59:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:00:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:01:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215717484 positionId=1215717484 openPrice=112348.38 | SL sent=112396.34730000001 | TP sent=112279.4127 | Type=Short\n",
            "17:02:07 | actualizaci√≥n (ciclo 1m)\n",
            "17:03:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:04:07 | actualizaci√≥n (ciclo 1m)\n",
            "17:05:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:06:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:07:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:08:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:09:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:10:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215729262 positionId=1215729262 openPrice=112386.38 | SL sent=112313.5545 | TP sent=112427.9455 | Type=Long\n",
            "17:11:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:12:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:13:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:14:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:15:05 | actualizaci√≥n (ciclo 1m)\n",
            "17:16:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:17:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:18:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:19:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:20:04 | actualizaci√≥n (ciclo 1m)\n",
            "17:21:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:22:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:23:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:24:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:25:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:26:07 | actualizaci√≥n (ciclo 1m)\n",
            "17:27:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:28:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215745166 positionId=1215745166 openPrice=112567.63 | SL sent=112503.33380000001 | TP sent=112595.9262 | Type=Long\n",
            "17:29:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:30:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215748004 positionId=1215748004 openPrice=112483.13 | SL sent=112530.3641 | TP sent=112434.8959 | Type=Short\n",
            "17:31:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:32:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:33:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:34:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:35:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:36:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "17:37:05 | actualizaci√≥n (ciclo 1m)\n",
            "17:38:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:39:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:40:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:41:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:42:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:43:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:44:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:45:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:46:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:47:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:48:07 | actualizaci√≥n (ciclo 1m)\n",
            "17:49:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:50:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:51:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215767192 positionId=1215767192 openPrice=112198.88 | SL sent=112277.6351 | TP sent=112178.12490000001 | Type=Short\n",
            "17:52:07 | actualizaci√≥n (ciclo 1m)\n",
            "17:53:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:54:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:55:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:56:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:57:06 | actualizaci√≥n (ciclo 1m)\n",
            "17:58:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215774806 positionId=1215774806 openPrice=112227.63 | SL sent=112162.6651 | TP sent=112270.59490000001 | Type=Long\n",
            "17:59:08 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:00:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:01:04 | actualizaci√≥n (ciclo 1m)\n",
            "18:02:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:03:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:04:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:05:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:06:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:07:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:08:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215786103 positionId=1215786103 openPrice=112255.13 | SL sent=112292.02440000001 | TP sent=112187.2356 | Type=Short\n",
            "18:09:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:10:05 | actualizaci√≥n (ciclo 1m)\n",
            "18:11:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:12:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215791509 positionId=1215791509 openPrice=112259.63 | SL sent=112317.1762 | TP sent=112210.08380000001 | Type=Short\n",
            "18:13:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215792381 positionId=1215792381 openPrice=112323.25 | SL sent=112266.0071 | TP sent=112375.2529 | Type=Long\n",
            "18:14:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:15:04 | actualizaci√≥n (ciclo 1m)\n",
            "18:16:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:17:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:18:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:19:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:20:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:21:07 | actualizaci√≥n (ciclo 1m)\n",
            "18:22:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:23:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215803616 positionId=1215803616 openPrice=112395.13 | SL sent=112426.84670000001 | TP sent=112329.4133 | Type=Short\n",
            "18:24:07 | actualizaci√≥n (ciclo 1m)\n",
            "18:25:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:26:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:27:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:28:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:29:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:30:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:31:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215810661 positionId=1215810661 openPrice=112376.0 | SL sent=112426.3065 | TP sent=112340.1935 | Type=Short\n",
            "18:32:08 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:33:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:34:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:35:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:36:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:37:04 | actualizaci√≥n (ciclo 1m)\n",
            "18:38:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:39:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:40:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:41:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:42:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:43:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215820678 positionId=1215820678 openPrice=112361.88 | SL sent=112307.1072 | TP sent=112391.65280000001 | Type=Long\n",
            "18:44:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:45:04 | actualizaci√≥n (ciclo 1m)\n",
            "18:46:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:47:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:48:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:49:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:50:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:51:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:52:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:53:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:54:07 | actualizaci√≥n (ciclo 1m)\n",
            "18:55:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215833459 positionId=1215833459 openPrice=112514.63 | SL sent=112463.4929 | TP sent=112539.76710000001 | Type=Long\n",
            "18:56:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "18:57:04 | actualizaci√≥n (ciclo 1m)\n",
            "18:58:06 | actualizaci√≥n (ciclo 1m)\n",
            "18:59:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:00:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215840978 positionId=1215840978 openPrice=112462.63 | SL sent=112494.6888 | TP sent=112415.5712 | Type=Short\n",
            "19:01:07 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "19:02:04 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "19:03:04 | actualizaci√≥n (ciclo 1m)\n",
            "19:04:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:05:07 | actualizaci√≥n (ciclo 1m)\n",
            "19:06:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:07:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:08:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:09:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:10:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:11:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:12:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:13:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:14:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:15:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215857440 positionId=1215857440 openPrice=112198.38 | SL sent=112247.71440000001 | TP sent=112157.0456 | Type=Short\n",
            "19:16:08 | actualizaci√≥n (ciclo 1m)\n",
            "19:17:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ SELL placed | orderId=1215859080 positionId=1215859080 openPrice=112213.88 | SL sent=112255.6977 | TP sent=112168.0623 | Type=Short\n",
            "19:18:07 | actualizaci√≥n (ciclo 1m)\n",
            "19:19:06 | actualizaci√≥n (ciclo 1m)\n",
            "19:20:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚úÖ BUY placed | orderId=1215861509 positionId=1215861509 openPrice=112243.38 | SL sent=112188.7053 | TP sent=112274.05470000001 | Type=Long\n",
            "19:21:06 | actualizaci√≥n (ciclo 1m)\n",
            "‚Ñπ Position already open; synced last row and skipped new order.\n",
            "19:22:04 | actualizaci√≥n (ciclo 1m)\n"
          ]
        }
      ],
      "source": [
        "###############################################################################\n",
        "# EJECUCI√ìN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()     # solo en notebooks\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}