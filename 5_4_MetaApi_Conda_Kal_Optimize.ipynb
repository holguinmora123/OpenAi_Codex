{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaf03429",
      "metadata": {
        "id": "eaf03429"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nu7i62hlGthO",
      "metadata": {
        "id": "nu7i62hlGthO"
      },
      "outputs": [],
      "source": [
        "!pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ej4IeD7EeF0t",
      "metadata": {
        "id": "Ej4IeD7EeF0t"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade metaapi-cloud-sdk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DF5CD6j5nmdo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5CD6j5nmdo",
        "outputId": "d62877ad-7c9d-4e19-81b6-9c099b8d6b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.7\n",
            "ATR(ndarray high, ndarray low, ndarray close, int timeperiod=-0x80000000)\n",
            "\n",
            "ATR(high, low, close[, timeperiod=?])\n",
            "\n",
            "Average True Range (Volatility Indicators)\n",
            "\n",
            "Inputs:\n",
            "    prices: ['high', 'low', 'close']\n",
            "Parameters:\n",
            "    timeperiod: 14\n",
            "Outputs:\n",
            "    real\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function talib._ta_lib.ATR(high, low, close, timeperiod=-2147483648)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import talib as ta\n",
        "print(ta.__version__)  # Should print something like 0.4.28\n",
        "print(ta.ATR.__doc__)  # Confirm ATR function works\n",
        "ta.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime as dt\n",
        "import logging\n",
        "\n",
        "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
        "\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from metaapi_cloud_sdk import MetaApi\n",
        "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import json, re, traceback, requests\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "try:\n",
        "    from metaapi_cloud_sdk import MetaApi\n",
        "except Exception:\n",
        "    MetaApi = None  # permite importar el módulo incluso sin el SDK instalado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KFfn45ty82qv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFfn45ty82qv",
        "outputId": "24766c49-4816-4c2e-e1e9-4977903066ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5RqjoAtFwl",
        "outputId": "642f8a02-c449-4a60-8656-12f8fc4b8216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Course Folder/Forex/BTCUSD/\n"
          ]
        }
      ],
      "source": [
        "process = 'Train'\n",
        "SYMBOL = 'BTCUSD'\n",
        "\n",
        "root_data = f'/content/drive/MyDrive/Course Folder/Forex/{SYMBOL}/'\n",
        "print(root_data)\n",
        "\n",
        "rolling_window = 100\n",
        "\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI'\n",
        "ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'\n",
        "\n",
        "LOT      = 1.0\n",
        "COMMENT  = \"Insta\"\n",
        "CANDLE_NUMBER = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dTlwjirFwRfj",
      "metadata": {
        "id": "dTlwjirFwRfj"
      },
      "outputs": [],
      "source": [
        "# cache simple en módulo\n",
        "__CONNECTION_CHECKED = False\n",
        "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
        "\n",
        "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Conecta vía RPC y espera sincronización. Lanza excepción si no se logra.\n",
        "    Devuelve (account, rpc_conn).\n",
        "    \"\"\"\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # refrescamos para leer estado/connectionStatus\n",
        "    try:\n",
        "        await account.reload()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Conexión RPC + sincronización del terminal\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "    await rpc_conn.wait_synchronized()  # espera a que el terminal esté listo\n",
        "\n",
        "    # Sonda rápida para confirmar conectividad real con el terminal\n",
        "    try:\n",
        "        _ = await rpc_conn.get_account_information()\n",
        "    except Exception:\n",
        "        # si falla la sonda, igual devolvemos la conexión (ya sincronizada)\n",
        "        pass\n",
        "\n",
        "    return account, rpc_conn\n",
        "\n",
        "def _run(coro):\n",
        "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "def check_connection_once(token: str, account_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida la conexión y sincronización SOLO la primera vez que se llama.\n",
        "    En llamadas posteriores no vuelve a conectar.\n",
        "    \"\"\"\n",
        "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
        "    if __CONNECTION_CHECKED:\n",
        "        print(\"ℹ️ Conexión ya validada en esta sesión; no se repite.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
        "        __ACCOUNT_CONN = (account, rpc_conn)\n",
        "        __CONNECTION_CHECKED = True\n",
        "        print(f\"✅ Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
        "        return True\n",
        "    except TimeoutException as e:\n",
        "        print(f\"❌ Timeout esperando sincronización. ¿La cuenta está CONNECTED al broker? Detalle: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ No fue posible validar la conexión. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def _safe_json_dump(value):\n",
        "    try:\n",
        "        return json.dumps(value, indent=2, default=str)\n",
        "    except Exception:\n",
        "        return str(value)\n",
        "\n",
        "def print_order_error_details(ctx: dict, err: Exception):\n",
        "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
        "    print(\"\\n\" + \"✘\" * 70)\n",
        "    print(\"❌ Order failed\")\n",
        "    print(\"• Exception type:\", type(err).__name__)\n",
        "    print(\"• Message       :\", str(err))\n",
        "\n",
        "    # Known useful attributes often present on MetaApi exceptions\n",
        "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
        "        if hasattr(err, attr):\n",
        "            val = getattr(err, attr)\n",
        "            if val:\n",
        "                print(f\"• {attr:12}: {_safe_json_dump(val)}\")\n",
        "\n",
        "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
        "    msg = str(err)\n",
        "    m = re.search(r\"\\{.*\\}\", msg)\n",
        "    if m:\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "            print(\"• parsed_json  :\", _safe_json_dump(payload))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Stack (useful while debugging)\n",
        "    print(\"• traceback    :\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Context of the attempt\n",
        "    print(\"• context      :\", _safe_json_dump(ctx))\n",
        "    print(\"✘\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RjShygBIwWRp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjShygBIwWRp",
        "outputId": "dd723fab-d67a-44bb-91bf-40e33d5cdd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-05T13:55:26.596905] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-a.agiliumtrade.ai shared server.\n",
            "[2025-09-05T13:55:26.600965] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.new-york-b.agiliumtrade.ai shared server.\n",
            "[2025-09-05T13:55:26.811447] new-york:0: MetaApi websocket client connected to the MetaApi server\n",
            "[2025-09-05T13:55:26.839575] new-york:1: MetaApi websocket client connected to the MetaApi server\n",
            "✅ Conectado y sincronizado con MetaApi. account_id=163d9a57-1f07-4e78-a6af-036efe867c1b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfxZvvtJeFh1",
      "metadata": {
        "id": "HfxZvvtJeFh1"
      },
      "source": [
        "# Real_Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Sx6TbzBOBzZl",
      "metadata": {
        "id": "Sx6TbzBOBzZl"
      },
      "outputs": [],
      "source": [
        "SYMBOL = \"BTCUSD\"\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "FETCH_INTERVAL = 60\n",
        "\n",
        "FETCH_INTERVAL   = 300        # opcional (si lo usas en otra parte)\n",
        "time_frame_data  = \"5m\"       # <<--- cambia de \"1m\" a \"5m\"\n",
        "CANDEL_NUMBER    = 900        # sin cambio; solo afecta cuántas velas bajas inicialmente\n",
        "\n",
        "\n",
        "LOT     = 1.0\n",
        "COMMENT = \"Insta\"\n",
        "\n",
        "length_1 = 300\n",
        "length_2 = 410\n",
        "length_3 = 710\n",
        "length_4 = 870\n",
        "\n",
        "smooth_1 = 3\n",
        "smooth_2 = 3\n",
        "smooth_3 = 3\n",
        "smooth_4 = 5\n",
        "\n",
        "INITIAL_SL         = -2\n",
        "FIRST_STEP_ATR     = 0.5\n",
        "GAP_FIRST_STEP_ATR = 2\n",
        "\n",
        "REGION = \"new-york\"\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTUyNzAwODEsImV4cCI6MTc2MzA0NjA4MX0.KTSrBii1PVzfKdQTBv3vSWTXMkGvTGp1kPQZSZcJJxp6yXZax6A9TW_JaQc0mGVMxCgPjYO8P6WBjBYWEcVqNsCz-xlLnDVAio2FJuiI-sQfcB7C2kXBAm8Kh6C0QkU8E1bzE92qSGehfkmp5a29kCb8l5hEiyKuotN2UoDpbSIX5Te2xIIJRhHyryiJAbA4a1lkDG-kp5pTZwI5CsJI0T6zHPs87UsFLiHCW29YJU-BrztS84DEI8zEXj9FWXCxvsR4K88korkJ-fJnBliqB3OWu1usCefJhKb7z2A-G1gUQqa_X0uLr8VFMc4u7hUsY-83_7iatkOEfiJt2ioxQeNhPG1FEb6g0SGu6xBcmV9yMk2cQwY4php_TPORlIz-DqmqjNSSZACU2owVuxKFE-jrdl5C94qDCqQwgR7BzSbuL2G4DgUyWLZDE3zl4mfyLmvL1ilY1EpJwIEX9_6UFI_-igAqzQEl4WAAee1FohL6DgyS9kZ2XecgXV2i_M4QD04V0m2Y1HN0bORszejvNHoQbqM-7zHb7ZzD5qMzTCKiC6tGeQJqdmWDcNNYJcTecXiXSEvkFoqgk2Q1Ajr8e4Cd2phCMFIgvtLReNgUzFrf_71UuA76AqWONul6YSZj_VV7WYZlWftbgopiHPH0D_gcXva-zxfmQhxnBoY5KNI'\n",
        "ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sKKJrsFBfOoT",
      "metadata": {
        "id": "sKKJrsFBfOoT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PARÁMETROS / DEFAULTS SEGUROS (no rompen si faltan globales)\n",
        "# ============================================================================\n",
        "try:\n",
        "    FILE_PATH  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FILE_PATH = \"xauusd_data.csv\"\n",
        "\n",
        "try:\n",
        "    SYMBOL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    SYMBOL = \"BTCUSD\"\n",
        "\n",
        "try:\n",
        "    time_frame_data  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    time_frame_data = \"5m\"\n",
        "\n",
        "try:\n",
        "    CANDEL_NUMBER  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    CANDEL_NUMBER = 100\n",
        "\n",
        "try:\n",
        "    INITIAL_SL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    INITIAL_SL = -1.0  # múltiplos ATR (negativo para BUY)\n",
        "\n",
        "try:\n",
        "    FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FIRST_STEP_ATR = 0.5\n",
        "\n",
        "try:\n",
        "    GAP_FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    GAP_FIRST_STEP_ATR = 2.0\n",
        "\n",
        "try:\n",
        "    META_API_TOKEN  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    META_API_TOKEN = \"\"  # pon tu token real\n",
        "\n",
        "try:\n",
        "    ACCOUNT_ID  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    ACCOUNT_ID = \"\"  # pon tu account id\n",
        "\n",
        "try:\n",
        "    REGION  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    REGION = \"new-york\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOGGING\n",
        "# ============================================================================\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\", \"websockets\"):\n",
        "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
        "if \"MetaApi\" in globals() and MetaApi:\n",
        "    try:\n",
        "        MetaApi.enable_logging()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FILTRO KALMAN + INSTA\n",
        "# ============================================================================\n",
        "def kalman_line(source: pd.Series | Sequence[float], kalman_length: int, smooth: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Filtro Kalman (versión simple) + suavizado EWM.\n",
        "    Retorna una Serie alineada al índice de 'source' si es Series.\n",
        "    \"\"\"\n",
        "    src = pd.Series(source)\n",
        "    n = len(src)\n",
        "    if n == 0:\n",
        "        return src\n",
        "\n",
        "    kf_c = np.empty(n, dtype=float)\n",
        "    velo_c = np.zeros(n, dtype=float)\n",
        "\n",
        "    sqrt_term = np.sqrt(max(kalman_length, 1) / 10000.0 * 2.0)\n",
        "    length_term = max(kalman_length, 1) / 10000.0\n",
        "\n",
        "    kf_c[0] = float(src.iloc[0])\n",
        "    velo_c[0] = 0.0\n",
        "\n",
        "    for i in range(1, n):\n",
        "        prev_kf = kf_c[i - 1] if np.isfinite(kf_c[i - 1]) else float(src.iloc[i])\n",
        "        dk = float(src.iloc[i]) - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term\n",
        "        velo_c[i] = velo_c[i - 1] + length_term * dk\n",
        "        kf_c[i] = smooth_c + velo_c[i]\n",
        "\n",
        "    kf_c_series = pd.Series(kf_c, index=src.index)\n",
        "    kal = kf_c_series.ewm(span=max(int(smooth), 1), adjust=False).mean()\n",
        "    return kal\n",
        "\n",
        "\n",
        "def insta(src: Sequence[float], a: float) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implementación de filtro 'insta' tipo Pine (lag e instantáneo).\n",
        "    Devuelve (lag, it).\n",
        "    \"\"\"\n",
        "    src = np.asarray(src, dtype=float)\n",
        "    n = src.size\n",
        "    it = np.full(n, np.nan)\n",
        "    lag = np.full(n, np.nan)\n",
        "\n",
        "    def get(arr, i):\n",
        "        return arr[i] if 0 <= i < n else np.nan\n",
        "\n",
        "    def nz(x, y):\n",
        "        return x if np.isfinite(x) else y\n",
        "\n",
        "    for i in range(n):\n",
        "        s0, s1, s2 = get(src, i), get(src, i - 1), get(src, i - 2)\n",
        "        fallback = (s0 + 2 * s1 + s2) / 4 if all(map(np.isfinite, [s0, s1, s2])) else np.nan\n",
        "\n",
        "        it_prev1, it_prev2 = get(it, i - 1), get(it, i - 2)\n",
        "\n",
        "        term1 = (a - (a * a) / 4.0) * (s0 if np.isfinite(s0) else 0.0)\n",
        "        term2 = 0.5 * a * a * (s1 if np.isfinite(s1) else 0.0)\n",
        "        term3 = -(a - 0.75 * a * a) * (s2 if np.isfinite(s2) else 0.0)\n",
        "        term4 = 2 * (1 - a) * nz(it_prev1, fallback)\n",
        "        term5 = (1 - a) * (1 - a) * nz(it_prev2, fallback)\n",
        "\n",
        "        it[i] = term1 + term2 + term3 + term4 - term5\n",
        "\n",
        "        it_prev2_for_lag = get(it, i - 2)\n",
        "        lag[i] = 2 * it[i] - it_prev2_for_lag if np.isfinite(it_prev2_for_lag) else np.nan\n",
        "\n",
        "    return lag, it\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UTILIDADES DE COLUMNAS / CSV\n",
        "# ============================================================================\n",
        "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Crea/normaliza columnas clave con dtypes consistentes y\n",
        "    elimina columnas heredadas ('id', 'actionType') si existieran.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # elimina columnas heredadas\n",
        "    for col in (\"id\", \"actionType\"):\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    col_types = {\n",
        "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
        "        \"orderId\":       \"string\",\n",
        "        \"magic\":         \"Int64\",\n",
        "        \"symbol\":        \"string\",\n",
        "        \"openPrice\":     \"float64\",\n",
        "        \"comment\":       \"string\",\n",
        "        \"Type\":          \"string\",\n",
        "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
        "        \"Stop_Loss_atr\": \"float64\",\n",
        "        \"Stop_Loss_$\":   \"float64\",\n",
        "        \"Real_SL\":       \"float64\",\n",
        "        \"ATR\":           \"float64\",\n",
        "        \"atr_mult_high\": \"float64\",\n",
        "        \"atr_mult_low\":  \"float64\",\n",
        "        \"trade_size\":    \"float64\",\n",
        "        \"profits\":       \"float64\",\n",
        "        \"base_px\":       \"float64\",\n",
        "        \"atr_base\":      \"float64\",\n",
        "        \"source\":        \"Int64\",\n",
        "        \"time\":          \"datetime64[ns, UTC]\",\n",
        "        \"open\":          \"float64\",\n",
        "        \"high\":          \"float64\",\n",
        "        \"low\":           \"float64\",\n",
        "        \"close\":         \"float64\",\n",
        "        \"volume\":        \"float64\",\n",
        "        \"tickVolume\":    \"float64\",\n",
        "        \"spread\":        \"float64\",\n",
        "    }\n",
        "\n",
        "    for c, dtp in col_types.items():\n",
        "        if c not in df.columns:\n",
        "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.NaT\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
        "            elif dtp == \"string\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
        "            else:\n",
        "                df[c] = np.nan\n",
        "\n",
        "    # normaliza sin romper datos existentes\n",
        "    for c, dtp in col_types.items():\n",
        "        try:\n",
        "            if dtp == \"string\":\n",
        "                df[c] = df[c].astype(\"string\")\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = df[c].astype(\"Int64\")\n",
        "            elif isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "            # floats/num los dejamos tal cual para no forzar conversiones peligrosas\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la última fila\n",
        "    mode=\"missing\": rellena donde esté NaT/NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "\n",
        "def _fmt_dt_cols(df: pd.DataFrame, cols=(\"System_time\", \"time\", \"Entry_Date\")) -> None:\n",
        "    \"\"\"Formatea columnas datetime a string 'YYYY-mm-dd HH:MM:SS' sin tz.\"\"\"\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Reordena columnas y guarda CSV con tiempos formateados.\n",
        "    Orden deseado:\n",
        "      • System_time antes de 'time'\n",
        "      • 'source' a la derecha de 'time' y antes de 'open'\n",
        "      • Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
        "      • Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    df_out = df.copy()\n",
        "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "    _fmt_dt_cols(df_out)\n",
        "\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols:\n",
        "            return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns:\n",
        "                    cols.insert(i, c)\n",
        "                    i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns and c not in cols:\n",
        "                    cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols:\n",
        "            cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols:\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else:\n",
        "                cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    cols = [c for c in df_out.columns if c not in (\"id\", \"brokerTime\", \"actionType\")]\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "\n",
        "    # Filtra por columnas existentes para evitar ValueError en to_csv\n",
        "    ordered = [c for c in ordered if c in df_out.columns]\n",
        "    df_out.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "\n",
        "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Migra CSV existente:\n",
        "      • Elimina columnas heredadas.\n",
        "      • Asegura columnas nuevas: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
        "      • Formatea tiempos.\n",
        "      • Reordena columnas al formato actual.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    ensure_cols = {\n",
        "        \"System_time\": pd.NaT,\n",
        "        \"profits\":     np.nan,\n",
        "        \"trade_size\":  np.nan,\n",
        "        \"base_px\":     np.nan,\n",
        "        \"atr_base\":    np.nan,\n",
        "        \"source\":      pd.NA,\n",
        "        \"Real_SL\":     np.nan,\n",
        "    }\n",
        "    for c, default in ensure_cols.items():\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    _fmt_dt_cols(df)\n",
        "\n",
        "    # Reusar lógica de save_csv para reordenar\n",
        "    save_csv(df, path=path)\n",
        "\n",
        "\n",
        "def _load_csv(path: str = FILE_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware y elimina columnas heredadas.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        dtype={\n",
        "            \"orderId\": \"string\",\n",
        "            \"symbol\":  \"string\",\n",
        "            \"comment\": \"string\",\n",
        "            \"Type\":    \"string\",\n",
        "        }\n",
        "    )\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# METAAPI: CONEXIÓN Y DATOS\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio, datetime as dt\n",
        "\n",
        "async def connect_metaapi(token: str, account_id: str, *, rpc_timeout=60, retries=3):\n",
        "    \"\"\"\n",
        "    Devuelve (account) con RPC intentado. Si la sincronización falla (DNS / timeout),\n",
        "    seguimos en modo REST con el mismo 'account' (las llamadas RPC tendrán fallback).\n",
        "    \"\"\"\n",
        "    if \"MetaApi\" not in globals() or MetaApi is None:\n",
        "        raise RuntimeError(\"metaapi_cloud_sdk no está disponible en el entorno.\")\n",
        "\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # Asegurar despliegue/conexión de la cuenta (no falla si el SDK no expone algo)\n",
        "    try:\n",
        "        await account.reload()\n",
        "        if getattr(account, \"state\", \"\").upper() != \"DEPLOYED\":\n",
        "            await account.deploy()\n",
        "            if hasattr(account, \"wait_deployed\"):\n",
        "                await account.wait_deployed()\n",
        "        if hasattr(account, \"wait_connected\"):\n",
        "            await account.wait_connected()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    conn = account.get_rpc_connection()\n",
        "\n",
        "    # Intentos de conectar/sincronizar RPC con backoff\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            await conn.connect()\n",
        "            try:\n",
        "                # según versión del SDK, wait_synchronized puede o no aceptar timeout\n",
        "                await asyncio.wait_for(conn.wait_synchronized(), timeout=rpc_timeout)\n",
        "            except TypeError:\n",
        "                await conn.wait_synchronized()\n",
        "            except asyncio.TimeoutError:\n",
        "                raise TimeoutError(\"RPC wait_synchronized timeout\")\n",
        "            return account  # RPC OK\n",
        "        except Exception as e:\n",
        "            if attempt == retries:\n",
        "                logging.warning(\"RPC no sincronizó (%s). Continuando con REST-only; habrá fallbacks.\", e)\n",
        "                return account\n",
        "            await asyncio.sleep(min(5 * attempt, 15))\n",
        "\n",
        "    return account\n",
        "\n",
        "\n",
        "\n",
        "async def get_current_candle_snapshot(account,\n",
        "                                      rpc_conn,\n",
        "                                      symbol: str = SYMBOL,\n",
        "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve 1 fila con la vela más reciente (puede ser la vela en curso).\n",
        "    \"\"\"\n",
        "    def _to_row(c: dict) -> dict:\n",
        "        return {\n",
        "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
        "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
        "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
        "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
        "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
        "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
        "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
        "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
        "        }\n",
        "\n",
        "    # 1) RPC\n",
        "    try:\n",
        "        try:\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe, limit=1)\n",
        "        except TypeError:\n",
        "            to_ts = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe,\n",
        "                                                 start_time=from_ts, end_time=to_ts)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Histórico como respaldo\n",
        "    try:\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe=timeframe, start_time=None, limit=1)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
        "\n",
        "\n",
        "async def get_candles_5m(account, start: dt.datetime | None, limit: int = CANDEL_NUMBER):\n",
        "    \"\"\"Descarga velas históricas (usa time_frame_data configurado).\"\"\"\n",
        "    candles = await account.get_historical_candles(symbol=SYMBOL, timeframe=time_frame_data, start_time=start, limit=limit)\n",
        "    return pd.DataFrame([{\n",
        "        \"time\": pd.to_datetime(c[\"time\"], utc=True),\n",
        "        \"open\": c[\"open\"], \"high\": c[\"high\"], \"low\": c[\"low\"], \"close\": c[\"close\"],\n",
        "        \"volume\": c[\"volume\"], \"tickVolume\": c[\"tickVolume\"], \"spread\": c[\"spread\"]\n",
        "    } for c in candles])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TIMEFRAME SYNC\n",
        "# ============================================================================\n",
        "def seconds_until_next_tf(tf: str = \"5m\", *, offset_sec: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Segundos hasta el próximo cierre de vela del timeframe tf (+offset).\n",
        "    Soporta sufijos 'm','h','d'. Ej: '1m','5m','15m','1h','4h','1d'.\n",
        "    \"\"\"\n",
        "    tf = (tf or \"5m\").strip().lower()\n",
        "    if tf.endswith(\"mn\"):\n",
        "        tf = tf[:-2] + \"m\"\n",
        "    try:\n",
        "        if tf.endswith(\"m\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(minutes=max(step, 1))\n",
        "        elif tf.endswith(\"h\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(hours=max(step, 1))\n",
        "        elif tf.endswith(\"d\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(days=max(step, 1))\n",
        "        else:\n",
        "            delta = dt.timedelta(minutes=1)\n",
        "    except Exception:\n",
        "        delta = dt.timedelta(minutes=1)\n",
        "\n",
        "    now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "    epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "    secs = int((now - epoch).total_seconds())\n",
        "    stepS = int(delta.total_seconds())\n",
        "\n",
        "    next_boundary = ((secs // stepS) + 1) * stepS\n",
        "    target = epoch + dt.timedelta(seconds=next_boundary + max(offset_sec, 0))\n",
        "    wait_s = (target - now).total_seconds()\n",
        "    return max(wait_s, 0.5)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SEÑALES / KALMAN\n",
        "# ============================================================================\n",
        "def generate_trade_signals(\n",
        "    df: pd.DataFrame,\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula 4 líneas de Kalman sobre 'close' y crea:\n",
        "      • kal_1, kal_2, kal_3, kal_4\n",
        "      • Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
        "      • Close_Trade: -1 si kal_4 < kal_4.shift()  → cierra BUY\n",
        "                     +1 si kal_4 > kal_4.shift()  → cierra SELL\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if \"close\" not in df.columns:\n",
        "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
        "\n",
        "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").ffill()\n",
        "\n",
        "    def _clamp_int(x, mn=1):\n",
        "        try:\n",
        "            x = int(x)\n",
        "        except Exception:\n",
        "            x = mn\n",
        "        return max(x, mn)\n",
        "\n",
        "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
        "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
        "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
        "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
        "\n",
        "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
        "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
        "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
        "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
        "\n",
        "    k1_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1)\n",
        "    k2_up = df[\"kal_2\"] > df[\"kal_2\"].shift(1)\n",
        "    k3_up = df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
        "    k1_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1)\n",
        "    k2_dn = df[\"kal_2\"] < df[\"kal_2\"].shift(1)\n",
        "    k3_dn = df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
        "\n",
        "    bull = k1_up & k2_up & k3_up\n",
        "    bear = k1_dn & k2_dn & k3_dn\n",
        "    aux = np.where(bull, 1, np.where(bear, -1, np.nan))\n",
        "    df[\"Open_Trade\"] = np.where(pd.Series(aux).shift(1) != aux, aux, np.nan)\n",
        "\n",
        "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)\n",
        "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)\n",
        "    close_raw = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
        "    close_sr = pd.Series(close_raw)\n",
        "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REST HELPERS (MetaApi REST)\n",
        "# ============================================================================\n",
        "def _rest_place_order(auth_token: str,\n",
        "                      account_id: str,\n",
        "                      region: str,\n",
        "                      symbol: str,\n",
        "                      side: str,\n",
        "                      volume: float,\n",
        "                      comment: str = \"Kal\",\n",
        "                      magic: int | None = None,\n",
        "                      stop_loss: float | None = None,\n",
        "                      timeout: int = 20):\n",
        "    side = side.upper().strip()\n",
        "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
        "    if side not in action_map:\n",
        "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
        "\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\n",
        "        \"symbol\": symbol,\n",
        "        \"actionType\": action_map[side],\n",
        "        \"volume\": float(volume),\n",
        "        \"comment\": str(comment)\n",
        "    }\n",
        "    if magic is not None:\n",
        "        try:\n",
        "            payload[\"magic\"] = int(magic)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if stop_loss is not None:\n",
        "        try:\n",
        "            payload[\"stopLoss\"] = float(stop_loss)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "def _rest_get_positions(auth_token: str,\n",
        "                        account_id: str,\n",
        "                        region: str,\n",
        "                        symbol: str | None = None,\n",
        "                        timeout: int = 15):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
        "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    params = {}\n",
        "    if symbol:\n",
        "        params[\"symbol\"] = str(symbol)\n",
        "    try:\n",
        "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        class _Dummy:\n",
        "            status_code = 0\n",
        "            def json(self): return {\"error\": str(e)}\n",
        "            text = str(e)\n",
        "        return _Dummy()\n",
        "\n",
        "\n",
        "def _rest_modify_position(auth_token: str,\n",
        "                          account_id: str,\n",
        "                          region: str,\n",
        "                          position_id: str,\n",
        "                          stop_loss: float | None = None,\n",
        "                          take_profit: float | None = None,\n",
        "                          timeout: int = 20):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
        "    if stop_loss is not None:\n",
        "        payload[\"stopLoss\"] = float(stop_loss)\n",
        "    if take_profit is not None:\n",
        "        payload[\"takeProfit\"] = float(take_profit)\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "# --- REST fallback to close a position --------------------------------------\n",
        "def _rest_close_position(auth_token: str,\n",
        "                         account_id: str,\n",
        "                         region: str,\n",
        "                         position_id: str,\n",
        "                         timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Cierra una posición por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    # RPC con timeout corto para evitar cuelgues cuando el subscribe falla\n",
        "    async def _rpc_try(fn, *args, **kwargs):\n",
        "        try:\n",
        "            return await asyncio.wait_for(fn(*args, **kwargs), timeout=4)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    if rpc_conn:\n",
        "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
        "        if not positions:\n",
        "            positions = await _rpc_try(rpc_conn.get_positions)\n",
        "\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: CLOSE / ORDER\n",
        "# ============================================================================\n",
        "\n",
        "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"✘ No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt:  return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close_rpc(pid: str) -> bool:\n",
        "        try:\n",
        "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
        "                return True\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid  = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "\n",
        "        ok = await _try_close_rpc(pid)\n",
        "        if not ok:\n",
        "            # REST fallback\n",
        "            r = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
        "            ok = getattr(r, \"status_code\", 0) == 200\n",
        "\n",
        "        if ok:\n",
        "            print(f\"✅ Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "        else:\n",
        "            print(f\"✘ No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: ABRIR / CERRAR / SINCRONIZAR / SL DINÁMICO\n",
        "# ============================================================================\n",
        "async def open_trade(df: pd.DataFrame,\n",
        "                     rpc_conn,\n",
        "                     symbol: str = SYMBOL,\n",
        "                     lot: float = 1.0,\n",
        "                     comment: str = \"Kal\",\n",
        "                     magic: int = 900001):\n",
        "    \"\"\"\n",
        "    Abre mercado SOLO si NO hay posición con ese magic.\n",
        "    Al abrir, calcula y envía el SL inicial:\n",
        "       SL = close ± ATR * INITIAL_SL  (según BUY/SELL; INITIAL_SL suele ser negativo)\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                return int(pm) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt: return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    open_with_magic = [p for p in positions if _has_magic(p)]\n",
        "\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    row = df.index[-1]\n",
        "\n",
        "    # Ya hay posición con este magic → sincroniza y sale\n",
        "    if open_with_magic:\n",
        "        p = open_with_magic[0]\n",
        "        df.at[row, \"System_time\"] = now_utc\n",
        "        pm = p.get(\"magic\")\n",
        "        pc = str(p.get(\"comment\") or \"\")\n",
        "        clean_cmt, mag_from_cmt = _split_comment_magic(pc)\n",
        "\n",
        "        if pm is not None and str(pm).strip() != \"\":\n",
        "            try: df.at[row, \"magic\"] = int(pm)\n",
        "            except Exception: df.at[row, \"magic\"] = int(magic)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.at[row, \"magic\"] = int(magic)\n",
        "\n",
        "        df.at[row, \"symbol\"]    = str(p.get(\"symbol\") or symbol)\n",
        "        df.at[row, \"openPrice\"] = float(p.get(\"openPrice\") or p.get(\"price\") or np.nan)\n",
        "        df.at[row, \"comment\"]   = clean_cmt or str(comment)\n",
        "\n",
        "        vol = p.get(\"volume\") or p.get(\"lots\") or None\n",
        "        if vol is not None:\n",
        "            try: df.at[row, \"trade_size\"] = float(vol)\n",
        "            except Exception: pass\n",
        "\n",
        "        side = _side_of(p)\n",
        "        if side: df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
        "        if pd.isna(df.at[row, \"Entry_Date\"]): df.at[row, \"Entry_Date\"] = now_utc\n",
        "\n",
        "        save_csv(df)\n",
        "        print(\"ℹ Position already open; synced last row and skipped new order.\")\n",
        "        return\n",
        "\n",
        "    # No hay posición → decidir lado por Open_Trade\n",
        "    sig = df[\"Open_Trade\"].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
        "    if side_req is None:\n",
        "        return\n",
        "\n",
        "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
        "    atr_val = float(df[\"ATR\"].iloc[-1]) if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
        "    sl_mult = float(INITIAL_SL) if INITIAL_SL is not None else np.nan\n",
        "\n",
        "    sl_to_send = None\n",
        "    if np.isfinite(prev_close) and np.isfinite(atr_val) and np.isfinite(sl_mult):\n",
        "        if side_req == \"BUY\":\n",
        "            sl_to_send = prev_close + atr_val * sl_mult\n",
        "        else:\n",
        "            sl_to_send = prev_close - atr_val * sl_mult\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_place_order(\n",
        "            auth_token=META_API_TOKEN,\n",
        "            account_id=ACCOUNT_ID,\n",
        "            region=REGION,\n",
        "            symbol=symbol,\n",
        "            side=side_req,\n",
        "            volume=float(lot),\n",
        "            comment=str(comment),\n",
        "            magic=int(magic),\n",
        "            stop_loss=sl_to_send,\n",
        "            timeout=20\n",
        "        )\n",
        "    )\n",
        "    if getattr(resp, \"status_code\", 0) != 200:\n",
        "        try: err = resp.json()\n",
        "        except Exception: err = {\"raw\": getattr(resp, \"text\", \"\")[:500]}\n",
        "        print(\"✘ Order failed\", getattr(resp, \"status_code\", None), json.dumps(err, indent=2, ensure_ascii=False))\n",
        "        return\n",
        "\n",
        "    data = resp.json()\n",
        "    order_id = str(data.get(\"orderId\") or \"\")\n",
        "    position_id = str(data.get(\"positionId\") or \"\")\n",
        "\n",
        "    df.at[row, \"System_time\"] = now_utc\n",
        "    df.at[row, \"orderId\"]     = order_id\n",
        "    df.at[row, \"magic\"]       = int(magic)\n",
        "    df.at[row, \"symbol\"]      = symbol\n",
        "    df.at[row, \"comment\"]     = str(comment)\n",
        "    df.at[row, \"Entry_Date\"]  = now_utc\n",
        "    df.at[row, \"trade_size\"]  = float(lot)\n",
        "\n",
        "    if sl_to_send is not None and np.isfinite(sl_to_send):\n",
        "        df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
        "        df.at[row, \"Stop_Loss_atr\"] = float(sl_mult)\n",
        "\n",
        "    # Recuperar openPrice y Type desde API\n",
        "    open_price, fetched_typ = np.nan, None\n",
        "    if rpc_conn:\n",
        "        try:\n",
        "            for _ in range(60):\n",
        "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "                match = None\n",
        "                for p in (pos_list or []):\n",
        "                    pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                    if pid == position_id or pid == order_id or _has_magic(p):\n",
        "                        match = p; break\n",
        "                if match:\n",
        "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
        "                    if val is not None: open_price = float(val)\n",
        "                    t = match.get(\"type\")\n",
        "                    if isinstance(t, str):\n",
        "                        tu = t.upper(); fetched_typ = \"Long\" if \"BUY\" in tu else (\"Short\" if \"SELL\" in tu else None)\n",
        "                    elif t == 0:\n",
        "                        fetched_typ = \"Long\"\n",
        "                    elif t == 1:\n",
        "                        fetched_typ = \"Short\"\n",
        "                    break\n",
        "                await asyncio.sleep(0.5)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not np.isfinite(open_price):\n",
        "        try: open_price = float(df.at[row, \"close\"])\n",
        "        except Exception: open_price = np.nan\n",
        "\n",
        "    df.at[row, \"openPrice\"] = open_price\n",
        "    df.at[row, \"Type\"] = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
        "\n",
        "    save_csv(df)\n",
        "    print(f\"✅ {side_req} placed | orderId={order_id} positionId={position_id} \"\n",
        "          f\"openPrice={open_price if np.isfinite(open_price) else 'NaN'} \"\n",
        "          f\"| SL sent={sl_to_send if sl_to_send is not None else 'None'} \"\n",
        "          f\"| Type={df.at[row,'Type']}\")\n",
        "\n",
        "\n",
        "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Por bloque de trade:\n",
        "      • FFill de metadatos\n",
        "      • base_px / atr_base desde la apertura del bloque\n",
        "      • atr_mult_high/low\n",
        "      • 'profits' solo si está NaN\n",
        "      • Propaga 'Real_SL' dentro del bloque con el último valor no nulo\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    for c in ('time', 'Entry_Date'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
        "\n",
        "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "        else:\n",
        "            df[c].values[:] = np.nan\n",
        "\n",
        "    if 'profits' not in df.columns:\n",
        "        df['profits'] = np.nan\n",
        "    if 'base_px' not in df.columns:\n",
        "        df['base_px'] = np.nan\n",
        "    if 'atr_base' not in df.columns:\n",
        "        df['atr_base'] = np.nan\n",
        "    if 'Real_SL' not in df.columns:\n",
        "        df['Real_SL'] = np.nan\n",
        "\n",
        "    starts_mask = pd.Series(False, index=df.index)\n",
        "    if 'orderId' in df.columns:\n",
        "        oid = df['orderId']\n",
        "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
        "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
        "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
        "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
        "    if not starts_mask.any():\n",
        "        return df\n",
        "\n",
        "    groups = starts_mask.cumsum()\n",
        "    trade_ids = groups[starts_mask].unique()\n",
        "\n",
        "    meta_cols = [\"orderId\", \"magic\", \"symbol\", \"openPrice\", \"comment\", \"Type\", \"Entry_Date\", \"trade_size\"]\n",
        "\n",
        "    for gid in trade_ids:\n",
        "        mask = (groups == gid)\n",
        "        start_idx = df.index[mask][0]\n",
        "\n",
        "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
        "        try:\n",
        "            base_px = float(base_px)\n",
        "        except Exception:\n",
        "            base_px = np.nan\n",
        "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
        "            try:\n",
        "                base_px = float(df.at[start_idx, 'close'])\n",
        "            except Exception:\n",
        "                base_px = np.nan\n",
        "\n",
        "        atr_base = np.nan\n",
        "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'atr_base'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'ATR'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "\n",
        "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
        "\n",
        "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
        "\n",
        "        if np.isfinite(base_px):\n",
        "            df.loc[mask, 'base_px'] = base_px\n",
        "        if np.isfinite(atr_base):\n",
        "            df.loc[mask, 'atr_base'] = atr_base\n",
        "\n",
        "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long', 'Short'):\n",
        "            if typ == 'Long':\n",
        "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((df.loc[mask, 'low'] - base_px) / atr_base).round(2)\n",
        "            else:\n",
        "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((base_px - df.loc[mask, 'low']) / atr_base).round(2)\n",
        "\n",
        "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
        "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long', 'Short'):\n",
        "            m_nan = mask & df['profits'].isna()\n",
        "            if m_nan.any():\n",
        "                if typ == 'Long':\n",
        "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
        "                else:\n",
        "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
        "\n",
        "    # Propaga Real_SL por bloque\n",
        "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
        "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
        "    else:\n",
        "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
        "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
        "\n",
        "    if starts.any():\n",
        "        grp = starts.cumsum()\n",
        "        for gid in grp[starts].unique():\n",
        "            m = (grp == gid)\n",
        "            ser = df.loc[m, 'Real_SL']\n",
        "            if ser.notna().any():\n",
        "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
        "                df.loc[m, 'Real_SL'] = val\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def tick_dyn_atr(df: pd.DataFrame,\n",
        "                 initial_atr: float = INITIAL_SL,\n",
        "                 first_step_atr: float = FIRST_STEP_ATR,\n",
        "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Dinámica de stop en múltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
        "      • Escribe 'tick_dyn_atr' (múltiplos)\n",
        "      • Calcula y actualiza 'Stop_Loss_$' y 'Stop_Loss_atr'\n",
        "    \"\"\"\n",
        "    col_name = 'tick_dyn_atr'\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = np.nan\n",
        "    if 'Stop_Loss_$' not in df.columns:\n",
        "        df['Stop_Loss_$'] = np.nan\n",
        "    if 'Stop_Loss_atr' not in df.columns:\n",
        "        df['Stop_Loss_atr'] = np.nan\n",
        "\n",
        "    in_trade = False\n",
        "    trade_active = False\n",
        "    broken = False\n",
        "    sl_val = float(initial_atr)\n",
        "    next_threshold = float(first_step_atr)\n",
        "    prev_sl = float(initial_atr)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        new_open = (\n",
        "            (('orderId' in df.columns) and pd.notna(row.get('orderId'))) or\n",
        "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
        "            (pd.notna(row.get('Entry_Date')))\n",
        "        )\n",
        "\n",
        "        if new_open and not in_trade:\n",
        "            in_trade = True\n",
        "            trade_active = True\n",
        "            broken = False\n",
        "            sl_val = float(initial_atr)\n",
        "            next_threshold = float(first_step_atr)\n",
        "            prev_sl = sl_val\n",
        "            entry_dt = row.get('time')\n",
        "            if entry_dt is not None:\n",
        "                df.at[idx, 'Entry_Date'] = entry_dt\n",
        "\n",
        "        if in_trade:\n",
        "            m_high = row.get('atr_mult_high', np.nan)\n",
        "            m_low = row.get('atr_mult_low', np.nan)\n",
        "            best_pnl = np.nanmax([m_high, m_low])\n",
        "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
        "\n",
        "            if trade_active and not broken:\n",
        "                while best_pnl >= next_threshold:\n",
        "                    sl_val += float(gap_first_step_atr)\n",
        "                    next_threshold += float(gap_first_step_atr)\n",
        "\n",
        "                below_prev = (\n",
        "                    (np.isfinite(m_high) and float(m_high) < prev_sl) or\n",
        "                    (np.isfinite(m_low) and float(m_low) < prev_sl)\n",
        "                )\n",
        "                if below_prev:\n",
        "                    broken = True\n",
        "                    trade_active = False\n",
        "                    in_trade = False\n",
        "\n",
        "            df.at[idx, col_name] = np.nan if broken else sl_val\n",
        "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
        "            prev_sl = sl_val\n",
        "\n",
        "    # Convierte múltiplos a precio\n",
        "    try:\n",
        "        sl_mult = df[col_name].astype(float)\n",
        "        base = df['base_px'].astype(float)\n",
        "        atr = df['atr_base'].astype(float)\n",
        "        typ = df['Type'].astype('string')\n",
        "\n",
        "        stop_price = np.where(\n",
        "            (typ == 'Long') & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "            base + atr * sl_mult,\n",
        "            np.where(\n",
        "                (typ == 'Short') & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "                base - atr * sl_mult,\n",
        "                np.nan\n",
        "            )\n",
        "        )\n",
        "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    \"\"\"\n",
        "    Cierra posiciones según la pendiente de kal_4 codificada en Close_Trade:\n",
        "      • Close_Trade == -1  → cerrar BUY\n",
        "      • Close_Trade == +1  → cerrar SELL\n",
        "    \"\"\"\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"✘ No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close(pid: str) -> bool:\n",
        "        try:\n",
        "            await rpc_conn.close_position(pid)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await rpc_conn.close_position({\"positionId\": pid})\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"✘ Falló cierre positionId={pid}: {e}\")\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "        ok = await _try_close(pid)\n",
        "        if ok:\n",
        "            print(f\"✅ Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "\n",
        "\n",
        "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
        "                                 rpc_conn,\n",
        "                                 symbol: str = SYMBOL,\n",
        "                                 magic: int = 900001,\n",
        "                                 tol: float = 0.01) -> None:\n",
        "    \"\"\"\n",
        "    Copia desde la posición viva los campos de mercado al DF.\n",
        "    'profits' solo se escribe en la ÚLTIMA FILA para no sobreescribir histórico.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt:\n",
        "            return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    pos = next((p for p in positions if _has_magic(p) and _sym_ok(p)), None)\n",
        "    if not pos:\n",
        "        pos = next((p for p in positions if _has_magic(p)), None)\n",
        "    if not pos:\n",
        "        return\n",
        "\n",
        "    order_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    magic_val = pos.get(\"magic\")\n",
        "    symbol_val = pos.get(\"symbol\")\n",
        "    open_price = pos.get(\"openPrice\") or pos.get(\"price\")\n",
        "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\") or None\n",
        "    stop_loss = pos.get(\"stopLoss\")\n",
        "    volume_val = pos.get(\"volume\") or pos.get(\"lots\")\n",
        "    profit_val = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
        "                  else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
        "\n",
        "    t = pos.get(\"type\")\n",
        "    typ = None\n",
        "    if isinstance(t, str):\n",
        "        tu = t.upper()\n",
        "        if \"BUY\" in tu:  typ = \"Long\"\n",
        "        if \"SELL\" in tu: typ = \"Short\"\n",
        "    elif t == 0:\n",
        "        typ = \"Long\"\n",
        "    elif t == 1:\n",
        "        typ = \"Short\"\n",
        "\n",
        "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
        "    clean_cmt, mag_from_cmt = _split_comment_magic(str(comment_raw or \"\"))\n",
        "\n",
        "    block_mask = pd.Series(False, index=df.index)\n",
        "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
        "        block_mask = (df[\"orderId\"] == order_id)\n",
        "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
        "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
        "        starts = ed.notna() & (ed != ed.shift(1))\n",
        "        if starts.any():\n",
        "            last_start = df.index[starts].max()\n",
        "            block_mask = (df.index >= last_start)\n",
        "        else:\n",
        "            block_mask = ed.notna() & (ed >= entry_dt)\n",
        "    if not block_mask.any():\n",
        "        block_mask.iloc[-1] = True\n",
        "\n",
        "    try:\n",
        "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
        "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.loc[block_mask, \"magic\"] = int(magic)\n",
        "\n",
        "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
        "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
        "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
        "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
        "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
        "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
        "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    last_idx = df.index[-1]\n",
        "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
        "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
        "    elif mag_from_cmt is not None:\n",
        "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
        "    else:\n",
        "        df.at[last_idx, \"magic\"] = int(magic)\n",
        "\n",
        "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
        "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
        "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
        "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
        "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
        "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
        "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
        "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
        "\n",
        "\n",
        "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
        "    \"\"\"Devuelve (prev, last) con los dos últimos valores no-NaN distintos.\"\"\"\n",
        "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        return (np.nan, np.nan)\n",
        "    last = float(s.iloc[-1])\n",
        "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
        "    return (prev, last)\n",
        "\n",
        "\n",
        "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
        "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\"):\n",
        "            if key in p and p[key] is not None:\n",
        "                try:\n",
        "                    if int(p[key]) == int(magic):\n",
        "                        return True\n",
        "                except Exception:\n",
        "                    if str(p[key]).strip() == str(magic):\n",
        "                        return True\n",
        "        if f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    for p in positions:\n",
        "        if _sym_ok(p) and _has_magic(p):\n",
        "            return p\n",
        "    for p in positions:\n",
        "        if _has_magic(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
        "                                     rpc_conn,\n",
        "                                     *,\n",
        "                                     symbol: str,\n",
        "                                     magic: int,\n",
        "                                     auth_token: str,\n",
        "                                     account_id: str,\n",
        "                                     region: str,\n",
        "                                     tol: float = 0.0) -> dict:\n",
        "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
        "    if not np.isfinite(last_sl):\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vacío\"}\n",
        "\n",
        "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
        "    if not changed:\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
        "\n",
        "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "\n",
        "    if not pos:\n",
        "        last_oid = None\n",
        "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
        "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
        "        if last_oid:\n",
        "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "            for p in positions:\n",
        "                pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                if pid == last_oid:\n",
        "                    pos = p\n",
        "                    break\n",
        "\n",
        "    if not pos:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posición con ese magic\"}\n",
        "\n",
        "    position_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    if not position_id:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vacío\"}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_modify_position(\n",
        "            auth_token=auth_token,\n",
        "            account_id=account_id,\n",
        "            region=region,\n",
        "            position_id=position_id,\n",
        "            stop_loss=float(last_sl),\n",
        "            timeout=15\n",
        "        )\n",
        "    )\n",
        "    ok = getattr(resp, \"status_code\", 0) == 200\n",
        "    err = None\n",
        "    if not ok:\n",
        "        try:\n",
        "            err = json.dumps(resp.json())[:300]\n",
        "        except Exception:\n",
        "            err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
        "\n",
        "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
        "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
        "\n",
        "\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    try:\n",
        "        positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    if not positions:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions() or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cvhz3M-fTIw",
      "metadata": {
        "id": "5cvhz3M-fTIw"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    \"\"\"\n",
        "    Bucle principal:\n",
        "      • Crea/migra el CSV inicial (ATR interno, señales Kalman).\n",
        "      • Sin posición → espera al cierre del TF, procesa última vela cerrada y evalúa entrada.\n",
        "      • Con posición → cada 20s recalcula, cierra si corresponde, actualiza SL dinámico y\n",
        "        ENVÍA SIEMPRE la modificación de SL cuando cambie Stop_Loss_$ (usa los dos últimos\n",
        "        valores distintos). Registra en log una línea adicional confirmando el envío.\n",
        "    \"\"\"\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    # 0) Conexión MetaApi / RPC\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    # Parámetros (con defaults tolerantes a faltantes globales)\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    MAGIC    = 900001\n",
        "    LENGTHS  = (300, 410, 710, 870)\n",
        "    SMOOTHS  = (3, 3, 3, 5)\n",
        "    LOT_     = globals().get(\"LOT\", 1.0)\n",
        "    COMMENT_ = globals().get(\"COMMENT\", \"Kal\")\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    # Helpers locales\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    import datetime as dt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    def _parse_tf_to_delta(tf: str) -> dt.timedelta:\n",
        "        \"\"\"'1m','5m','15m','1h','4h','1d' → timedelta (fallback 1m).\"\"\"\n",
        "        tf = (tf or \"1m\").strip().lower()\n",
        "        if tf.endswith(\"mn\"):\n",
        "            tf = tf[:-2] + \"m\"\n",
        "        try:\n",
        "            if tf.endswith(\"m\"):\n",
        "                return dt.timedelta(minutes=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"h\"):\n",
        "                return dt.timedelta(hours=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"d\"):\n",
        "                return dt.timedelta(days=max(int(tf[:-1]), 1))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return dt.timedelta(minutes=1)\n",
        "\n",
        "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
        "        \"\"\"Floor de ts a múltiplo exacto del timeframe (UTC).\"\"\"\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
        "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        secs  = int((ts - epoch).total_seconds())\n",
        "        step  = int(delta.total_seconds()) or 60\n",
        "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
        "\n",
        "    def _calc_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
        "        \"\"\"\n",
        "        ATR estilo Wilder: TR = max(H-L, |H-C1|, |L-C1|), ATR = RMA(TR, period).\n",
        "        Usa ewm(alpha=1/period) como aproximación de RMA.\n",
        "        \"\"\"\n",
        "        h, l, c = df[\"high\"].astype(float), df[\"low\"].astype(float), df[\"close\"].astype(float)\n",
        "        c1 = c.shift(1)\n",
        "        tr = np.maximum.reduce([\n",
        "            (h - l).to_numpy(),\n",
        "            (h - c1).abs().to_numpy(),\n",
        "            (l - c1).abs().to_numpy()\n",
        "        ])\n",
        "        atr = pd.Series(tr, index=df.index).ewm(alpha=1/period, adjust=False).mean()\n",
        "        return atr.round(4)\n",
        "\n",
        "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
        "        \"\"\"True si existe posición con ese magic (prefiere helper global si existe).\"\"\"\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            return pos is not None\n",
        "        except Exception:\n",
        "            # Fallback mínimo si el helper no está disponible\n",
        "            positions = []\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            if not positions:\n",
        "                r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "                if getattr(r, \"status_code\", 0) == 200:\n",
        "                    try:\n",
        "                        positions = r.json() or []\n",
        "                    except Exception:\n",
        "                        positions = []\n",
        "            if not positions:\n",
        "                return False\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if ok:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "    async def _get_api_type(rpc_conn, symbol: str, magic: int):\n",
        "        \"\"\"'Long' / 'Short' / None usando get_pos_with_magic si existe.\"\"\"\n",
        "        side = None\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            if not pos:\n",
        "                return None\n",
        "            t = pos.get(\"type\")\n",
        "            if isinstance(t, str):\n",
        "                tu = t.upper()\n",
        "                side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "            elif t == 0:\n",
        "                side = \"BUY\"\n",
        "            elif t == 1:\n",
        "                side = \"SELL\"\n",
        "        except Exception:\n",
        "            # Fallback simple\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if not ok:\n",
        "                    continue\n",
        "                t = p.get(\"type\")\n",
        "                if isinstance(t, str):\n",
        "                    tu = t.upper()\n",
        "                    side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "                elif t == 0:\n",
        "                    side = \"BUY\"\n",
        "                elif t == 1:\n",
        "                    side = \"SELL\"\n",
        "                break\n",
        "        if side is None:\n",
        "            return None\n",
        "        return \"Long\" if side == \"BUY\" else \"Short\"\n",
        "\n",
        "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: str | None) -> None:\n",
        "        \"\"\"Escribe 'Type' en el bloque activo o en la última fila si no se detecta bloque.\"\"\"\n",
        "        if not api_type or df_all.empty:\n",
        "            return\n",
        "        last_oid = df_all.get(\"orderId\")\n",
        "        if last_oid is not None and last_oid.notna().any():\n",
        "            last_oid_val = last_oid.dropna().iloc[-1]\n",
        "            mask = (df_all[\"orderId\"] == last_oid_val)\n",
        "        else:\n",
        "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
        "            starts = ed.notna() & (ed != ed.shift(1))\n",
        "            if starts.any():\n",
        "                start_idx = df_all.index[starts].max()\n",
        "                mask = (df_all.index >= start_idx)\n",
        "            else:\n",
        "                mask = pd.Series(False, index=df_all.index)\n",
        "        if mask.any():\n",
        "            df_all.loc[mask, \"Type\"] = api_type\n",
        "        else:\n",
        "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
        "\n",
        "    async def _sleep_to_20s_grid():\n",
        "        \"\"\"Duerme hasta 00/20/40s del minuto actual (alineación estable).\"\"\"\n",
        "        now = dt.datetime.utcnow()\n",
        "        rem = now.second % 20\n",
        "        if rem == 0 and now.microsecond == 0:\n",
        "            return\n",
        "        next_sec = now.second - rem + 20\n",
        "        if next_sec >= 60:\n",
        "            target = (now + dt.timedelta(minutes=1)).replace(second=0, microsecond=0)\n",
        "        else:\n",
        "            target = now.replace(second=next_sec, microsecond=0)\n",
        "        await asyncio.sleep((target - now).total_seconds())\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    # 1) Crear/migrar archivo inicial\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        df = await get_candles_5m(account, start=None, limit=CANDEL_NUMBER)\n",
        "        if len(df) >= 14:\n",
        "            df[\"ATR\"] = _calc_atr(df, 14)\n",
        "        l1, l2, l3, l4 = LENGTHS\n",
        "        s1, s2, s3, s4 = SMOOTHS\n",
        "        generate_trade_signals(df, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "        _ensure_order_cols(df)\n",
        "        stamp_system_time(df, \"last\")\n",
        "        save_csv(df)\n",
        "        print(f\"✔ Archivo inicial creado con {len(df)} velas\")\n",
        "    else:\n",
        "        migrate_csv_if_needed(FILE_PATH)\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    # 2) Loop principal\n",
        "    # ───────────────────────────────────────────────────────────────────\n",
        "    while True:\n",
        "\n",
        "        # ───────────── SIN posición → esperar cierre de TF y evaluar entrada\n",
        "        if not await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "            await asyncio.sleep(seconds_until_next_tf(time_frame_data, offset_sec=3))\n",
        "\n",
        "            now_utc  = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            delta    = _parse_tf_to_delta(time_frame_data)\n",
        "            this_bar = _floor_to_frame(now_utc, delta)\n",
        "            prev_bar = this_bar - delta  # última vela CERRADA\n",
        "\n",
        "            # Tomar solo la última vela <= prev_bar\n",
        "            df_new = await get_candles_5m(account, start=None, limit=50)\n",
        "            df_new = (df_new[df_new[\"time\"] <= prev_bar]\n",
        "                      .drop_duplicates(\"time\")\n",
        "                      .sort_values(\"time\"))\n",
        "            if not df_new.empty:\n",
        "                df_new = df_new.iloc[[-1]]\n",
        "\n",
        "            df_all = _load_csv()\n",
        "            existing_times = set(pd.to_datetime(df_all[\"time\"], utc=True)) if (not df_all.empty and \"time\" in df_all.columns) else set()\n",
        "\n",
        "            if df_all.empty:\n",
        "                df_all = df_new.copy()\n",
        "            else:\n",
        "                df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
        "                          .drop_duplicates(\"time\")\n",
        "                          .sort_values(\"time\")\n",
        "                          .reset_index(drop=True))\n",
        "\n",
        "            if len(df_all) >= 14:\n",
        "                df_all[\"ATR\"] = _calc_atr(df_all, 14)\n",
        "\n",
        "            l1, l2, l3, l4 = LENGTHS\n",
        "            s1, s2, s3, s4 = SMOOTHS\n",
        "            generate_trade_signals(df_all, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "            _ensure_order_cols(df_all)\n",
        "\n",
        "            # marcar source=1 para filas nuevas\n",
        "            if not df_new.empty:\n",
        "                new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
        "                if new_times:\n",
        "                    df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
        "\n",
        "            # abrir (si hay señal)\n",
        "            await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT_, comment=COMMENT_, magic=MAGIC)\n",
        "\n",
        "            # si se abrió, sincroniza TYPE y columnas de mercado desde API inmediatamente\n",
        "            api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
        "            _sync_type_in_df(df_all, api_type)\n",
        "            await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "            stamp_system_time(df_all, \"last\")\n",
        "            save_csv(df_all)\n",
        "            print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"), \"| actualización (modo búsqueda de entrada)\")\n",
        "\n",
        "        # ───────────── CON posición → cada 20s seguimiento/SL\n",
        "        else:\n",
        "            print(f\"▶ Modo seguimiento: posición con magic {MAGIC} detectada\")\n",
        "\n",
        "            while await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "\n",
        "                # 0) Alinear a rejilla 00/20/40s\n",
        "                await _sleep_to_20s_grid()\n",
        "\n",
        "                # 1) Cargar CSV y última 'time'\n",
        "                df_all = _load_csv()\n",
        "                last_time = None\n",
        "                if not df_all.empty and \"time\" in df_all.columns:\n",
        "                    last_time = pd.to_datetime(df_all[\"time\"], errors=\"coerce\", utc=True).max()\n",
        "\n",
        "                # 2) Traer vela/s nuevas (> last_time)\n",
        "                df_new = await get_current_candle_snapshot(account, rpc_conn, symbol=SYMBOL, timeframe=time_frame_data)\n",
        "                df_new = df_new.drop_duplicates(\"time\").sort_values(\"time\")\n",
        "                df_inc = df_new[df_new[\"time\"] > last_time] if (last_time is not None and pd.notna(last_time)) else df_new\n",
        "\n",
        "                # 3) Fusionar SOLO nuevas\n",
        "                if not df_inc.empty:\n",
        "                    df_all = pd.concat([df_all, df_inc], ignore_index=True)\n",
        "\n",
        "                # 4) Recalcular ATR y señales\n",
        "                if len(df_all) >= 14:\n",
        "                    df_all[\"ATR\"] = _calc_atr(df_all, 14)\n",
        "\n",
        "                l1, l2, l3, l4 = LENGTHS\n",
        "                s1, s2, s3, s4 = SMOOTHS\n",
        "                generate_trade_signals(df_all, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "\n",
        "                # 5) Intentar cerrar si hay señal de cierre (pendiente de kal_4)\n",
        "                await close_order(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC, close_col=\"Close_Trade\")\n",
        "\n",
        "                # 6) Recalcular métricas y stop dinámico\n",
        "                atr_close(df_all)\n",
        "                tick_dyn_atr(df_all)\n",
        "\n",
        "                # 7) Enviar modificación de SL SI CAMBIÓ Stop_Loss_$\n",
        "                sl_res = await modify_stoploss_if_changed(\n",
        "                    df_all, rpc_conn,\n",
        "                    symbol=SYMBOL, magic=MAGIC,\n",
        "                    auth_token=META_API_TOKEN,\n",
        "                    account_id=ACCOUNT_ID,\n",
        "                    region=REGION,\n",
        "                    tol=0.0  # exige diferencia estricta\n",
        "                )\n",
        "\n",
        "                # 8) Sincronizar SIEMPRE columnas de mercado (incluye Real_SL leído)\n",
        "                await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "                # 9) Sellar hora / marcar source y guardar\n",
        "                stamp_system_time(df_all, \"last\")\n",
        "                _ensure_order_cols(df_all)\n",
        "                if not df_inc.empty:\n",
        "                    df_all.loc[df_all[\"time\"].isin(df_inc[\"time\"]), \"source\"] = 9\n",
        "                else:\n",
        "                    df_all.at[df_all.index[-1], \"source\"] = 9\n",
        "                save_csv(df_all)\n",
        "\n",
        "                # 10) Logs (incluye segunda línea cuando SL se envió/intentó enviar)\n",
        "                base = (dt.datetime.utcnow().strftime(\"%H:%M:%S\")\n",
        "                        + \" | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\")\n",
        "                print(base)\n",
        "                if sl_res.get(\"changed\"):\n",
        "                    if sl_res.get(\"sent\"):\n",
        "                        print(f\"   ↳ SL actualizado en broker a {sl_res['price']:.2f} (positionId={sl_res.get('position_id','?')})\")\n",
        "                    else:\n",
        "                        msg_err = sl_res.get(\"err\") or \"error desconocido\"\n",
        "                        price = sl_res.get(\"price\", np.nan)\n",
        "                        print(f\"   ↳ SL cambió a {price:.2f} pero no se pudo enviar ({msg_err})\")\n",
        "\n",
        "            print(f\"⏹ La posición con magic {MAGIC} se cerró → regreso a búsqueda de entrada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W481PrVufZ8Z",
      "metadata": {
        "id": "W481PrVufZ8Z"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yI6v2q_xfRtg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI6v2q_xfRtg",
        "outputId": "442d5713-66bc-4239-db02-f286c89cd59f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:00:04 | actualización (modo búsqueda de entrada)\n",
            "✅ SELL placed | orderId=1213383479 positionId=1213383479 openPrice=112880.5 | SL sent=113438.2314 | Type=Short\n",
            "14:05:04 | actualización (modo búsqueda de entrada)\n",
            "▶ Modo seguimiento: posición con magic 900001 detectada\n",
            "14:05:20 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:05:42 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:06:01 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:06:21 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:06:41 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:07:01 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:07:20 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL actualizado en broker a 112880.50 (positionId=1213383479)\n",
            "14:07:40 | seguimiento 20s: señales, posible cierre, atr_close + tick_dyn_atr + sync_market\n",
            "   ↳ SL cambió a 112880.50 pero no se pudo enviar (No hay posición con ese magic)\n",
            "⏹ La posición con magic 900001 se cerró → regreso a búsqueda de entrada\n",
            "14:10:03 | actualización (modo búsqueda de entrada)\n"
          ]
        }
      ],
      "source": [
        "###############################################################################\n",
        "# EJECUCIÓN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()     # solo en notebooks\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}