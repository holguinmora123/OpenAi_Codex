{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/holguinmora123/OpenAi_Codex/blob/main/5_4_MetaApi_Conda_Kal_Optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf03429",
      "metadata": {
        "id": "eaf03429"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nu7i62hlGthO",
      "metadata": {
        "id": "nu7i62hlGthO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f543f6-604a-40c8-e605-415c956e7a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from ta-lib) (1.3.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from ta-lib) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta-lib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Ej4IeD7EeF0t",
      "metadata": {
        "id": "Ej4IeD7EeF0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fed0d9a-9b65-4cf5-b8ed-15063275ca8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: metaapi-cloud-sdk in /usr/local/lib/python3.12/dist-packages (29.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.12.15)\n",
            "Requirement already satisfied: python-engineio<4.0.0,>=3.14.2 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (4.15.0)\n",
            "Requirement already satisfied: iso8601 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2025.2)\n",
            "Requirement already satisfied: python-socketio<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (0.28.1)\n",
            "Requirement already satisfied: metaapi-cloud-copyfactory-sdk<13.0.0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (12.0.0)\n",
            "Requirement already satisfied: metaapi-cloud-metastats-sdk<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (6.0.0)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade metaapi-cloud-sdk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DF5CD6j5nmdo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5CD6j5nmdo",
        "outputId": "9ff20857-b73e-412e-9dc6-fa3da5cee826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.7\n",
            "ATR(ndarray high, ndarray low, ndarray close, int timeperiod=-0x80000000)\n",
            "\n",
            "ATR(high, low, close[, timeperiod=?])\n",
            "\n",
            "Average True Range (Volatility Indicators)\n",
            "\n",
            "Inputs:\n",
            "    prices: ['high', 'low', 'close']\n",
            "Parameters:\n",
            "    timeperiod: 14\n",
            "Outputs:\n",
            "    real\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function talib._ta_lib.ATR(high, low, close, timeperiod=-2147483648)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import talib as ta\n",
        "print(ta.__version__)  # Should print something like 0.4.28\n",
        "print(ta.ATR.__doc__)  # Confirm ATR function works\n",
        "ta.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime as dt\n",
        "import logging\n",
        "\n",
        "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
        "\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from metaapi_cloud_sdk import MetaApi\n",
        "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "#from collections.abc import Sequence\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import json, re, traceback, requests\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "try:\n",
        "    from metaapi_cloud_sdk import MetaApi\n",
        "except Exception:\n",
        "    MetaApi = None  # permite importar el m\u00f3dulo incluso sin el SDK instalado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KFfn45ty82qv",
      "metadata": {
        "id": "KFfn45ty82qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1ac7e0-646d-4323-8bd6-e796c9db1e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5RqjoAtFwl",
        "outputId": "cb4b88d0-57a5-45d2-d553-2f34e2bb3a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Course Folder/Forex/XAUUSD/\n"
          ]
        }
      ],
      "source": [
        "process = 'Train'\n",
        "SYMBOL = 'XAUUSD'\n",
        "\n",
        "root_data = f'/content/drive/MyDrive/Course Folder/Forex/{SYMBOL}/'\n",
        "print(root_data)\n",
        "\n",
        "rolling_window = 100\n",
        "\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoiY29weWZhY3RvcnktYXBpIiwibWV0aG9kcyI6WyJjb3B5ZmFjdG9yeS1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoibXQtbWFuYWdlci1hcGkiLCJtZXRob2RzIjpbIm10LW1hbmFnZXItYXBpOnJlc3Q6ZGVhbGluZzoqOioiLCJtdC1tYW5hZ2VyLWFwaTpyZXN0OnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJiaWxsaW5nLWFwaSIsIm1ldGhvZHMiOlsiYmlsbGluZy1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTk3NjkyMTMsImV4cCI6MTc2NzU0NTIxM30.icWvqBmO9uUbHubK_Vr95aLr9fvHft_Qv-kH197w9V9JZfVeqoBY96Kf37sc2nI6sjoP_VoMshRRn7hFV1-rn5ve6-TR8wBxVGeX8ny7HcS70CJZzVtIrqjtWdxn4UB5dbTT5dUlNvGKjRLELFYPRcH-kP1YPXqwoS8EYTdh3KgKKJ-wr7B6vJnoPyL87ew80J-5r4yZYWl_c3lLOxoEJzUvVQ41mRwbuTLcVXZB3d9u0nfjP-Uufn43a25lizzvT4MmfP6jt0OL2Lh23M08LL25Zr_spCw7kz-KZAPM300AAIKWQ30Bh9weKGh8gFrUxxofRWhBmPnnGhvGuNk5q5xDoI17yv64t7WJ_VgPzPseK4A5SHM56Ifhh6CNZ-xgd1CyJzwA6oqP1TEVXrrWtkGM3ese2HvIS9sTlRw7sP9fBMCgYWecp98nk9D_3nawg4BxuPciwFyJuY8Afqbwfj1cUKMPsaDgWEkxDsdZ9jL1ZNJf2FXN_rRmyZX5YUw3RLifsU_66e2nsw2YqtN2E_HdugBvBm24KW2gKM0QeWihjfNtuI4Ve7MaaLhON5RfacOCAIhIYB47hZDccNTzvXznYmDhmInNcVNgohYig7F3cP8hETmsJd0TUdRgT80IXta4HLWQQnSCKxoszsBtv4k5US0PCM-5HZxEy7shxhk'\n",
        "#ACCOUNT_ID     = '163d9a57-1f07-4e78-a6af-036efe867c1b'\n",
        "ACCOUNT_ID     = '26ca5360-ebcf-4361-859a-b682a6dc383c'\n",
        "\n",
        "LOT      = 1.0\n",
        "COMMENT  = \"Insta\"\n",
        "CANDLE_NUMBER = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dTlwjirFwRfj",
      "metadata": {
        "id": "dTlwjirFwRfj"
      },
      "outputs": [],
      "source": [
        "# cache simple en m\u00f3dulo\n",
        "__CONNECTION_CHECKED = False\n",
        "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
        "\n",
        "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Conecta v\u00eda RPC y espera sincronizaci\u00f3n. Lanza excepci\u00f3n si no se logra.\n",
        "    Devuelve (account, rpc_conn).\n",
        "    \"\"\"\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # refrescamos para leer estado/connectionStatus\n",
        "    try:\n",
        "        await account.reload()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Conexi\u00f3n RPC + sincronizaci\u00f3n del terminal\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "    await rpc_conn.wait_synchronized()  # espera a que el terminal est\u00e9 listo\n",
        "\n",
        "    # Sonda r\u00e1pida para confirmar conectividad real con el terminal\n",
        "    try:\n",
        "        _ = await rpc_conn.get_account_information()\n",
        "    except Exception:\n",
        "        # si falla la sonda, igual devolvemos la conexi\u00f3n (ya sincronizada)\n",
        "        pass\n",
        "\n",
        "    return account, rpc_conn\n",
        "\n",
        "def _run(coro):\n",
        "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "def check_connection_once(token: str, account_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida la conexi\u00f3n y sincronizaci\u00f3n SOLO la primera vez que se llama.\n",
        "    En llamadas posteriores no vuelve a conectar.\n",
        "    \"\"\"\n",
        "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
        "    if __CONNECTION_CHECKED:\n",
        "        print(\"\u2139\ufe0f Conexi\u00f3n ya validada en esta sesi\u00f3n; no se repite.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
        "        __ACCOUNT_CONN = (account, rpc_conn)\n",
        "        __CONNECTION_CHECKED = True\n",
        "        print(f\"\u2705 Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
        "        return True\n",
        "    except TimeoutException as e:\n",
        "        print(f\"\u274c Timeout esperando sincronizaci\u00f3n. \u00bfLa cuenta est\u00e1 CONNECTED al broker? Detalle: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c No fue posible validar la conexi\u00f3n. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def _safe_json_dump(value):\n",
        "    try:\n",
        "        return json.dumps(value, indent=2, default=str)\n",
        "    except Exception:\n",
        "        return str(value)\n",
        "\n",
        "def print_order_error_details(ctx: dict, err: Exception):\n",
        "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
        "    print(\"\\n\" + \"\u2718\" * 70)\n",
        "    print(\"\u274c Order failed\")\n",
        "    print(\"\u2022 Exception type:\", type(err).__name__)\n",
        "    print(\"\u2022 Message       :\", str(err))\n",
        "\n",
        "    # Known useful attributes often present on MetaApi exceptions\n",
        "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
        "        if hasattr(err, attr):\n",
        "            val = getattr(err, attr)\n",
        "            if val:\n",
        "                print(f\"\u2022 {attr:12}: {_safe_json_dump(val)}\")\n",
        "\n",
        "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
        "    msg = str(err)\n",
        "    m = re.search(r\"\\{.*\\}\", msg)\n",
        "    if m:\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "            print(\"\u2022 parsed_json  :\", _safe_json_dump(payload))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Stack (useful while debugging)\n",
        "    print(\"\u2022 traceback    :\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Context of the attempt\n",
        "    print(\"\u2022 context      :\", _safe_json_dump(ctx))\n",
        "    print(\"\u2718\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RjShygBIwWRp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjShygBIwWRp",
        "outputId": "75e03869-7cb8-4605-c848-ca9ad859670a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-10-07T15:36:57.555927] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-a.agiliumtrade.ai shared server.\n",
            "[2025-10-07T15:36:57.563326] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-b.agiliumtrade.ai shared server.\n",
            "[2025-10-07T15:36:58.561104] london:1: MetaApi websocket client connected to the MetaApi server\n",
            "[2025-10-07T15:36:58.585938] london:0: MetaApi websocket client connected to the MetaApi server\n",
            "\u2705 Conectado y sincronizado con MetaApi. account_id=26ca5360-ebcf-4361-859a-b682a6dc383c\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfxZvvtJeFh1",
      "metadata": {
        "id": "HfxZvvtJeFh1"
      },
      "source": [
        "# Real_Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Sx6TbzBOBzZl",
      "metadata": {
        "id": "Sx6TbzBOBzZl"
      },
      "outputs": [],
      "source": [
        "SYMBOL = \"XAUUSD\"\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "FETCH_INTERVAL   = 60\n",
        "time_frame_data  = \"1m\"\n",
        "CANDEL_NUMBER    = 900\n",
        "\n",
        "LOT     = 0.5\n",
        "COMMENT = \"Kalman\"\n",
        "\n",
        "length_1 = 600\n",
        "length_2 = 520\n",
        "length_3 = 710\n",
        "length_4 = 1130\n",
        "\n",
        "smooth_1 = 3\n",
        "smooth_2 = 3\n",
        "smooth_3 = 3\n",
        "smooth_4 = 7\n",
        "\n",
        "INITIAL_SL         = -2\n",
        "FIRST_STEP_ATR     = 0.5\n",
        "GAP_FIRST_STEP_ATR = 2\n",
        "\n",
        "#REGION = \"new-york\"\n",
        "REGION = \"london\"\n",
        "\n",
        "META_API_TOKEN = 'eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiJhOGYxYmQ1ZTY2YzlhYWYxYzM4ZjVjMmI0MGFhZjMwYyIsImFjY2Vzc1J1bGVzIjpbeyJpZCI6InRyYWRpbmctYWNjb3VudC1tYW5hZ2VtZW50LWFwaSIsIm1ldGhvZHMiOlsidHJhZGluZy1hY2NvdW50LW1hbmFnZW1lbnQtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVzdC1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcnBjLWFwaSIsIm1ldGhvZHMiOlsibWV0YWFwaS1hcGk6d3M6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6Im1ldGFhcGktcmVhbC10aW1lLXN0cmVhbWluZy1hcGkiLCJtZXRob2RzIjpbIm1ldGFhcGktYXBpOndzOnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJtZXRhc3RhdHMtYXBpIiwibWV0aG9kcyI6WyJtZXRhc3RhdHMtYXBpOnJlc3Q6cHVibGljOio6KiJdLCJyb2xlcyI6WyJyZWFkZXIiLCJ3cml0ZXIiXSwicmVzb3VyY2VzIjpbIio6JFVTRVJfSUQkOioiXX0seyJpZCI6InJpc2stbWFuYWdlbWVudC1hcGkiLCJtZXRob2RzIjpbInJpc2stbWFuYWdlbWVudC1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoiY29weWZhY3RvcnktYXBpIiwibWV0aG9kcyI6WyJjb3B5ZmFjdG9yeS1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciIsIndyaXRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfSx7ImlkIjoibXQtbWFuYWdlci1hcGkiLCJtZXRob2RzIjpbIm10LW1hbmFnZXItYXBpOnJlc3Q6ZGVhbGluZzoqOioiLCJtdC1tYW5hZ2VyLWFwaTpyZXN0OnB1YmxpYzoqOioiXSwicm9sZXMiOlsicmVhZGVyIiwid3JpdGVyIl0sInJlc291cmNlcyI6WyIqOiRVU0VSX0lEJDoqIl19LHsiaWQiOiJiaWxsaW5nLWFwaSIsIm1ldGhvZHMiOlsiYmlsbGluZy1hcGk6cmVzdDpwdWJsaWM6KjoqIl0sInJvbGVzIjpbInJlYWRlciJdLCJyZXNvdXJjZXMiOlsiKjokVVNFUl9JRCQ6KiJdfV0sImlnbm9yZVJhdGVMaW1pdHMiOmZhbHNlLCJ0b2tlbklkIjoiMjAyMTAyMTMiLCJpbXBlcnNvbmF0ZWQiOmZhbHNlLCJyZWFsVXNlcklkIjoiYThmMWJkNWU2NmM5YWFmMWMzOGY1YzJiNDBhYWYzMGMiLCJpYXQiOjE3NTk3NjkyMTMsImV4cCI6MTc2NzU0NTIxM30.icWvqBmO9uUbHubK_Vr95aLr9fvHft_Qv-kH197w9V9JZfVeqoBY96Kf37sc2nI6sjoP_VoMshRRn7hFV1-rn5ve6-TR8wBxVGeX8ny7HcS70CJZzVtIrqjtWdxn4UB5dbTT5dUlNvGKjRLELFYPRcH-kP1YPXqwoS8EYTdh3KgKKJ-wr7B6vJnoPyL87ew80J-5r4yZYWl_c3lLOxoEJzUvVQ41mRwbuTLcVXZB3d9u0nfjP-Uufn43a25lizzvT4MmfP6jt0OL2Lh23M08LL25Zr_spCw7kz-KZAPM300AAIKWQ30Bh9weKGh8gFrUxxofRWhBmPnnGhvGuNk5q5xDoI17yv64t7WJ_VgPzPseK4A5SHM56Ifhh6CNZ-xgd1CyJzwA6oqP1TEVXrrWtkGM3ese2HvIS9sTlRw7sP9fBMCgYWecp98nk9D_3nawg4BxuPciwFyJuY8Afqbwfj1cUKMPsaDgWEkxDsdZ9jL1ZNJf2FXN_rRmyZX5YUw3RLifsU_66e2nsw2YqtN2E_HdugBvBm24KW2gKM0QeWihjfNtuI4Ve7MaaLhON5RfacOCAIhIYB47hZDccNTzvXznYmDhmInNcVNgohYig7F3cP8hETmsJd0TUdRgT80IXta4HLWQQnSCKxoszsBtv4k5US0PCM-5HZxEy7shxhk'\n",
        "ACCOUNT_ID     = '26ca5360-ebcf-4361-859a-b682a6dc383c'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sKKJrsFBfOoT",
      "metadata": {
        "id": "sKKJrsFBfOoT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PAR\u00c1METROS / DEFAULTS SEGUROS (no rompen si faltan globales)\n",
        "# ============================================================================\n",
        "try:\n",
        "    FILE_PATH  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FILE_PATH = \"xauusd_data.csv\"\n",
        "\n",
        "try:\n",
        "    SYMBOL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    SYMBOL = \"XAUUSD\"\n",
        "\n",
        "try:\n",
        "    time_frame_data  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    time_frame_data = \"1m\"\n",
        "\n",
        "try:\n",
        "    CANDEL_NUMBER  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    CANDEL_NUMBER = 100\n",
        "\n",
        "try:\n",
        "    INITIAL_SL  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    INITIAL_SL = -1.0  # m\u00faltiplos ATR (negativo para BUY)\n",
        "\n",
        "try:\n",
        "    FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    FIRST_STEP_ATR = 0.5\n",
        "\n",
        "try:\n",
        "    GAP_FIRST_STEP_ATR  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    GAP_FIRST_STEP_ATR = 2.0\n",
        "\n",
        "try:\n",
        "    META_API_TOKEN  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    META_API_TOKEN = \"\"  # pon tu token real\n",
        "\n",
        "try:\n",
        "    ACCOUNT_ID  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    ACCOUNT_ID = \"\"  # pon tu account id\n",
        "\n",
        "try:\n",
        "    REGION  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    REGION = \"new-york\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOGGING\n",
        "# ============================================================================\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\", \"websockets\"):\n",
        "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
        "if \"MetaApi\" in globals() and MetaApi:\n",
        "    try:\n",
        "        MetaApi.enable_logging()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FILTRO KALMAN + INSTA\n",
        "# ============================================================================\n",
        "\n",
        "def kalman_line(source: pd.Series | Sequence[float], kalman_length: int, smooth: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    N\u00facleo Kalman + EMA con semilla SMA (comportamiento de TradingView ta.ema).\n",
        "    Devuelve una Serie 'kal' indexada como 'source'.\n",
        "    - Warmup: NaN en las primeras 'smooth-1' barras (igual a Pine).\n",
        "    \"\"\"\n",
        "    src = pd.Series(source, dtype=\"float64\").copy()\n",
        "    n = len(src)\n",
        "    kal = pd.Series(np.nan, index=src.index, dtype=\"float64\", name=\"kal\")\n",
        "    if n == 0:\n",
        "        return kal\n",
        "\n",
        "    Lk = max(int(kalman_length), 1)\n",
        "    Ls = max(int(smooth), 1)\n",
        "\n",
        "    # Par\u00e1metros como en Pine\n",
        "    sqrt_term   = np.sqrt(Lk / 10000.0 * 2.0)\n",
        "    length_term = Lk / 10000.0\n",
        "\n",
        "    # N\u00facleo Kalman (kf_c) con la misma l\u00f3gica nz() de Pine para los previos\n",
        "    kf_c = np.empty(n, dtype=\"float64\")\n",
        "    kf_c[:] = np.nan\n",
        "    velo = np.empty(n, dtype=\"float64\")\n",
        "    velo[:] = np.nan\n",
        "\n",
        "    # Bar 0 (equivalente a: dk=0, velo=0, kf_c = source)\n",
        "    kf_c[0] = float(src.iloc[0])\n",
        "    velo[0] = 0.0\n",
        "\n",
        "    for i in range(1, n):\n",
        "        prev_kf   = kf_c[i-1] if np.isfinite(kf_c[i-1]) else float(src.iloc[i])\n",
        "        prev_velo = velo[i-1] if np.isfinite(velo[i-1]) else 0.0\n",
        "        dk = float(src.iloc[i]) - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term\n",
        "        velo[i]  = prev_velo + length_term * dk\n",
        "        kf_c[i]  = smooth_c + velo[i]\n",
        "\n",
        "    kf = pd.Series(kf_c, index=src.index)\n",
        "\n",
        "    # ta.ema(kf, Ls) con semilla SMA y warmup NaN\n",
        "    if n >= Ls:\n",
        "        # Semilla SMA\n",
        "        sma0 = float(kf.iloc[:Ls].mean())\n",
        "        kal.iloc[Ls-1] = sma0\n",
        "        alpha = 2.0 / (Ls + 1.0)\n",
        "        for i in range(Ls, n):\n",
        "            kal.iloc[i] = alpha * kf.iloc[i] + (1.0 - alpha) * kal.iloc[i-1]\n",
        "    # Si n < Ls: todo NaN, como en Pine\n",
        "\n",
        "    return kal\n",
        "\n",
        "# ============================================================================\n",
        "# UTILIDADES DE COLUMNAS / CSV\n",
        "# ============================================================================\n",
        "\n",
        "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Crea/normaliza columnas clave con dtypes consistentes y\n",
        "    elimina columnas heredadas ('id', 'actionType') si existieran.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # elimina columnas heredadas\n",
        "    for col in (\"id\", \"actionType\"):\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    col_types = {\n",
        "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
        "        \"orderId\":       \"string\",\n",
        "        \"magic\":         \"Int64\",\n",
        "        \"symbol\":        \"string\",\n",
        "        \"openPrice\":     \"float64\",\n",
        "        \"comment\":       \"string\",\n",
        "        \"Type\":          \"string\",\n",
        "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
        "        \"Stop_Loss_atr\": \"float64\",\n",
        "        \"Stop_Loss_$\":   \"float64\",\n",
        "        \"Take_Profit_atr\": \"float64\",\n",
        "        \"Take_Profit_$\":   \"float64\",\n",
        "        \"Real_SL\":       \"float64\",\n",
        "        \"ATR\":           \"float64\",\n",
        "        \"atr_mult_high\": \"float64\",\n",
        "        \"atr_mult_low\":  \"float64\",\n",
        "        \"trade_size\":    \"float64\",\n",
        "        \"profits\":       \"float64\",\n",
        "        \"base_px\":       \"float64\",\n",
        "        \"atr_base\":      \"float64\",\n",
        "        \"source\":        \"Int64\",\n",
        "        \"time\":          \"datetime64[ns, UTC]\",\n",
        "        \"open\":          \"float64\",\n",
        "        \"high\":          \"float64\",\n",
        "        \"low\":           \"float64\",\n",
        "        \"close\":         \"float64\",\n",
        "        \"volume\":        \"float64\",\n",
        "        \"tickVolume\":    \"float64\",\n",
        "        \"spread\":        \"float64\",\n",
        "    }\n",
        "\n",
        "    for c, dtp in col_types.items():\n",
        "        if c not in df.columns:\n",
        "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.NaT\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
        "            elif dtp == \"string\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
        "            else:\n",
        "                df[c] = np.nan\n",
        "\n",
        "    # normaliza sin romper datos existentes\n",
        "    for c, dtp in col_types.items():\n",
        "        try:\n",
        "            if dtp == \"string\":\n",
        "                df[c] = df[c].astype(\"string\")\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = df[c].astype(\"Int64\")\n",
        "            elif isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "            # floats/num los dejamos tal cual para no forzar conversiones peligrosas\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la \u00faltima fila\n",
        "    mode=\"missing\": rellena donde est\u00e9 NaT/NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "\n",
        "def _fmt_dt_cols(df: pd.DataFrame, cols=(\"System_time\", \"time\", \"Entry_Date\")) -> None:\n",
        "    \"\"\"Formatea columnas datetime a string 'YYYY-mm-dd HH:MM:SS' sin tz.\"\"\"\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Reordena columnas y guarda CSV con tiempos formateados.\n",
        "    Orden deseado:\n",
        "      \u2022 System_time antes de 'time'\n",
        "      \u2022 'source' a la derecha de 'time' y antes de 'open'\n",
        "      \u2022 Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
        "      \u2022 Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    df_out = df.copy()\n",
        "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "    _fmt_dt_cols(df_out)\n",
        "\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols:\n",
        "            return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns:\n",
        "                    cols.insert(i, c)\n",
        "                    i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns and c not in cols:\n",
        "                    cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols:\n",
        "            cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols:\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else:\n",
        "                cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    cols = [c for c in df_out.columns if c not in (\"id\", \"brokerTime\", \"actionType\")]\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "\n",
        "    # Filtra por columnas existentes para evitar ValueError en to_csv\n",
        "    ordered = [c for c in ordered if c in df_out.columns]\n",
        "    df_out.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "\n",
        "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Migra CSV existente:\n",
        "      \u2022 Elimina columnas heredadas.\n",
        "      \u2022 Asegura columnas nuevas: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
        "      \u2022 Formatea tiempos.\n",
        "      \u2022 Reordena columnas al formato actual.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    ensure_cols = {\n",
        "        \"System_time\": pd.NaT,\n",
        "        \"profits\":     np.nan,\n",
        "        \"trade_size\":  np.nan,\n",
        "        \"base_px\":     np.nan,\n",
        "        \"atr_base\":    np.nan,\n",
        "        \"source\":      pd.NA,\n",
        "        \"Real_SL\":     np.nan,\n",
        "    }\n",
        "    for c, default in ensure_cols.items():\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    _fmt_dt_cols(df)\n",
        "\n",
        "    # Reusar l\u00f3gica de save_csv para reordenar\n",
        "    save_csv(df, path=path)\n",
        "\n",
        "\n",
        "def _load_csv(path: str = FILE_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware y elimina columnas heredadas.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        dtype={\n",
        "            \"orderId\": \"string\",\n",
        "            \"symbol\":  \"string\",\n",
        "            \"comment\": \"string\",\n",
        "            \"Type\":    \"string\",\n",
        "        }\n",
        "    )\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# METAAPI: CONEXI\u00d3N Y DATOS\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio, datetime as dt\n",
        "\n",
        "async def connect_metaapi(token: str, account_id: str, *, rpc_timeout=60, retries=3):\n",
        "    \"\"\"\n",
        "    Devuelve (account) con RPC intentado. Si la sincronizaci\u00f3n falla (DNS / timeout),\n",
        "    seguimos en modo REST con el mismo 'account' (las llamadas RPC tendr\u00e1n fallback).\n",
        "    \"\"\"\n",
        "    if \"MetaApi\" not in globals() or MetaApi is None:\n",
        "        raise RuntimeError(\"metaapi_cloud_sdk no est\u00e1 disponible en el entorno.\")\n",
        "\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "\n",
        "    # Asegurar despliegue/conexi\u00f3n de la cuenta (no falla si el SDK no expone algo)\n",
        "    try:\n",
        "        await account.reload()\n",
        "        if getattr(account, \"state\", \"\").upper() != \"DEPLOYED\":\n",
        "            await account.deploy()\n",
        "            if hasattr(account, \"wait_deployed\"):\n",
        "                await account.wait_deployed()\n",
        "        if hasattr(account, \"wait_connected\"):\n",
        "            await account.wait_connected()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    conn = account.get_rpc_connection()\n",
        "\n",
        "    # Intentos de conectar/sincronizar RPC con backoff\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            await conn.connect()\n",
        "            try:\n",
        "                # seg\u00fan versi\u00f3n del SDK, wait_synchronized puede o no aceptar timeout\n",
        "                await asyncio.wait_for(conn.wait_synchronized(), timeout=rpc_timeout)\n",
        "            except TypeError:\n",
        "                await conn.wait_synchronized()\n",
        "            except asyncio.TimeoutError:\n",
        "                raise TimeoutError(\"RPC wait_synchronized timeout\")\n",
        "            return account  # RPC OK\n",
        "        except Exception as e:\n",
        "            if attempt == retries:\n",
        "                logging.warning(\"RPC no sincroniz\u00f3 (%s). Continuando con REST-only; habr\u00e1 fallbacks.\", e)\n",
        "                return account\n",
        "            await asyncio.sleep(min(5 * attempt, 15))\n",
        "\n",
        "    return account\n",
        "\n",
        "\n",
        "async def get_current_candle_snapshot(account,\n",
        "                                      rpc_conn,\n",
        "                                      symbol: str = SYMBOL,\n",
        "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve 1 fila con la vela m\u00e1s reciente (puede ser la vela en curso).\n",
        "    \"\"\"\n",
        "    def _to_row(c: dict) -> dict:\n",
        "        return {\n",
        "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
        "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
        "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
        "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
        "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
        "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
        "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
        "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
        "        }\n",
        "\n",
        "    # 1) RPC\n",
        "    try:\n",
        "        try:\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe, limit=1)\n",
        "        except TypeError:\n",
        "            to_ts = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
        "            candles = await rpc_conn.get_candles(symbol=symbol, timeframe=timeframe,\n",
        "                                                 start_time=from_ts, end_time=to_ts)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Hist\u00f3rico como respaldo\n",
        "    try:\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe=timeframe, start_time=None, limit=1)\n",
        "        if candles:\n",
        "            return pd.DataFrame([_to_row(candles[-1])])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
        "\n",
        "\n",
        "async def get_candles_5m(account,\n",
        "                         start: dt.datetime | None,\n",
        "                         limit: int = CANDEL_NUMBER,\n",
        "                         *,\n",
        "                         retries: int = 5,\n",
        "                         retry_delay: float = 5.0) -> pd.DataFrame:\n",
        "    \"\"\"Descarga velas hist\u00f3ricas (usa time_frame_data configurado) con reintentos y fallback REST.\"\"\"\n",
        "    columns = [\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"]\n",
        "    last_err: Exception | None = None\n",
        "    max_retries = max(int(retries), 1)\n",
        "\n",
        "    def _candles_to_df(candles) -> pd.DataFrame:\n",
        "        rows = [{\n",
        "            \"time\": pd.to_datetime(c.get(\"time\"), utc=True),\n",
        "            \"open\": c.get(\"open\"),\n",
        "            \"high\": c.get(\"high\"),\n",
        "            \"low\": c.get(\"low\"),\n",
        "            \"close\": c.get(\"close\"),\n",
        "            \"volume\": c.get(\"volume\"),\n",
        "            \"tickVolume\": c.get(\"tickVolume\"),\n",
        "            \"spread\": c.get(\"spread\")\n",
        "        } for c in (candles or [])]\n",
        "        return pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "    def _normalize_start(ts_in: dt.datetime | pd.Timestamp | None) -> dt.datetime | None:\n",
        "        if ts_in is None:\n",
        "            return None\n",
        "        ts = pd.Timestamp(ts_in)\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.tz_localize('UTC')\n",
        "        else:\n",
        "            ts = ts.tz_convert('UTC')\n",
        "        return ts.to_pydatetime()\n",
        "\n",
        "    async def _rest_fallback() -> pd.DataFrame:\n",
        "        loop = asyncio.get_running_loop()\n",
        "\n",
        "        def _call_rest() -> pd.DataFrame:\n",
        "            base_url = (\n",
        "                f\"https://mt-market-data-client-api-v1.{REGION}.agiliumtrade.ai/\"\n",
        "                f\"users/current/accounts/{ACCOUNT_ID}/historical-market-data/symbols/{SYMBOL}/\"\n",
        "                f\"timeframes/{time_frame_data}/candles\"\n",
        "            )\n",
        "            params: Dict[str, Any] = {\"limit\": int(limit)}\n",
        "            if start is not None:\n",
        "                ts = pd.Timestamp(start)\n",
        "                if ts.tzinfo is None:\n",
        "                    ts = ts.tz_localize('UTC')\n",
        "                else:\n",
        "                    ts = ts.tz_convert('UTC')\n",
        "                params[\"startTime\"] = ts.isoformat().replace('+00:00', 'Z')\n",
        "            headers = {\"Accept\": \"application/json\", \"auth-token\": META_API_TOKEN}\n",
        "            resp = requests.get(base_url, headers=headers, params=params, timeout=20)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json() or []\n",
        "            rows = [{\n",
        "                \"time\": pd.to_datetime(item.get(\"time\"), utc=True),\n",
        "                \"open\": item.get(\"open\"),\n",
        "                \"high\": item.get(\"high\"),\n",
        "                \"low\": item.get(\"low\"),\n",
        "                \"close\": item.get(\"close\"),\n",
        "                \"volume\": item.get(\"volume\"),\n",
        "                \"tickVolume\": item.get(\"tickVolume\"),\n",
        "                \"spread\": item.get(\"spread\")\n",
        "            } for item in data]\n",
        "            return pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "        return await loop.run_in_executor(None, _call_rest)\n",
        "\n",
        "    start_norm = _normalize_start(start)\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            candles = await asyncio.wait_for(\n",
        "                account.get_historical_candles(\n",
        "                    symbol=SYMBOL,\n",
        "                    timeframe=time_frame_data,\n",
        "                    start_time=start_norm,\n",
        "                    limit=limit\n",
        "                ),\n",
        "                timeout=30\n",
        "            )\n",
        "            return _candles_to_df(candles)\n",
        "        except (asyncio.TimeoutError, TimeoutException) as exc:\n",
        "            last_err = exc\n",
        "            logging.warning(\n",
        "                \"Timeout al descargar velas (intento %s/%s): %s\",\n",
        "                attempt,\n",
        "                max_retries,\n",
        "                exc\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            last_err = exc\n",
        "            logging.warning(\n",
        "                \"Fallo al descargar velas (intento %s/%s): %s\",\n",
        "                attempt,\n",
        "                max_retries,\n",
        "                exc\n",
        "            )\n",
        "            try:\n",
        "                await account.reload()\n",
        "                if hasattr(account, \"wait_connected\"):\n",
        "                    await account.wait_connected()\n",
        "            except Exception:\n",
        "                pass\n",
        "        if attempt < max_retries:\n",
        "            await asyncio.sleep(min(retry_delay * attempt, 30))\n",
        "\n",
        "    try:\n",
        "        df_rest = await _rest_fallback()\n",
        "        if not df_rest.empty:\n",
        "            logging.warning(\n",
        "                \"Usando fallback REST tras %s intentos fallidos de RPC.\",\n",
        "                max_retries\n",
        "            )\n",
        "            return df_rest\n",
        "    except Exception as rest_err:\n",
        "        logging.error(\"Fallo el fallback REST para velas: %s\", rest_err)\n",
        "        if last_err is not None:\n",
        "            raise last_err from rest_err\n",
        "        raise\n",
        "\n",
        "    if last_err is not None:\n",
        "        logging.error(\"No se pudieron obtener velas tras %s intentos: %s\", max_retries, last_err)\n",
        "        raise last_err\n",
        "\n",
        "    return pd.DataFrame(columns=columns)\n",
        "\n",
        "def seconds_until_next_tf(tf: str = \"5m\", *, offset_sec: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Segundos hasta el pr\u00f3ximo cierre de vela del timeframe tf (+offset).\n",
        "    Soporta sufijos 'm','h','d'. Ej: '1m','5m','15m','1h','4h','1d'.\n",
        "    \"\"\"\n",
        "    tf = (tf or \"5m\").strip().lower()\n",
        "    if tf.endswith(\"mn\"):\n",
        "        tf = tf[:-2] + \"m\"\n",
        "    try:\n",
        "        if tf.endswith(\"m\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(minutes=max(step, 1))\n",
        "        elif tf.endswith(\"h\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(hours=max(step, 1))\n",
        "        elif tf.endswith(\"d\"):\n",
        "            step = int(tf[:-1])\n",
        "            delta = dt.timedelta(days=max(step, 1))\n",
        "        else:\n",
        "            delta = dt.timedelta(minutes=1)\n",
        "    except Exception:\n",
        "        delta = dt.timedelta(minutes=1)\n",
        "\n",
        "    now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "    epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "    secs = int((now - epoch).total_seconds())\n",
        "    stepS = int(delta.total_seconds())\n",
        "\n",
        "    next_boundary = ((secs // stepS) + 1) * stepS\n",
        "    target = epoch + dt.timedelta(seconds=next_boundary + max(offset_sec, 0))\n",
        "    wait_s = (target - now).total_seconds()\n",
        "    return max(wait_s, 0.5)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SE\u00d1ALES / KALMAN\n",
        "# ============================================================================\n",
        "def generate_trade_signals(\n",
        "    df: pd.DataFrame,\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula 4 l\u00edneas de Kalman sobre 'close' y crea:\n",
        "      \u2022 kal_1, kal_2, kal_3, kal_4\n",
        "      \u2022 Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
        "      \u2022 Close_Trade: -1 si kal_4 < kal_4.shift()  \u2192 cierra BUY\n",
        "                     +1 si kal_4 > kal_4.shift()  \u2192 cierra SELL\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if \"close\" not in df.columns:\n",
        "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
        "\n",
        "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").ffill()\n",
        "\n",
        "    def _clamp_int(x, mn=1):\n",
        "        try:\n",
        "            x = int(x)\n",
        "        except Exception:\n",
        "            x = mn\n",
        "        return max(x, mn)\n",
        "\n",
        "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
        "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
        "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
        "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
        "\n",
        "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
        "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
        "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
        "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
        "\n",
        "    k1_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1)\n",
        "    k2_up = df[\"kal_2\"] > df[\"kal_2\"].shift(1)\n",
        "    k3_up = df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
        "    k1_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1)\n",
        "    k2_dn = df[\"kal_2\"] < df[\"kal_2\"].shift(1)\n",
        "    k3_dn = df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
        "\n",
        "    bull = k1_up & k2_up & k3_up\n",
        "    bear = k1_dn & k2_dn & k3_dn\n",
        "    aux = np.where(bull, 1, np.where(bear, -1, np.nan))\n",
        "    df[\"Open_Trade\"] = np.where(pd.Series(aux).shift(1) != aux, aux, np.nan)\n",
        "\n",
        "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)\n",
        "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)\n",
        "    close_raw = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
        "    close_sr = pd.Series(close_raw)\n",
        "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REST HELPERS (MetaApi REST)\n",
        "# ============================================================================\n",
        "def _rest_place_order(auth_token: str,\n",
        "                      account_id: str,\n",
        "                      region: str,\n",
        "                      symbol: str,\n",
        "                      side: str,\n",
        "                      volume: float,\n",
        "                      comment: str = \"Kal\",\n",
        "                      magic: int | None = None,\n",
        "                      stop_loss: float | None = None,\n",
        "                      take_profit: float | None = None,\n",
        "                      timeout: int = 20):\n",
        "    side = side.upper().strip()\n",
        "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
        "    if side not in action_map:\n",
        "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
        "\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\n",
        "        \"symbol\": symbol,\n",
        "        \"actionType\": action_map[side],\n",
        "        \"volume\": float(volume),\n",
        "        \"comment\": str(comment)\n",
        "    }\n",
        "    if magic is not None:\n",
        "        try:\n",
        "            payload[\"magic\"] = int(magic)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if stop_loss is not None:\n",
        "        try:\n",
        "            payload[\"stopLoss\"] = float(stop_loss)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if take_profit is not None:\n",
        "        try:\n",
        "            payload[\"takeProfit\"] = float(take_profit)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "def _rest_get_positions(auth_token: str,\n",
        "                        account_id: str,\n",
        "                        region: str,\n",
        "                        symbol: str | None = None,\n",
        "                        timeout: int = 15):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
        "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    params = {}\n",
        "    if symbol:\n",
        "        params[\"symbol\"] = str(symbol)\n",
        "    try:\n",
        "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        class _Dummy:\n",
        "            status_code = 0\n",
        "            def json(self): return {\"error\": str(e)}\n",
        "            text = str(e)\n",
        "        return _Dummy()\n",
        "\n",
        "\n",
        "def _rest_modify_position(auth_token: str,\n",
        "                          account_id: str,\n",
        "                          region: str,\n",
        "                          position_id: str,\n",
        "                          stop_loss: float | None = None,\n",
        "                          take_profit: float | None = None,\n",
        "                          timeout: int = 20):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload: Dict[str, Any] = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
        "    if stop_loss is not None:\n",
        "        payload[\"stopLoss\"] = float(stop_loss)\n",
        "    if take_profit is not None:\n",
        "        payload[\"takeProfit\"] = float(take_profit)\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "# --- REST fallback to close a position --------------------------------------\n",
        "def _rest_close_position(auth_token: str,\n",
        "                         account_id: str,\n",
        "                         region: str,\n",
        "                         position_id: str,\n",
        "                         timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Cierra una posici\u00f3n por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    # RPC con timeout corto para evitar cuelgues cuando el subscribe falla\n",
        "    async def _rpc_try(fn, *args, **kwargs):\n",
        "        try:\n",
        "            return await asyncio.wait_for(fn(*args, **kwargs), timeout=4)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    if rpc_conn:\n",
        "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
        "        if not positions:\n",
        "            positions = await _rpc_try(rpc_conn.get_positions)\n",
        "\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: CLOSE / ORDER\n",
        "# ============================================================================\n",
        "\n",
        "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"\u2718 No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt:  return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close_rpc(pid: str) -> bool:\n",
        "        try:\n",
        "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
        "                return True\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid  = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "\n",
        "        ok = await _try_close_rpc(pid)\n",
        "        if not ok:\n",
        "            # REST fallback\n",
        "            r = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
        "            ok = getattr(r, \"status_code\", 0) == 200\n",
        "\n",
        "        if ok:\n",
        "            print(f\"\u2705 Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "        else:\n",
        "            print(f\"\u2718 No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
        "\n",
        "# ============================================================================\n",
        "# OPERATIVA: ABRIR / CERRAR / SINCRONIZAR / SL DIN\u00c1MICO\n",
        "# ============================================================================\n",
        "async def open_trade(df: pd.DataFrame,\n",
        "                     rpc_conn,\n",
        "                     symbol: str = SYMBOL,\n",
        "                     lot: float = 1.0,\n",
        "                     comment: str = \"Kal\",\n",
        "                     magic: int = 900001):\n",
        "    \"\"\"\n",
        "    Abre mercado SOLO si NO hay posici\u00f3n con ese magic.\n",
        "    Al abrir, calcula y env\u00eda el SL inicial:\n",
        "       SL = close \u00b1 ATR * INITIAL_SL  (seg\u00fan BUY/SELL; INITIAL_SL suele ser negativo)\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                return int(pm) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt: return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    open_with_magic = [p for p in positions if _has_magic(p)]\n",
        "\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    row = df.index[-1]\n",
        "\n",
        "    # Ya hay posici\u00f3n con este magic \u2192 sincroniza y sale\n",
        "    if open_with_magic:\n",
        "        p = open_with_magic[0]\n",
        "        df.at[row, \"System_time\"] = now_utc\n",
        "        pm = p.get(\"magic\")\n",
        "        pc = str(p.get(\"comment\") or \"\")\n",
        "        clean_cmt, mag_from_cmt = _split_comment_magic(pc)\n",
        "\n",
        "        if pm is not None and str(pm).strip() != \"\":\n",
        "            try: df.at[row, \"magic\"] = int(pm)\n",
        "            except Exception: df.at[row, \"magic\"] = int(magic)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.at[row, \"magic\"] = int(magic)\n",
        "\n",
        "        df.at[row, \"symbol\"]    = str(p.get(\"symbol\") or symbol)\n",
        "        df.at[row, \"openPrice\"] = float(p.get(\"openPrice\") or p.get(\"price\") or np.nan)\n",
        "        df.at[row, \"comment\"]   = clean_cmt or str(comment)\n",
        "\n",
        "        vol = p.get(\"volume\") or p.get(\"lots\") or None\n",
        "        if vol is not None:\n",
        "            try: df.at[row, \"trade_size\"] = float(vol)\n",
        "            except Exception: pass\n",
        "\n",
        "        side = _side_of(p)\n",
        "        if side: df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
        "        if pd.isna(df.at[row, \"Entry_Date\"]): df.at[row, \"Entry_Date\"] = now_utc\n",
        "\n",
        "        save_csv(df)\n",
        "        print(\"\u2139 Position already open; synced last row and skipped new order.\")\n",
        "        return\n",
        "\n",
        "    # No hay posici\u00f3n \u2192 decidir lado por Open_Trade\n",
        "    sig = df[\"Open_Trade\"].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
        "    if side_req is None:\n",
        "        return\n",
        "\n",
        "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
        "    atr_val = float(df[\"ATR\"].iloc[-1]) if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
        "    atr_mult = 1.0\n",
        "\n",
        "    sl_to_send = tp_to_send = None\n",
        "    if np.isfinite(prev_close) and np.isfinite(atr_val):\n",
        "        dist = atr_val * atr_mult\n",
        "        if side_req == \"BUY\":\n",
        "            sl_to_send = prev_close - dist\n",
        "            tp_to_send = prev_close + dist\n",
        "        else:\n",
        "            sl_to_send = prev_close + dist\n",
        "            tp_to_send = prev_close - dist\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_place_order(\n",
        "            auth_token=META_API_TOKEN,\n",
        "            account_id=ACCOUNT_ID,\n",
        "            region=REGION,\n",
        "            symbol=symbol,\n",
        "            side=side_req,\n",
        "            volume=float(lot),\n",
        "            comment=str(comment),\n",
        "            magic=int(magic),\n",
        "            stop_loss=sl_to_send,\n",
        "            take_profit=tp_to_send,\n",
        "            timeout=20\n",
        "        )\n",
        "    )\n",
        "    if getattr(resp, \"status_code\", 0) != 200:\n",
        "        try: err = resp.json()\n",
        "        except Exception: err = {\"raw\": getattr(resp, \"text\", \"\")[:500]}\n",
        "        print(\"\u2718 Order failed\", getattr(resp, \"status_code\", None), json.dumps(err, indent=2, ensure_ascii=False))\n",
        "        return\n",
        "\n",
        "    data = resp.json()\n",
        "    order_id = str(data.get(\"orderId\") or \"\")\n",
        "    position_id = str(data.get(\"positionId\") or \"\")\n",
        "    if position_id and (sl_to_send is not None or tp_to_send is not None):\n",
        "        resp_mod = await loop.run_in_executor(\n",
        "            None,\n",
        "            lambda: _rest_modify_position(\n",
        "                auth_token=META_API_TOKEN,\n",
        "                account_id=ACCOUNT_ID,\n",
        "                region=REGION,\n",
        "                position_id=position_id,\n",
        "                stop_loss=sl_to_send,\n",
        "                take_profit=tp_to_send,\n",
        "                timeout=15\n",
        "            )\n",
        "        )\n",
        "        if getattr(resp_mod, \"status_code\", 0) != 200:\n",
        "            try:\n",
        "                err_mod = resp_mod.json()\n",
        "            except Exception:\n",
        "                err_mod = {\"raw\": getattr(resp_mod, \"text\", \"\")[:500]}\n",
        "            print(\"\u2718 Modify failed\", getattr(resp_mod, \"status_code\", None), json.dumps(err_mod, indent=2, ensure_ascii=False))\n",
        "\n",
        "    df.at[row, \"System_time\"] = now_utc\n",
        "    df.at[row, \"orderId\"]     = order_id\n",
        "    df.at[row, \"magic\"]       = int(magic)\n",
        "    df.at[row, \"symbol\"]      = symbol\n",
        "    df.at[row, \"comment\"]     = str(comment)\n",
        "    df.at[row, \"Entry_Date\"]  = now_utc\n",
        "    df.at[row, \"trade_size\"]  = float(lot)\n",
        "\n",
        "    if sl_to_send is not None and np.isfinite(sl_to_send):\n",
        "        df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
        "        df.at[row, \"Stop_Loss_atr\"] = float(atr_mult)\n",
        "    if tp_to_send is not None and np.isfinite(tp_to_send):\n",
        "        df.at[row, \"Take_Profit_$\"]   = float(tp_to_send)\n",
        "        df.at[row, \"Take_Profit_atr\"] = float(atr_mult)\n",
        "    # Recuperar openPrice y Type desde API\n",
        "    open_price, fetched_typ = np.nan, None\n",
        "    if rpc_conn:\n",
        "        try:\n",
        "            for _ in range(60):\n",
        "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "                match = None\n",
        "                for p in (pos_list or []):\n",
        "                    pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                    if pid == position_id or pid == order_id or _has_magic(p):\n",
        "                        match = p; break\n",
        "                if match:\n",
        "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
        "                    if val is not None: open_price = float(val)\n",
        "                    t = match.get(\"type\")\n",
        "                    if isinstance(t, str):\n",
        "                        tu = t.upper(); fetched_typ = \"Long\" if \"BUY\" in tu else (\"Short\" if \"SELL\" in tu else None)\n",
        "                    elif t == 0:\n",
        "                        fetched_typ = \"Long\"\n",
        "                    elif t == 1:\n",
        "                        fetched_typ = \"Short\"\n",
        "                    break\n",
        "                await asyncio.sleep(0.5)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not np.isfinite(open_price):\n",
        "        try: open_price = float(df.at[row, \"close\"])\n",
        "        except Exception: open_price = np.nan\n",
        "\n",
        "    df.at[row, \"openPrice\"] = open_price\n",
        "    df.at[row, \"Type\"] = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
        "\n",
        "    save_csv(df)\n",
        "    print(f\"\u2705 {side_req} placed | orderId={order_id} positionId={position_id} \"\n",
        "          f\"openPrice={open_price if np.isfinite(open_price) else 'NaN'} \"\n",
        "          f\"| SL sent={sl_to_send if sl_to_send is not None else 'None'} \"\n",
        "          f\"| TP sent={tp_to_send if tp_to_send is not None else 'None'} \"\n",
        "          f\"| Type={df.at[row,'Type']}\")\n",
        "\n",
        "\n",
        "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Por bloque de trade:\n",
        "      \u2022 FFill de metadatos\n",
        "      \u2022 base_px / atr_base desde la apertura del bloque\n",
        "      \u2022 atr_mult_high/low\n",
        "      \u2022 'profits' solo si est\u00e1 NaN\n",
        "      \u2022 Propaga 'Real_SL' dentro del bloque con el \u00faltimo valor no nulo\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    for c in ('time', 'Entry_Date'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
        "\n",
        "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "        else:\n",
        "            df[c].values[:] = np.nan\n",
        "\n",
        "    if 'profits' not in df.columns:\n",
        "        df['profits'] = np.nan\n",
        "    if 'base_px' not in df.columns:\n",
        "        df['base_px'] = np.nan\n",
        "    if 'atr_base' not in df.columns:\n",
        "        df['atr_base'] = np.nan\n",
        "    if 'Real_SL' not in df.columns:\n",
        "        df['Real_SL'] = np.nan\n",
        "\n",
        "    starts_mask = pd.Series(False, index=df.index)\n",
        "    if 'orderId' in df.columns:\n",
        "        oid = df['orderId']\n",
        "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
        "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
        "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
        "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
        "    if not starts_mask.any():\n",
        "        return df\n",
        "\n",
        "    groups = starts_mask.cumsum()\n",
        "    trade_ids = groups[starts_mask].unique()\n",
        "\n",
        "    meta_cols = [\"orderId\", \"magic\", \"symbol\", \"openPrice\", \"comment\", \"Type\", \"Entry_Date\", \"trade_size\"]\n",
        "\n",
        "    for gid in trade_ids:\n",
        "        mask = (groups == gid)\n",
        "        start_idx = df.index[mask][0]\n",
        "\n",
        "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
        "        try:\n",
        "            base_px = float(base_px)\n",
        "        except Exception:\n",
        "            base_px = np.nan\n",
        "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
        "            try:\n",
        "                base_px = float(df.at[start_idx, 'close'])\n",
        "            except Exception:\n",
        "                base_px = np.nan\n",
        "\n",
        "        atr_base = np.nan\n",
        "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'atr_base'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
        "            try:\n",
        "                atr_base = float(df.at[start_idx, 'ATR'])\n",
        "            except Exception:\n",
        "                atr_base = np.nan\n",
        "\n",
        "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
        "\n",
        "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
        "\n",
        "        if np.isfinite(base_px):\n",
        "            df.loc[mask, 'base_px'] = base_px\n",
        "        if np.isfinite(atr_base):\n",
        "            df.loc[mask, 'atr_base'] = atr_base\n",
        "\n",
        "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long', 'Short'):\n",
        "            if typ == 'Long':\n",
        "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((df.loc[mask, 'low'] - base_px) / atr_base).round(2)\n",
        "            else:\n",
        "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low'] = ((base_px - df.loc[mask, 'low']) / atr_base).round(2)\n",
        "\n",
        "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
        "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long', 'Short'):\n",
        "            m_nan = mask & df['profits'].isna()\n",
        "            if m_nan.any():\n",
        "                if typ == 'Long':\n",
        "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
        "                else:\n",
        "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
        "\n",
        "    # Propaga Real_SL por bloque\n",
        "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
        "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
        "    else:\n",
        "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
        "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
        "\n",
        "    if starts.any():\n",
        "        grp = starts.cumsum()\n",
        "        for gid in grp[starts].unique():\n",
        "            m = (grp == gid)\n",
        "            ser = df.loc[m, 'Real_SL']\n",
        "            if ser.notna().any():\n",
        "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
        "                df.loc[m, 'Real_SL'] = val\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def tick_dyn_atr(df: pd.DataFrame,\n",
        "                 initial_atr: float = INITIAL_SL,\n",
        "                 first_step_atr: float = FIRST_STEP_ATR,\n",
        "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Din\u00e1mica de stop en m\u00faltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
        "      \u2022 Escribe 'tick_dyn_atr' (m\u00faltiplos)\n",
        "      \u2022 Calcula y actualiza 'Stop_Loss_$' y 'Stop_Loss_atr'\n",
        "    \"\"\"\n",
        "    col_name = 'tick_dyn_atr'\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = np.nan\n",
        "    if 'Stop_Loss_$' not in df.columns:\n",
        "        df['Stop_Loss_$'] = np.nan\n",
        "    if 'Stop_Loss_atr' not in df.columns:\n",
        "        df['Stop_Loss_atr'] = np.nan\n",
        "\n",
        "    in_trade = False\n",
        "    trade_active = False\n",
        "    broken = False\n",
        "    sl_val = float(initial_atr)\n",
        "    next_threshold = float(first_step_atr)\n",
        "    prev_sl = float(initial_atr)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        new_open = (\n",
        "            (('orderId' in df.columns) and pd.notna(row.get('orderId'))) or\n",
        "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
        "            (pd.notna(row.get('Entry_Date')))\n",
        "        )\n",
        "\n",
        "        if new_open and not in_trade:\n",
        "            in_trade = True\n",
        "            trade_active = True\n",
        "            broken = False\n",
        "            sl_val = float(initial_atr)\n",
        "            next_threshold = float(first_step_atr)\n",
        "            prev_sl = sl_val\n",
        "            entry_dt = row.get('time')\n",
        "            if entry_dt is not None:\n",
        "                df.at[idx, 'Entry_Date'] = entry_dt\n",
        "\n",
        "        if in_trade:\n",
        "            m_high = row.get('atr_mult_high', np.nan)\n",
        "            m_low = row.get('atr_mult_low', np.nan)\n",
        "            best_pnl = np.nanmax([m_high, m_low])\n",
        "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
        "\n",
        "            if trade_active and not broken:\n",
        "                while best_pnl >= next_threshold:\n",
        "                    sl_val += float(gap_first_step_atr)\n",
        "                    next_threshold += float(gap_first_step_atr)\n",
        "\n",
        "                below_prev = (\n",
        "                    (np.isfinite(m_high) and float(m_high) < prev_sl) or\n",
        "                    (np.isfinite(m_low) and float(m_low) < prev_sl)\n",
        "                )\n",
        "                if below_prev:\n",
        "                    broken = True\n",
        "                    trade_active = False\n",
        "                    in_trade = False\n",
        "\n",
        "            df.at[idx, col_name] = np.nan if broken else sl_val\n",
        "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
        "            prev_sl = sl_val\n",
        "\n",
        "    # Convierte m\u00faltiplos a precio\n",
        "    try:\n",
        "        atr_mult = df[col_name].astype(float)\n",
        "        base = df['base_px'].astype(float)\n",
        "        atr = df['atr_base'].astype(float)\n",
        "        typ = df['Type'].astype('string')\n",
        "\n",
        "        stop_price = np.where(\n",
        "            (typ == 'Long') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "            base + atr * atr_mult,\n",
        "            np.where(\n",
        "                (typ == 'Short') & np.isfinite(atr_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "                base - atr * atr_mult,\n",
        "                np.nan\n",
        "            )\n",
        "        )\n",
        "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    \"\"\"\n",
        "    Cierra posiciones seg\u00fan la pendiente de kal_4 codificada en Close_Trade:\n",
        "      \u2022 Close_Trade == -1  \u2192 cerrar BUY\n",
        "      \u2022 Close_Trade == +1  \u2192 cerrar SELL\n",
        "    \"\"\"\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"\u2718 No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close(pid: str) -> bool:\n",
        "        try:\n",
        "            await rpc_conn.close_position(pid)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await rpc_conn.close_position({\"positionId\": pid})\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"\u2718 Fall\u00f3 cierre positionId={pid}: {e}\")\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "        ok = await _try_close(pid)\n",
        "        if ok:\n",
        "            print(f\"\u2705 Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "\n",
        "\n",
        "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
        "                                 rpc_conn,\n",
        "                                 symbol: str = SYMBOL,\n",
        "                                 magic: int = 900001,\n",
        "                                 tol: float = 0.01) -> None:\n",
        "    \"\"\"\n",
        "    Copia desde la posici\u00f3n viva los campos de mercado al DF.\n",
        "    'profits' solo se escribe en la \u00daLTIMA FILA para no sobreescribir hist\u00f3rico.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt:\n",
        "            return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    pos = next((p for p in positions if _has_magic(p) and _sym_ok(p)), None)\n",
        "    if not pos:\n",
        "        pos = next((p for p in positions if _has_magic(p)), None)\n",
        "    if not pos:\n",
        "        return\n",
        "\n",
        "    order_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    magic_val = pos.get(\"magic\")\n",
        "    symbol_val = pos.get(\"symbol\")\n",
        "    open_price = pos.get(\"openPrice\") or pos.get(\"price\")\n",
        "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\") or None\n",
        "    stop_loss = pos.get(\"stopLoss\")\n",
        "    volume_val = pos.get(\"volume\") or pos.get(\"lots\")\n",
        "    profit_val = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
        "                  else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
        "\n",
        "    t = pos.get(\"type\")\n",
        "    typ = None\n",
        "    if isinstance(t, str):\n",
        "        tu = t.upper()\n",
        "        if \"BUY\" in tu:  typ = \"Long\"\n",
        "        if \"SELL\" in tu: typ = \"Short\"\n",
        "    elif t == 0:\n",
        "        typ = \"Long\"\n",
        "    elif t == 1:\n",
        "        typ = \"Short\"\n",
        "\n",
        "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
        "    clean_cmt, mag_from_cmt = _split_comment_magic(str(comment_raw or \"\"))\n",
        "\n",
        "    block_mask = pd.Series(False, index=df.index)\n",
        "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
        "        block_mask = (df[\"orderId\"] == order_id)\n",
        "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
        "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
        "        starts = ed.notna() & (ed != ed.shift(1))\n",
        "        if starts.any():\n",
        "            last_start = df.index[starts].max()\n",
        "            block_mask = (df.index >= last_start)\n",
        "        else:\n",
        "            block_mask = ed.notna() & (ed >= entry_dt)\n",
        "    if not block_mask.any():\n",
        "        block_mask.iloc[-1] = True\n",
        "\n",
        "    try:\n",
        "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
        "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.loc[block_mask, \"magic\"] = int(magic)\n",
        "\n",
        "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
        "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
        "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
        "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
        "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
        "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
        "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    last_idx = df.index[-1]\n",
        "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
        "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
        "    elif mag_from_cmt is not None:\n",
        "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
        "    else:\n",
        "        df.at[last_idx, \"magic\"] = int(magic)\n",
        "\n",
        "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
        "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
        "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
        "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
        "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
        "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
        "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
        "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
        "\n",
        "\n",
        "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
        "    \"\"\"Devuelve (prev, last) con los dos \u00faltimos valores no-NaN distintos.\"\"\"\n",
        "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        return (np.nan, np.nan)\n",
        "    last = float(s.iloc[-1])\n",
        "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
        "    return (prev, last)\n",
        "\n",
        "\n",
        "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
        "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\"):\n",
        "            if key in p and p[key] is not None:\n",
        "                try:\n",
        "                    if int(p[key]) == int(magic):\n",
        "                        return True\n",
        "                except Exception:\n",
        "                    if str(p[key]).strip() == str(magic):\n",
        "                        return True\n",
        "        if f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    for p in positions:\n",
        "        if _sym_ok(p) and _has_magic(p):\n",
        "            return p\n",
        "    for p in positions:\n",
        "        if _has_magic(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
        "                                     rpc_conn,\n",
        "                                     *,\n",
        "                                     symbol: str,\n",
        "                                     magic: int,\n",
        "                                     auth_token: str,\n",
        "                                     account_id: str,\n",
        "                                     region: str,\n",
        "                                     tol: float = 0.0) -> dict:\n",
        "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
        "    if not np.isfinite(last_sl):\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vac\u00edo\"}\n",
        "\n",
        "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
        "    if not changed:\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
        "\n",
        "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "\n",
        "    if not pos:\n",
        "        last_oid = None\n",
        "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
        "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
        "        if last_oid:\n",
        "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "            for p in positions:\n",
        "                pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                if pid == last_oid:\n",
        "                    pos = p\n",
        "                    break\n",
        "\n",
        "    if not pos:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posici\u00f3n con ese magic\"}\n",
        "\n",
        "    position_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    if not position_id:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vac\u00edo\"}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_modify_position(\n",
        "            auth_token=auth_token,\n",
        "            account_id=account_id,\n",
        "            region=region,\n",
        "            position_id=position_id,\n",
        "            stop_loss=float(last_sl),\n",
        "            timeout=15\n",
        "        )\n",
        "    )\n",
        "    ok = getattr(resp, \"status_code\", 0) == 200\n",
        "    err = None\n",
        "    if not ok:\n",
        "        try:\n",
        "            err = json.dumps(resp.json())[:300]\n",
        "        except Exception:\n",
        "            err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
        "\n",
        "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
        "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
        "\n",
        "\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    try:\n",
        "        positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    if not positions:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions() or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cvhz3M-fTIw",
      "metadata": {
        "id": "5cvhz3M-fTIw"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    \"\"\"\n",
        "    Bucle principal:\n",
        "      \u2022 Crea/migra el CSV inicial (ATR interno, se\u00f1ales Kalman).\n",
        "      \u2022 Cada 5 minutos procesa la \u00faltima vela cerrada y, si corresponde,\n",
        "        abre una operaci\u00f3n con stop-loss y take-profit.\n",
        "    \"\"\"\n",
        "\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    # 0) Conexi\u00f3n MetaApi / RPC\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    # Par\u00e1metros (con defaults tolerantes a faltantes globales)\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    MAGIC    = 900001\n",
        "    LENGTHS  = (300, 520, 710, 1130)\n",
        "    SMOOTHS  = (3, 3, 3, 7)\n",
        "    LOT_     = globals().get(\"LOT\", 0.1)\n",
        "    COMMENT_ = globals().get(\"COMMENT\", \"Kal\")\n",
        "\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    # Helpers locales\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    import datetime as dt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    def _parse_tf_to_delta(tf: str) -> dt.timedelta:\n",
        "        \"\"\"'1m','5m','15m','1h','4h','1d' \u2192 timedelta (fallback 1m).\"\"\"\n",
        "        tf = (tf or \"1m\").strip().lower()\n",
        "        if tf.endswith(\"mn\"):\n",
        "            tf = tf[:-2] + \"m\"\n",
        "        try:\n",
        "            if tf.endswith(\"m\"):\n",
        "                return dt.timedelta(minutes=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"h\"):\n",
        "                return dt.timedelta(hours=max(int(tf[:-1]), 1))\n",
        "            if tf.endswith(\"d\"):\n",
        "                return dt.timedelta(days=max(int(tf[:-1]), 1))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return dt.timedelta(minutes=1)\n",
        "\n",
        "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
        "        \"\"\"Floor de ts a m\u00faltiplo exacto del timeframe (UTC).\"\"\"\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
        "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        secs  = int((ts - epoch).total_seconds())\n",
        "        step  = int(delta.total_seconds()) or 60\n",
        "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
        "\n",
        "    def _calc_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
        "        \"\"\"\n",
        "        ATR estilo Wilder: TR = max(H-L, |H-C1|, |L-C1|), ATR = RMA(TR, period).\n",
        "        Usa ewm(alpha=1/period) como aproximaci\u00f3n de RMA.\n",
        "        \"\"\"\n",
        "        h, l, c = df[\"high\"].astype(float), df[\"low\"].astype(float), df[\"close\"].astype(float)\n",
        "        c1 = c.shift(1)\n",
        "        tr = np.maximum.reduce([\n",
        "            (h - l).to_numpy(),\n",
        "            (h - c1).abs().to_numpy(),\n",
        "            (l - c1).abs().to_numpy()\n",
        "        ])\n",
        "        atr = pd.Series(tr, index=df.index).ewm(alpha=1/period, adjust=False).mean()\n",
        "        return atr.round(4)\n",
        "\n",
        "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
        "        \"\"\"True si existe posici\u00f3n con ese magic (prefiere helper global si existe).\"\"\"\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            return pos is not None\n",
        "        except Exception:\n",
        "            positions = []\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            if not positions:\n",
        "                r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "                if getattr(r, \"status_code\", 0) == 200:\n",
        "                    try:\n",
        "                        positions = r.json() or []\n",
        "                    except Exception:\n",
        "                        positions = []\n",
        "            if not positions:\n",
        "                return False\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if ok:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "    async def _get_api_type(rpc_conn, symbol: str, magic: int):\n",
        "        \"\"\"'Long' / 'Short' / None usando get_pos_with_magic si existe.\"\"\"\n",
        "        side = None\n",
        "        try:\n",
        "            pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "            if not pos:\n",
        "                return None\n",
        "            t = pos.get(\"type\")\n",
        "            if isinstance(t, str):\n",
        "                tu = t.upper()\n",
        "                side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "            elif t == 0:\n",
        "                side = \"BUY\"\n",
        "            elif t == 1:\n",
        "                side = \"SELL\"\n",
        "        except Exception:\n",
        "            try:\n",
        "                positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "            for p in positions:\n",
        "                pm = p.get(\"magic\")\n",
        "                cmt = str(p.get(\"comment\") or \"\")\n",
        "                ok = False\n",
        "                if pm is not None:\n",
        "                    try: ok = int(pm) == int(magic)\n",
        "                    except Exception: ok = False\n",
        "                if (not ok) and f\"magic={magic}\" in cmt:\n",
        "                    ok = True\n",
        "                if not ok:\n",
        "                    continue\n",
        "                t = p.get(\"type\")\n",
        "                if isinstance(t, str):\n",
        "                    tu = t.upper()\n",
        "                    side = \"BUY\" if \"BUY\" in tu else (\"SELL\" if \"SELL\" in tu else None)\n",
        "                elif t == 0:\n",
        "                    side = \"BUY\"\n",
        "                elif t == 1:\n",
        "                    side = \"SELL\"\n",
        "                break\n",
        "        if side is None:\n",
        "            return None\n",
        "        return \"Long\" if side == \"BUY\" else \"Short\"\n",
        "\n",
        "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: str | None) -> None:\n",
        "        \"\"\"Escribe 'Type' en el bloque activo o en la \u00faltima fila si no se detecta bloque.\"\"\"\n",
        "        if not api_type or df_all.empty:\n",
        "            return\n",
        "        last_oid = df_all.get(\"orderId\")\n",
        "        if last_oid is not None and last_oid.notna().any():\n",
        "            last_oid_val = last_oid.dropna().iloc[-1]\n",
        "            mask = (df_all[\"orderId\"] == last_oid_val)\n",
        "        else:\n",
        "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
        "            starts = ed.notna() & (ed != ed.shift(1))\n",
        "            if starts.any():\n",
        "                start_idx = df_all.index[starts].max()\n",
        "                mask = (df_all.index >= start_idx)\n",
        "            else:\n",
        "                mask = pd.Series(False, index=df_all.index)\n",
        "        if mask.any():\n",
        "            df_all.loc[mask, \"Type\"] = api_type\n",
        "        else:\n",
        "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
        "\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    # 1) Crear/migrar archivo inicial\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        df = await get_candles_5m(account, start=None, limit=CANDEL_NUMBER)\n",
        "        if len(df) >= 14:\n",
        "            df[\"ATR\"] = _calc_atr(df, 14)\n",
        "        l1, l2, l3, l4 = LENGTHS\n",
        "        s1, s2, s3, s4 = SMOOTHS\n",
        "        generate_trade_signals(df, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "        _ensure_order_cols(df)\n",
        "        stamp_system_time(df, \"last\")\n",
        "        save_csv(df)\n",
        "        print(f\"\u2714 Archivo inicial creado con {len(df)} velas\")\n",
        "    else:\n",
        "        migrate_csv_if_needed(FILE_PATH)\n",
        "\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    # 2) Loop principal\n",
        "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "    async def _wait_for_closed_candle(prev_bar: dt.datetime,\n",
        "                                      last_known_time: pd.Timestamp | None,\n",
        "                                      delta: dt.timedelta) -> pd.DataFrame:\n",
        "        \"\"\"Obtiene la \u00faltima vela cerrada <= prev_bar reintentando durante un margen de seguridad.\"\"\"\n",
        "        wait_limit = max(dt.timedelta(seconds=45), delta)\n",
        "        wait_limit = min(wait_limit, dt.timedelta(minutes=5))\n",
        "        deadline = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc) + wait_limit\n",
        "        poll_sleep = max(3, min(20, int(delta.total_seconds() // 6) or 1))\n",
        "        warned = False\n",
        "        latest_candidate: pd.DataFrame | None = None\n",
        "        last_snapshot = pd.DataFrame()\n",
        "        while True:\n",
        "            fresh = await get_candles_5m(account, start=None, limit=50)\n",
        "            fresh = (fresh[fresh[\"time\"] <= prev_bar]\n",
        "                     .drop_duplicates(\"time\")\n",
        "                     .sort_values(\"time\"))\n",
        "            last_snapshot = fresh.copy()\n",
        "            if not fresh.empty:\n",
        "                latest_candidate = fresh.iloc[[-1]].copy()\n",
        "                latest_time = pd.to_datetime(latest_candidate[\"time\"], utc=True, errors=\"coerce\")\n",
        "                latest_time = latest_time.iloc[-1] if not latest_time.empty else None\n",
        "                if last_known_time is None or (latest_time is not None and latest_time > last_known_time):\n",
        "                    return latest_candidate\n",
        "                if (not warned and last_known_time is not None\n",
        "                        and latest_time is not None and latest_time <= last_known_time):\n",
        "                    ts_txt = prev_bar.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    print(f\"\u26a0\ufe0f Vela cerrada {ts_txt} a\u00fan no publicada; reintentando...\", flush=True)\n",
        "                    warned = True\n",
        "            if dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc) >= deadline:\n",
        "                if latest_candidate is not None:\n",
        "                    return latest_candidate\n",
        "                return last_snapshot\n",
        "            await asyncio.sleep(poll_sleep)\n",
        "\n",
        "    reconnect_backoff = 5.0\n",
        "    reconnect_backoff_max = 60.0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            await asyncio.sleep(seconds_until_next_tf(time_frame_data, offset_sec=3))\n",
        "\n",
        "            now_utc  = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            delta    = _parse_tf_to_delta(time_frame_data)\n",
        "            this_bar = _floor_to_frame(now_utc, delta)\n",
        "            prev_bar = this_bar - delta  # \u00faltima vela CERRADA\n",
        "\n",
        "            df_all = _load_csv()\n",
        "            last_known_time = None\n",
        "            if not df_all.empty and \"time\" in df_all.columns:\n",
        "                known_times = pd.to_datetime(df_all[\"time\"], utc=True, errors=\"coerce\").dropna()\n",
        "                if not known_times.empty:\n",
        "                    last_known_time = known_times.iloc[-1]\n",
        "\n",
        "            df_new = await _wait_for_closed_candle(prev_bar, last_known_time, delta)\n",
        "\n",
        "            prev_bar_txt = prev_bar.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            latest_new_time = None\n",
        "            if not df_new.empty and \"time\" in df_new.columns:\n",
        "                latest_times = pd.to_datetime(df_new[\"time\"], utc=True, errors=\"coerce\").dropna()\n",
        "                if not latest_times.empty:\n",
        "                    latest_new_time = latest_times.iloc[-1]\n",
        "            if last_known_time is not None and (latest_new_time is None or latest_new_time <= last_known_time):\n",
        "                print(f\"\u26a0\ufe0f Sin nueva vela cerrada para {prev_bar_txt} UTC; se reintentar\u00e1 en el siguiente ciclo.\")\n",
        "\n",
        "            existing_times = (\n",
        "                set(pd.to_datetime(df_all[\"time\"], utc=True))\n",
        "                if (not df_all.empty and \"time\" in df_all.columns)\n",
        "                else set()\n",
        "            )\n",
        "\n",
        "            if df_all.empty:\n",
        "                df_all = df_new.copy()\n",
        "            else:\n",
        "                df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
        "                          .drop_duplicates(\"time\")\n",
        "                          .sort_values(\"time\")\n",
        "                          .reset_index(drop=True))\n",
        "\n",
        "            if len(df_all) >= 14:\n",
        "                df_all[\"ATR\"] = _calc_atr(df_all, 14)\n",
        "\n",
        "            l1, l2, l3, l4 = LENGTHS\n",
        "            s1, s2, s3, s4 = SMOOTHS\n",
        "            generate_trade_signals(df_all, l1, l2, l3, l4, s1, s2, s3, s4)\n",
        "            _ensure_order_cols(df_all)\n",
        "\n",
        "            if not df_new.empty:\n",
        "                new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
        "                if new_times:\n",
        "                    df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
        "\n",
        "            await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT_, comment=COMMENT_, magic=MAGIC)\n",
        "\n",
        "            api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
        "            _sync_type_in_df(df_all, api_type)\n",
        "            await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "            stamp_system_time(df_all, \"last\")\n",
        "            save_csv(df_all)\n",
        "            print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"), \"| actualizaci\u00f3n (ciclo \"+time_frame_data+\")\")\n",
        "\n",
        "            reconnect_backoff = 5.0\n",
        "        except asyncio.CancelledError:\n",
        "            raise\n",
        "        except Exception as loop_err:\n",
        "            logging.exception(\"Error en ciclo principal MetaApi: %s\", loop_err)\n",
        "            print(f\"\u26a0\ufe0f Error en ciclo principal: {loop_err}\", flush=True)\n",
        "            await asyncio.sleep(reconnect_backoff)\n",
        "            reconnect_backoff = min(reconnect_backoff * 2, reconnect_backoff_max)\n",
        "            try:\n",
        "                account = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "                rpc_conn = account.get_rpc_connection()\n",
        "                try:\n",
        "                    await rpc_conn.connect()\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    await asyncio.wait_for(rpc_conn.wait_synchronized(), timeout=30)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                reconnect_backoff = 5.0\n",
        "                print(\"\ud83d\udd04 Reconexi\u00f3n MetaApi completada.\", flush=True)\n",
        "            except Exception as recon_err:\n",
        "                logging.exception(\"Fallo reconectando a MetaApi: %s\", recon_err)\n",
        "                print(f\"\u274c Fall\u00f3 la reconexi\u00f3n a MetaApi: {recon_err}\", flush=True)\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W481PrVufZ8Z",
      "metadata": {
        "id": "W481PrVufZ8Z"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yI6v2q_xfRtg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI6v2q_xfRtg",
        "outputId": "250fed3a-009b-404c-e2b9-2f498c4e555f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2714 Archivo inicial creado con 900 velas\n",
            "\u26a0\ufe0f Vela cerrada 2025-10-07 15:37:00 a\u00fan no publicada; reintentando...\n",
            "\u26a0\ufe0f Sin nueva vela cerrada para 2025-10-07 15:37:00 UTC; se reintentar\u00e1 en el siguiente ciclo.\n",
            "15:39:11 | actualizaci\u00f3n (ciclo 1m)\n",
            "15:40:06 | actualizaci\u00f3n (ciclo 1m)\n",
            "15:41:04 | actualizaci\u00f3n (ciclo 1m)\n",
            "[2025-10-07T15:41:12.132157] london:0: MetaApi websocket client disconnected from the MetaApi server\n",
            "[2025-10-07T15:41:13.133978] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-a.agiliumtrade.ai shared server.\n",
            "[2025-10-07T15:41:18.469456] london:1: MetaApi websocket client disconnected from the MetaApi server\n",
            "[2025-10-07T15:41:19.470585] Connecting MetaApi websocket client to the MetaApi server via https://mt-client-api-v1.london-b.agiliumtrade.ai shared server.\n"
          ]
        }
      ],
      "source": [
        "###############################################################################\n",
        "# EJECUCI\u00d3N\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c05e894"
      },
      "source": [
        "# Filter the DataFrame to include only the last 200 rows for plotting\n",
        "df_plot = df.iloc[500:751].copy()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(df_plot['time'], df_plot['close'], label='Close', color='black', linewidth=1)\n",
        "plt.plot(df_plot['time'], df_plot['kal_1'], label='Kal_1', color='blue', linestyle='--')\n",
        "plt.plot(df_plot['time'], df_plot['kal_2'], label='Kal_2', color='red', linestyle='--')\n",
        "plt.plot(df_plot['time'], df_plot['kal_3'], label='Kal_3', color='green', linestyle='--')\n",
        "\n",
        "# Add vertical lines for trade signals\n",
        "for index, row in df_plot.iterrows():\n",
        "    if row['Open_Trade'] == 1:\n",
        "        plt.axvline(row['time'], color='blue', linestyle='-', linewidth=1, alpha=0.7, label='Buy Signal' if row['time'] == df_plot['time'].iloc[0] else \"\")\n",
        "    elif row['Open_Trade'] == -1:\n",
        "        plt.axvline(row['time'], color='red', linestyle='-', linewidth=1, alpha=0.7, label='Sell Signal' if row['time'] == df_plot['time'].iloc[0] else \"\")\n",
        "\n",
        "plt.title('Close Price and Kalman Lines with Trade Signals')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "8c05e894",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
