{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf83164c",
      "metadata": {
        "id": "bf83164c"
      },
      "source": [
        "# Pendientes\n",
        "\n",
        "Nada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ysa-7eLvEMpE",
      "metadata": {
        "id": "Ysa-7eLvEMpE"
      },
      "source": [
        "# Gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9GJAW8qAEM8f",
      "metadata": {
        "id": "9GJAW8qAEM8f"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FcIdAmrNERw-",
      "metadata": {
        "id": "FcIdAmrNERw-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "id": "EB5RqjoAtFwl"
      },
      "outputs": [],
      "source": [
        "symbol = \"BTCUSD\"\n",
        "root_data = f'/content/drive/MyDrive/Course Folder/Forex/XAUUSD/'\n",
        "print(root_data)\n",
        "\n",
        "direction = 'Short'\n",
        "direction_number = -1\n",
        "\n",
        "symbol = 'BTCUSD'\n",
        "strategy = 'Kalman'\n",
        "time_frame = 'M5'\n",
        "\n",
        "trade_evolution = 'st_Max'\n",
        "result_field = 'st_PnL'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y6QRdBKwrX98",
      "metadata": {
        "id": "y6QRdBKwrX98"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R9bTNmBwK_Up",
      "metadata": {
        "id": "R9bTNmBwK_Up"
      },
      "outputs": [],
      "source": [
        "!pip install ta-lib\n",
        "import talib as ta\n",
        "print(ta.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import joblib\n",
        "import math\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Tuple, List, Optional, Dict, Any\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import (roc_auc_score, f1_score, accuracy_score, log_loss, r2_score)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KFfn45ty82qv",
      "metadata": {
        "id": "KFfn45ty82qv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zhTndYVi1TEV",
      "metadata": {
        "id": "zhTndYVi1TEV"
      },
      "source": [
        "# Support Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kp4yJdGAjoeA",
      "metadata": {
        "id": "kp4yJdGAjoeA"
      },
      "outputs": [],
      "source": [
        "def kalman_line(source, kalman_length: int, smooth: int):\n",
        "\n",
        "    n = len(source)\n",
        "    kf_c = np.empty(n)            # núcleo del filtro\n",
        "    velo_c = np.zeros(n)          # componente de velocidad\n",
        "\n",
        "    sqrt_term   = np.sqrt(kalman_length / 10000.0 * 2.0)\n",
        "    length_term = kalman_length / 10000.0\n",
        "\n",
        "    # --------- inicialización (mismo efecto que `var` en Pine) ----------\n",
        "    kf_c[0]   = source.iloc[0]    # nz(kf_c[1], source) para la primera barra\n",
        "    velo_c[0] = 0.0\n",
        "\n",
        "    # ------------------- bucle recursivo -------------------------------\n",
        "    for i in range(1, n):\n",
        "        prev_kf = kf_c[i-1] if not np.isnan(kf_c[i-1]) else source.iloc[i]\n",
        "        dk      = source.iloc[i] - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term          # parte \"suave\"\n",
        "        velo_c[i] = velo_c[i-1] + length_term * dk   # acumulamos velocidad\n",
        "        kf_c[i]   = smooth_c + velo_c[i]             # estimación final\n",
        "\n",
        "    # -------------------- EMA final (ta.ema) ----------------------------\n",
        "    kf_c_series = pd.Series(kf_c, index=source.index)\n",
        "    kalman_line = kf_c_series.ewm(span=smooth, adjust=False).mean()\n",
        "    return kalman_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HahbLzEkjoeB",
      "metadata": {
        "id": "HahbLzEkjoeB"
      },
      "outputs": [],
      "source": [
        "def slope(src: pd.Series,\n",
        "          length_kal: int,\n",
        "          smooth_kal: int,\n",
        "          slopeLen: int,\n",
        "          offset: int) -> pd.DataFrame:\n",
        "\n",
        "    n = len(src)\n",
        "    kf_state = np.full(n, np.nan)\n",
        "    kf_velo  = np.zeros(n)\n",
        "    sqrt_factor = np.sqrt(length_kal / 10000.0 * 2.0)\n",
        "    vel_factor  = length_kal / 10000.0\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == 0:\n",
        "            prev_state = src.iloc[0]\n",
        "            prev_velo  = 0.0\n",
        "        else:\n",
        "            prev_state = kf_state[i-1] if not np.isnan(kf_state[i-1]) else src.iloc[i]\n",
        "            prev_velo  = kf_velo[i-1]\n",
        "\n",
        "        dk = src.iloc[i] - prev_state\n",
        "        smooth = prev_state + dk * sqrt_factor\n",
        "        kf_velo[i]  = prev_velo + vel_factor * dk\n",
        "        kf_state[i] = smooth + kf_velo[i]\n",
        "\n",
        "    # 2) EMA smoothing --------------------------------------------------\n",
        "    kal = pd.Series(kf_state, index=src.index).ewm(span=smooth_kal, adjust=False).mean()\n",
        "\n",
        "    # 3) Slope/divergence -----------------------------------------------\n",
        "    validLen = max(slopeLen, 1)\n",
        "    slope_div = kal.diff(validLen) / validLen\n",
        "    slope_signal = (slope_div > slope_div.shift(1)).astype(int)\n",
        "\n",
        "    # 4) Angle in degrees -----------------------------------------------\n",
        "    price_change = kal - kal.shift(validLen)\n",
        "    slope_angle = np.degrees(np.arctan(price_change))\n",
        "    slope_angle_signal = (slope_angle > slope_angle.shift(1)).astype(int)\n",
        "\n",
        "    # 5) Linear regression prediction ----------------------------------\n",
        "    def _linreg(y):\n",
        "        x = np.arange(len(y))\n",
        "        m, b = np.polyfit(x, y, 1)\n",
        "        return b + m * (len(y)-1)\n",
        "\n",
        "    slope_lin_reg = kal.rolling(window=slopeLen).apply(_linreg, raw=False)\n",
        "    slope_lin_reg = slope_lin_reg.shift(-offset)  # apply Pine-style offset\n",
        "    slope_lin_reg_signal = (slope_lin_reg > slope_lin_reg.shift(1)).astype(int)\n",
        "\n",
        "    # 6) Pack results ---------------------------------------------------\n",
        "    return pd.DataFrame({\n",
        "        'slope_div':            slope_div,\n",
        "        'slope_signal':         slope_signal,\n",
        "        'slope_angle':          slope_angle,\n",
        "        'slope_angle_signal':   slope_angle_signal,\n",
        "        'slope_lin_reg':        slope_lin_reg,\n",
        "        'slope_lin_reg_signal': slope_lin_reg_signal\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fB8uDm2F1TS1",
      "metadata": {
        "id": "fB8uDm2F1TS1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def scale_features_data(features):\n",
        "    # Standardise the input data (X)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Split the dataset into training and testing sets as 80:20\n",
        "    train_data = features.iloc[:(int(len(features) * 0.8))]\n",
        "    test_data = features.iloc[(int(len(features) * 0.8)):]\n",
        "\n",
        "    # Scale the training and testing sets\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(\n",
        "        train_data), columns=features.columns, index=train_data.index)\n",
        "    X_test = pd.DataFrame(data=scaler.transform(test_data),\n",
        "                          columns=features.columns, index=test_data.index)\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "# Function for creating target variable\n",
        "def target_var(data, window_size=20):\n",
        "    target = pd.DataFrame(index=data.index)\n",
        "    target['signal'] = data.Close.pct_change(window_size).shift(-window_size)\n",
        "\n",
        "    target.dropna(inplace=True)\n",
        "    target['signal'] = np.where(target['signal'] > 0, 1, -1)\n",
        "    return target\n",
        "\n",
        "# Function to split and scale the data\n",
        "def train_test_split(features, target, split_proportion=0.8):\n",
        "\n",
        "    split_index = int(len(features) * split_proportion)\n",
        "\n",
        "    X_train = features.iloc[:split_index]\n",
        "    X_test = features.iloc[split_index:]\n",
        "    y_train = target.iloc[:split_index]\n",
        "    y_test = target.iloc[split_index:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trade_analytics(trades):\n",
        "    analytics = pd.DataFrame(index=['Strategy'])\n",
        "    analytics['Total PnL'] = round(trades.PnL.sum(),2)\n",
        "    analytics['Total Trades'] = len(trades.loc[trades.Position!=0])\n",
        "    analytics['Number of Winners'] = len(trades.loc[trades.PnL>0])\n",
        "    analytics['Number of Losers'] = len(trades.loc[trades.PnL<=0])\n",
        "    analytics['Win (%)'] = round(100*analytics['Number of Winners']/analytics['Total Trades'],2)\n",
        "    analytics['Loss (%)'] = round(100*analytics['Number of Losers']/analytics['Total Trades'],2)\n",
        "    analytics['Average Profit of Winning Trade'] = round(trades.loc[trades.PnL>0].PnL.mean(),2)\n",
        "    analytics['Average Loss of Losing Trade'] = round(np.abs(trades.loc[trades.PnL<=0].PnL.mean()),2)\n",
        "    trades['Entry Date'] = pd.to_datetime(trades['Entry Date'])\n",
        "    trades['Exit Date'] = pd.to_datetime(trades['Exit Date'])\n",
        "    holding_period = trades['Exit Date'] - trades['Entry Date']\n",
        "    analytics['Average Holding Time'] = holding_period.mean()\n",
        "    analytics['Profit Factor'] = round((analytics['Win (%)']/100*analytics['Average Profit of Winning Trade'])/(analytics['Loss (%)']/100*analytics['Average Loss of Losing Trade']),2)\n",
        "    return analytics.T\n",
        "\n",
        "def performance_metrics(data):\n",
        "    data.set_index('Date', inplace=True)\n",
        "    performance_metrics = pd.DataFrame(index=['Strategy'])\n",
        "    data['Strategy Returns'] = data.signal.shift(1) * data.Close.pct_change()\n",
        "    data['Cumulative Returns'] = (data['Strategy Returns'] + 1.0).cumprod()\n",
        "    data['Cumulative Benchmark Returns'] = (data['Close'].pct_change() +1).cumprod()\n",
        "    data['Cumulative Returns'].plot(figsize=(15, 7), label='Strategy Returns')\n",
        "    data['Cumulative Benchmark Returns'].plot(label='Benchmark Returns')\n",
        "    plt.title('Equity Curve', fontsize=14)\n",
        "    plt.ylabel('Cumulative Returns', fontsize = 12)\n",
        "    plt.xlabel('Date', fontsize = 12)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    days = len(data['Cumulative Returns'])\n",
        "    performance_metrics['CAGR'] = \"{0:.2f}%\".format(\n",
        "        (data['Cumulative Returns'].iloc[-1]**(252/days)-1)*100)\n",
        "    performance_metrics['Annualised Volatility'] = \"{0:.2f}%\".format(\n",
        "        data['Strategy Returns'].std()*np.sqrt(252) * 100)\n",
        "    risk_free_rate = 0.02/252\n",
        "    performance_metrics['Sharpe Ratio'] = round(np.sqrt(252)*(np.mean(data['Strategy Returns']) -\n",
        "                                                        (risk_free_rate))/np.std(data['Strategy Returns']),2)\n",
        "    data['Peak'] = data['Cumulative Returns'].cummax()\n",
        "    data['Drawdown'] = ((data['Cumulative Returns'] - data['Peak'])/data['Peak'])\n",
        "    performance_metrics['Maximum Drawdown'] =  \"{0:.2f}%\".format((data['Drawdown'].min())*100)\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title('Drawdowns', fontsize=14)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.plot(data['Drawdown'], color='red')\n",
        "    plt.fill_between(data['Drawdown'].index, data['Drawdown'].values, color='red')\n",
        "    plt.show()\n",
        "    print(performance_metrics.T)"
      ],
      "metadata": {
        "id": "pKONFRdirWse"
      },
      "id": "pKONFRdirWse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_signals(X_test, aapl_test_prices_ts, model, scaler):\n",
        "    # Initialise current position\n",
        "    current_pos = 0\n",
        "\n",
        "    # Initialise count of holding days\n",
        "    hold_days = 0\n",
        "\n",
        "    # Iterate through the rows of test data\n",
        "    for dt, row in X_test.iterrows():\n",
        "        # Check if there is no position or holding period reaches 20 days\n",
        "        if current_pos == 0 or hold_days == 20:\n",
        "            # Prepare test data for prediction\n",
        "            test = pd.DataFrame(data=scaler.transform(\n",
        "                row.values.reshape(1, -1)), columns=X_test.columns)\n",
        "\n",
        "            # Generate signal based on test data\n",
        "            signal = model.predict(test)[-1]\n",
        "\n",
        "            # Update current position\n",
        "            current_pos = signal\n",
        "\n",
        "            # Update predicted and actual labels for the current date\n",
        "            aapl_test_prices_ts.loc[dt, 'signal'] = current_pos\n",
        "\n",
        "            # Reset holding days counter\n",
        "            hold_days = 0\n",
        "        elif current_pos != 0:\n",
        "            # If there is an existing position, increment holding days counter\n",
        "            hold_days += 1\n",
        "\n",
        "    # Forward fill the last observed value for 'y_pred'\n",
        "    aapl_test_prices_ts['signal'].ffill(inplace=True)\n",
        "\n",
        "    return aapl_test_prices_ts"
      ],
      "metadata": {
        "id": "MJ7UIHnlrSE2"
      },
      "id": "MJ7UIHnlrSE2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_portfolio_returns_csmom(monthly_returns, portfolio='long-short', lookback_months=12):\n",
        "    stock_monthly_returns = pd.DataFrame()\n",
        "\n",
        "    # Loop through each month after the lookback period\n",
        "    for i in range(lookback_months, len(monthly_returns)):\n",
        "        # Select the subset of monthly returns for the current lookback period\n",
        "        # Store the historical and holding monthly returns data and store in 'returns'\n",
        "        returns = monthly_returns[i - lookback_months:i + 1]\n",
        "\n",
        "        # Store the historical in 'trailing_returns'\n",
        "        trailing_returns = returns[:lookback_months]\n",
        "\n",
        "        # Extract the starting, ending, and holding months from the subset\n",
        "        starting_month = str(returns.index[0])[:7]\n",
        "        ending_month = str(returns.index[-2])[:7]\n",
        "        holding_month = str(returns.index[-1])[:7]\n",
        "\n",
        "        # Set returns data as the transposed scaled trailing returns\n",
        "        returns_data = trailing_returns.T\n",
        "\n",
        "        # Initialize the number of clusters and maximum number of stocks per cluster\n",
        "        num_clusters = 1\n",
        "        max_stocks_per_cluster = 10\n",
        "\n",
        "        # Perform hierarchical clustering using 'ward' linkage method\n",
        "        linkage_matrix = linkage(trailing_returns.T, method='ward')\n",
        "\n",
        "        # Assign cluster labels to stocks, ensuring each cluster has at most 10 stocks\n",
        "        clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "        # Assign the cluster labels to the original returns data\n",
        "        returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Adjust clusters until each cluster meets the constraint\n",
        "        while max(returns_data['Cluster'].value_counts()) > max_stocks_per_cluster:\n",
        "            num_clusters += 1\n",
        "            clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "            returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Define the minimum number of stocks in a cluster\n",
        "        minimum_stocks_in_cluster = 2\n",
        "\n",
        "        # Filter out clusters with fewer than the minimum number of stocks\n",
        "        filtered_clusters = returns_data.groupby('Cluster').filter(\n",
        "            lambda x: len(x) >= minimum_stocks_in_cluster)['Cluster'].unique()\n",
        "\n",
        "        # Assign the filtered cluster labels to the original price data\n",
        "        returns_data = returns_data[returns_data['Cluster'].isin(filtered_clusters)]\n",
        "\n",
        "        # Calculate the returns for each cluster and sum across clusters\n",
        "        cluster_returns = returns_data.groupby('Cluster').mean().sum(axis=1)\n",
        "\n",
        "        if portfolio == 'long-short':\n",
        "            # Identify stocks to go short and long based on cluster returns\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long and short\n",
        "            long_returns = hold_returns[long].mean()\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long and short portfolios for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long) + list(short)][holding_month]\n",
        "\n",
        "            # Adjust returns for short positions\n",
        "            monthly_portfolio_returns[short] *= -1\n",
        "\n",
        "        elif portfolio == 'long':\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long\n",
        "            long_returns = hold_returns[long].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long)][holding_month]\n",
        "\n",
        "        elif portfolio == 'short':\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go short\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the short portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                short)][holding_month]\n",
        "\n",
        "        # Append adjusted returns for the holding month to the stock_monthly_returns dataframe\n",
        "        stock_monthly_returns = stock_monthly_returns.append(monthly_portfolio_returns)\n",
        "\n",
        "    return stock_monthly_returns\n",
        "\n",
        "def plot_and_display_metrics_csmom(stock_monthly_returns):\n",
        "    portfolio_returns = stock_monthly_returns.mean(axis=1)\n",
        "    fig, ax = plt.subplots(figsize=(15, 7))\n",
        "    portfolio_returns.plot(ax=ax)\n",
        "\n",
        "    # Set the title and axis labels\n",
        "    ax.set_title('Portfolio Returns Over Time')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Returns')\n",
        "    ax.axhline(y=0, color='black', linestyle='-')\n",
        "\n",
        "    # Fill area below 0 with red color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns < 0, color='red', alpha=0.3)\n",
        "\n",
        "    # Fill area above 0 with green color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns >= 0, color='green', alpha=0.3)\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Calculate cumulative returns\n",
        "    cumulative_returns = (portfolio_returns + 1).cumprod()\n",
        "\n",
        "    # Convert index to datetime format\n",
        "    cumulative_returns.index = pd.to_datetime(cumulative_returns.index)\n",
        "\n",
        "    # Plot cumulative returns\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    cumulative_returns.plot()\n",
        "\n",
        "    # Labeling axes and title\n",
        "    plt.ylabel('Cumulative Returns', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Returns', fontsize=14)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate maximum cumulative returns up to each point\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "\n",
        "    # Calculate drawdown\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "\n",
        "    # Plot drawdown\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    # Fill area under the drawdown curve with red color\n",
        "    plt.fill_between(drawdown.index, drawdown, 0, color='red', alpha=0.3)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Drawdown', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # Display the metrics\n",
        "    # Calculate monthly Sharpe ratio\n",
        "    monthly_sharpe = portfolio_returns.mean() / portfolio_returns.std()\n",
        "\n",
        "    # Annualize Sharpe ratio for monthly data\n",
        "    sharpe = round(monthly_sharpe * ((12) ** 0.5),2)\n",
        "\n",
        "    # Calculate drawdown\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "    max_drawdown_index = drawdown.idxmin()\n",
        "    max_drawdown_date = max_drawdown_index.strftime('%Y-%m-%d')\n",
        "    max_drawdown_value = round(drawdown.min(),2)\n",
        "\n",
        "    # Create a DataFrame to hold the metrics\n",
        "    metrics = pd.DataFrame({\n",
        "        'Metric': ['Sharpe Ratio', 'Maximum Drawdown Date', 'Maximum Drawdown Value'],\n",
        "        'Value': [sharpe, max_drawdown_date, max_drawdown_value]\n",
        "    })\n",
        "\n",
        "    # Display metrics\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    display(metrics.rename_axis(None, axis=1))"
      ],
      "metadata": {
        "id": "7PeSUJ_prINM"
      },
      "id": "7PeSUJ_prINM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9aa65d54",
      "metadata": {
        "id": "9aa65d54"
      },
      "source": [
        "# Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I23127P7lBq_",
      "metadata": {
        "id": "I23127P7lBq_"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dunlv-TDqxB8",
      "metadata": {
        "id": "Dunlv-TDqxB8"
      },
      "outputs": [],
      "source": [
        "def get_parkinson(price_data, window=10, trading_periods=50, clean=True):\n",
        "    rs = (1.0 / (4.0 * math.log(2.0))) * ((price_data['High'] / price_data['Low']).apply(np.log))**2.0\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RLxsuLKaqxE3",
      "metadata": {
        "id": "RLxsuLKaqxE3"
      },
      "outputs": [],
      "source": [
        "def get_HodgesTompkins(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    vol = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).std() * math.sqrt(trading_periods)\n",
        "\n",
        "    h = window\n",
        "    n = (log_return.count() - h) + 1\n",
        "\n",
        "    adj_factor = 1.0 / (1.0 - (h / n) + ((h**2 - 1) / (3 * n**2)))\n",
        "\n",
        "    result = vol * adj_factor\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFCu8J7hqxHi",
      "metadata": {
        "id": "qFCu8J7hqxHi"
      },
      "outputs": [],
      "source": [
        "def get_skew(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).skew()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YnVDjmtDqxKU",
      "metadata": {
        "id": "YnVDjmtDqxKU"
      },
      "outputs": [],
      "source": [
        "def get_kurtosis(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).kurt()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WJvLEk7EqxM6",
      "metadata": {
        "id": "WJvLEk7EqxM6"
      },
      "outputs": [],
      "source": [
        "def get_YangZhang(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High'] / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low'] / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    log_oc = (price_data['Open'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_oc_sq = log_oc**2\n",
        "\n",
        "    log_cc = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_cc_sq = log_cc**2\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    close_vol = log_cc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    open_vol = log_oc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    window_rs = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "\n",
        "    k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
        "    result = (open_vol + k * close_vol + (1 - k) * window_rs).apply(np.sqrt) * math.sqrt(trading_periods)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OHmFUyfGqxPL",
      "metadata": {
        "id": "OHmFUyfGqxPL"
      },
      "outputs": [],
      "source": [
        "def get_RogersSatchell(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High']  / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low']   / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cSq3oNjkrdhT",
      "metadata": {
        "id": "cSq3oNjkrdhT"
      },
      "outputs": [],
      "source": [
        "def get_GermanKlass(price_data, window=22, trading_periods=50, clean=True):\n",
        "\n",
        "    log_hl = (price_data['High'] / price_data['Low']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = 0.5 * log_hl**2 - (2*math.log(2)-1) * log_co**2\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(window=window, center=False).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZnVB7nObuG_e",
      "metadata": {
        "id": "ZnVB7nObuG_e"
      },
      "outputs": [],
      "source": [
        "def create_features(stock_data: pd.DataFrame) -> pd.DataFrame:\n",
        "    kalman_periods = [100, 300, 500, 700, 900]\n",
        "    slope_length   = [3, 6, 9, 12, 15, 18, 21]\n",
        "\n",
        "    features = pd.DataFrame(index=stock_data.index)\n",
        "\n",
        "    # ── Kalman y derivados ─────────────────────────────────────────────\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Kalman and Derivatives\"):\n",
        "        kal = pd.Series(kalman_line(stock_data['Close'], kalman_length=period, smooth=3),\n",
        "                        index=stock_data.index)\n",
        "\n",
        "        features[f'Kal_{period}']        = kal\n",
        "        features[f'Close_Kal_{period}']  = stock_data['Close'] - kal\n",
        "        features[f'Kal_change_{period}'] = kal.diff()\n",
        "\n",
        "    # ── Slopes ─────────────────────────────────────────────────────────\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Slopes\"):\n",
        "        for sLen in slope_length:\n",
        "            df_s = slope(stock_data['Close'], length_kal=period, smooth_kal=3,\n",
        "                         slopeLen=sLen, offset=-1)\n",
        "\n",
        "            features[f'slope_div_{period}_{sLen}']             = df_s['slope_div']\n",
        "            features[f'slope_signal_{period}_{sLen}']          = df_s['slope_signal']\n",
        "            features[f'slope_angle_{period}_{sLen}']           = df_s['slope_angle']\n",
        "            features[f'slope_angle_signal_{period}_{sLen}']    = df_s['slope_angle_signal']\n",
        "            features[f'slope_lin_reg_{period}_{sLen}']         = df_s['slope_lin_reg']\n",
        "            features[f'slope_lin_reg_signal_{period}_{sLen}']  = df_s['slope_lin_reg_signal']\n",
        "\n",
        "    features.dropna(inplace=True)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dexDJat0XWBi",
      "metadata": {
        "id": "dexDJat0XWBi"
      },
      "source": [
        "## 1_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KiPN3qSWXWBj",
      "metadata": {
        "id": "KiPN3qSWXWBj"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df_1min = pd.read_csv(root_data + 'Data/'+symbol+'_M1.csv', index_col=0)\n",
        "df_1min.index = pd.to_datetime(df_1min.index)\n",
        "df_1min = df_1min.iloc[-500000:,]\n",
        "\n",
        "print('Min_Date : ', df_1min.index.min())\n",
        "print('Min_Date : ', df_1min.index.max())\n",
        "print('Number_Rows = ',len(df_1min.index))\n",
        "print('\\n')\n",
        "\n",
        "df_1min.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V4zWjMGtXWBk",
      "metadata": {
        "id": "V4zWjMGtXWBk"
      },
      "source": [
        "**Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DmkzrcRjXWBm",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DmkzrcRjXWBm"
      },
      "outputs": [],
      "source": [
        "features = create_features(df_1min)\n",
        "features = features.dropna()\n",
        "print(\"Number of features are:\", features.shape[1])\n",
        "print(features.shape)\n",
        "features.to_csv(root_data+'Results/'+symbol+'_'+direction+'_M1_Raw_Features.csv')\n",
        "features.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S3Iv9f1ZXWBm",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S3Iv9f1ZXWBm"
      },
      "outputs": [],
      "source": [
        "#for col in features.columns:\n",
        " # print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yGHSgJnYXWBn",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yGHSgJnYXWBn"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv(root_data+'Results/'+symbol+'_M1_Raw_Features.csv')\n",
        "features[\"Date\"] = pd.to_datetime(features[\"Date\"])\n",
        "print(features.shape)\n",
        "features.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VRhYUP2RXWBn",
      "metadata": {
        "id": "VRhYUP2RXWBn"
      },
      "source": [
        "**Scale_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kqXbXgJWXWBn",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqXbXgJWXWBn"
      },
      "outputs": [],
      "source": [
        "cols_to_scale = features.columns[1:]\n",
        "cols_to_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j6kGfSpyXWBo",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j6kGfSpyXWBo"
      },
      "outputs": [],
      "source": [
        "### Scale Features with Rolling Window\n",
        "window = 200\n",
        "rolling = features[cols_to_scale].rolling(window)\n",
        "features[cols_to_scale] = (features[cols_to_scale] - rolling.mean()) / rolling.std()\n",
        "\n",
        "print(\"DataFrame with rolling-scaled features:\")\n",
        "print(features.shape)\n",
        "features.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IXiZG3OpXWBo",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IXiZG3OpXWBo"
      },
      "outputs": [],
      "source": [
        "features.to_csv(root_data+'Results/'+symbol+'_M1_Scale_Features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DoqqPJxPxnBF",
      "metadata": {
        "id": "DoqqPJxPxnBF"
      },
      "source": [
        "## 5_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7l7vt7QxvyC",
      "metadata": {
        "id": "d7l7vt7QxvyC"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df_5min = pd.read_csv(root_data + 'Data/'+symbol+'_M5.csv', index_col=0)\n",
        "df_5min.index = pd.to_datetime(df_5min.index)\n",
        "df_5min = df_5min.iloc[-1000000:,]\n",
        "\n",
        "print('Min_Date : ', df_5min.index.min())\n",
        "print('Min_Date : ', df_5min.index.max())\n",
        "print('Number_Rows = ',len(df_5min.index))\n",
        "print('\\n')\n",
        "\n",
        "df_5min.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HBicVZcDx2CF",
      "metadata": {
        "id": "HBicVZcDx2CF"
      },
      "source": [
        "**Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XxU4wllsEUcN",
      "metadata": {
        "id": "XxU4wllsEUcN"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Create the Features Set\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "features = create_features(df_5min)\n",
        "features = features.dropna()\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(\"Number of features are:\", features.shape[1])\n",
        "print(features.shape)\n",
        "features.to_csv(root_data+'Results/'+symbol+'DRL_M5_Raw_Features.csv')\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "features.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NqrNWh5O9-1P",
      "metadata": {
        "id": "NqrNWh5O9-1P"
      },
      "outputs": [],
      "source": [
        "for col in features.columns:\n",
        "  print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j4AwNJic6icv",
      "metadata": {
        "id": "j4AwNJic6icv"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv(root_data+'Results/'+symbol+'_M5_Raw_Features.csv')\n",
        "features[\"Date\"] = pd.to_datetime(features[\"Date\"])\n",
        "print(features.shape)\n",
        "features.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QX4cSq3Ftqhm",
      "metadata": {
        "id": "QX4cSq3Ftqhm"
      },
      "source": [
        "**Scale_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1qEb1Cz1vzj6",
      "metadata": {
        "id": "1qEb1Cz1vzj6"
      },
      "outputs": [],
      "source": [
        "cols_to_scale = features.columns[1:]\n",
        "cols_to_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJcGUESttqhm",
      "metadata": {
        "id": "eJcGUESttqhm"
      },
      "outputs": [],
      "source": [
        "### Scale Features with Rolling Window\n",
        "start_time = time.time()\n",
        "\n",
        "window = 200\n",
        "rolling = features[cols_to_scale].rolling(window)\n",
        "features[cols_to_scale] = (features[cols_to_scale] - rolling.mean()) / rolling.std()\n",
        "\n",
        "print(\"DataFrame with rolling-scaled features:\")\n",
        "print(features.shape)\n",
        "features.head()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aXMGezVftqhn",
      "metadata": {
        "id": "aXMGezVftqhn"
      },
      "outputs": [],
      "source": [
        "features.to_csv(root_data+'Results/'+symbol+'DRL_M5_Scale_Features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jh9-mi26wsya",
      "metadata": {
        "id": "jh9-mi26wsya"
      },
      "source": [
        "## 10_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bgh3Xz5ux75A",
      "metadata": {
        "id": "bgh3Xz5ux75A"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df_10min = pd.read_csv(root_data + 'Data/'+symbol+'_M10.csv', index_col=0)\n",
        "df_10min.index = pd.to_datetime(df_10min.index)\n",
        "df_10min = df_10min.iloc[-100000:,]\n",
        "\n",
        "print('Min_Date : ', df_10min.index.min())\n",
        "print('Min_Date : ', df_10min.index.max())\n",
        "print('Number_Rows = ',len(df_10min.index))\n",
        "print('\\n')\n",
        "\n",
        "df_10min.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WZRt7qxmx75A",
      "metadata": {
        "id": "WZRt7qxmx75A"
      },
      "source": [
        "**Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Enk8RNZ3x75B",
      "metadata": {
        "id": "Enk8RNZ3x75B"
      },
      "outputs": [],
      "source": [
        "### Create the Features Set\n",
        "features_10min = create_features(df_10min)\n",
        "features_10min = features_10min.dropna()\n",
        "\n",
        "# Add \"10min_\" prefix to all column names except 'Date'\n",
        "features_10min = features_10min.add_prefix('10min_')\n",
        "features_10min.rename(columns={'10min_Date': 'Date'}, inplace=True)\n",
        "\n",
        "print(\"Number of features are:\", features.shape[1])\n",
        "print(features_10min.shape)\n",
        "features_10min.to_csv(root_data+'Results/'+symbol+'_M10_Raw_Features.csv')\n",
        "features_10min.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZREQYp1Px75B",
      "metadata": {
        "id": "ZREQYp1Px75B"
      },
      "outputs": [],
      "source": [
        "for col in features_10min.columns:\n",
        "  #print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F7wu9XEqx75B",
      "metadata": {
        "id": "F7wu9XEqx75B"
      },
      "outputs": [],
      "source": [
        "features_10min = pd.read_csv(root_data+'Results/'+symbol+'_M10_Raw_Features.csv')\n",
        "features_10min[\"Date\"] = pd.to_datetime(features_10min[\"Date\"])\n",
        "print(features_10min.shape)\n",
        "features_10min.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OM1OXL0tx75B",
      "metadata": {
        "id": "OM1OXL0tx75B"
      },
      "source": [
        "**Scale_features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_ktgzyhx75C",
      "metadata": {
        "id": "D_ktgzyhx75C"
      },
      "outputs": [],
      "source": [
        "cols_to_scale = features_10min.columns[1:]\n",
        "cols_to_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qeSWEQdTx75D",
      "metadata": {
        "id": "qeSWEQdTx75D"
      },
      "outputs": [],
      "source": [
        "### Scale Features with Rolling Window\n",
        "window = 200\n",
        "rolling = features_10min[cols_to_scale].rolling(window)\n",
        "features_10min[cols_to_scale] = (features_10min[cols_to_scale] - rolling.mean()) / rolling.std()\n",
        "\n",
        "print(\"DataFrame with rolling-scaled features:\")\n",
        "print(features_10min.shape)\n",
        "features_10min.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RAYGNhPex75E",
      "metadata": {
        "id": "RAYGNhPex75E"
      },
      "outputs": [],
      "source": [
        "features_10min.to_csv(root_data+'Results/'+symbol+'_M10_Scale_Features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A5c__w6s_dtY",
      "metadata": {
        "id": "A5c__w6s_dtY"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4j7J8tk_f3o-",
      "metadata": {
        "id": "4j7J8tk_f3o-"
      },
      "source": [
        "## Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6XOsumPycYz3",
      "metadata": {
        "id": "6XOsumPycYz3"
      },
      "outputs": [],
      "source": [
        "lab = pd.read_csv(root_data + 'Results/'+symbol+'_'+strategy+'_'+time_frame+'_Strategy_Gen_Labels.csv', index_col=0)\n",
        "lab['Date'] = pd.to_datetime(lab['Date'])\n",
        "\n",
        "print('Min_Date    : ',lab['Date'].min())\n",
        "print('Min_Date    : ',lab['Date'].max(),'\\n')\n",
        "print('Number_Rows : ',lab.shape,'\\n')\n",
        "print('Columns     : ',lab.columns)\n",
        "\n",
        "lab['Open_Trade'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FnEPev7hemCD",
      "metadata": {
        "id": "FnEPev7hemCD"
      },
      "outputs": [],
      "source": [
        "st_max_1 = lab.loc[((lab[result_field] > 0) & (lab['st_Max'] > 0)),'st_atr_max_PnL'].describe()\n",
        "st_max_2 = lab.loc[((lab[result_field] < 0) & (lab['st_Max'] > 0)),'st_atr_max_PnL'].describe()\n",
        "st_max_3 = lab.loc[((lab[result_field] < 0) & (lab['st_Max'] <= 0)),'st_atr_PnL'].describe()\n",
        "st_max_4 = lab.loc[((lab[result_field] < 0)),'st_atr_PnL'].describe()\n",
        "\n",
        "combined_descriptions = pd.concat([st_max_1, st_max_2, st_max_3, st_max_4], axis=1)\n",
        "combined_descriptions.columns = ['st_max_1', 'st_max_2', 'st_max_3', 'st_max_4']\n",
        "\n",
        "print(\"--- Descriptive Statistics when Open_Trade is -1 (Side-by-Side) ---\")\n",
        "display(combined_descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "apV2tezPsbux",
      "metadata": {
        "id": "apV2tezPsbux"
      },
      "outputs": [],
      "source": [
        "#analyse_column = 'st_atr_max_PnL'\n",
        "analyse_column = 'st_Max'\n",
        "\n",
        "st_max_0  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
        "st_max_1  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
        "st_max_2  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].sum()\n",
        "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].sum()\n",
        "\n",
        "\n",
        "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 0)),analyse_column].sum()\n",
        "\n",
        "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                   (lab['st_atr_max_PnL'] > 0.7) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].sum()\n",
        "\n",
        "st_max_5 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                   (lab['st_atr_max_PnL'] > 1) &\n",
        "                   (lab['st_atr_max_PnL'] <= 1.5)),analyse_column].sum()\n",
        "\n",
        "st_max_6 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                   (lab['st_atr_max_PnL'] > 1.5) &\n",
        "                   (lab['st_atr_max_PnL'] <= 2)),analyse_column].sum()\n",
        "\n",
        "st_max_7 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                   (lab['st_atr_max_PnL'] > 2)),analyse_column].sum()\n",
        "\n",
        "\n",
        "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
        "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
        "print(f'Above_Mean = {st_max_2:,.2f}')\n",
        "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
        "\n",
        "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
        "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
        "print(f'> 1 & <= 1.5 = {st_max_5:,.2f}')\n",
        "print(f'> 1.5 & <= 2 = {st_max_6:,.2f}')\n",
        "print(f'> 2 = {st_max_7:,.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p2fjs4Si792v",
      "metadata": {
        "id": "p2fjs4Si792v"
      },
      "outputs": [],
      "source": [
        "analyse_column = 'st_atr_max_PnL'\n",
        "\n",
        "st_max_0 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
        "st_max_1 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
        "st_max_2 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].count()\n",
        "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].count()\n",
        "\n",
        "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                    (lab['st_atr_max_PnL'] <= 0.5)),analyse_column].count()\n",
        "\n",
        "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                     (lab['st_atr_max_PnL'] >= 0.5) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].count()\n",
        "\n",
        "st_max_5 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
        "                   (lab['st_atr_max_PnL'] >= 1), analyse_column].count()\n",
        "\n",
        "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
        "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
        "print(f'Above_Mean = {st_max_2:,.2f}')\n",
        "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
        "\n",
        "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
        "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
        "print(f'> 1 = {st_max_5:,.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85DATjQqZd66",
      "metadata": {
        "id": "85DATjQqZd66"
      },
      "outputs": [],
      "source": [
        "# --- Parámetros / campos\n",
        "result_field = 'st_atr_max_PnL'\n",
        "\n",
        "valid = (\n",
        "    (lab['Type'] == direction) &\n",
        "    (lab['Open_Trade'].isin([1, -1])) &\n",
        "    (lab[result_field].notna())\n",
        ")\n",
        "\n",
        "# --- Conteos por rango (st_max_4..6)\n",
        "st_max_4 = (valid & (lab[result_field] <= 0.5)).sum()\n",
        "st_max_5 = (valid & (lab[result_field] >= 0.5) & (lab[result_field] <= 1.0)).sum()\n",
        "st_max_6 = (valid & (lab[result_field] > 1.0)).sum()\n",
        "\n",
        "print(f'<= 0.5          = {st_max_4:,d}')\n",
        "print(f'> 0.5 & <= 1.0  = {st_max_5:,d}')\n",
        "print(f'> 1.0           = {st_max_6:,d}')\n",
        "\n",
        "# --- Etiquetado en la columna \"label\" con valores 4/5/6\n",
        "lab['label'] = np.nan\n",
        "lab.loc[valid & (lab[result_field] <= 0.5), 'label'] = 0\n",
        "lab.loc[valid & (lab[result_field] > 0.5) & (lab[result_field] <= 1.0), 'label'] = 1\n",
        "lab.loc[valid & (lab[result_field] > 1.0), 'label'] = 2\n",
        "\n",
        "# --- Mantener solo filas válidas y con label\n",
        "lab = lab.loc[valid & lab['label'].notna()].copy()\n",
        "lab['label'] = lab['label'].astype('int8')\n",
        "\n",
        "# --- Ver distribución de labels 4/5/6\n",
        "print('\\nValue counts de label 4/5/6:')\n",
        "print(lab['label'].value_counts(dropna=False).sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h_lfP35ttuiG",
      "metadata": {
        "id": "h_lfP35ttuiG"
      },
      "outputs": [],
      "source": [
        "lab['expected'] = np.where(lab['label']==0, -100,\n",
        "                           np.where(lab['label']==1, 50,\n",
        "                                    np.where(lab['label']==2,lab[result_field],np.nan)))\n",
        "lab['expected'].sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_BCdqt3Y0Fjm",
      "metadata": {
        "id": "_BCdqt3Y0Fjm"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1I6_IHtGgbE"
      },
      "outputs": [],
      "source": [
        "raw_feat_1min = pd.read_csv(root_data+'Results/'+symbol+'_M1_Raw_Features.csv')\n",
        "raw_feat_1min[\"Date\"] = pd.to_datetime(raw_feat_1min[\"Date\"])\n",
        "print(raw_feat_1min.shape)\n",
        "#raw_feat_1min.head(5)"
      ],
      "id": "u1I6_IHtGgbE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jK6LavOGgbF"
      },
      "outputs": [],
      "source": [
        "scale_feat_1min = pd.read_csv(root_data+'Results/'+symbol+'_M1_Scale_Features.csv')\n",
        "scale_feat_1min = scale_feat_1min.drop('Unnamed: 0', axis=1)\n",
        "scale_feat_1min[\"Date\"] = pd.to_datetime(scale_feat_1min[\"Date\"])\n",
        "print(scale_feat_1min.shape)\n",
        "#scale_feat_5min.head(5)"
      ],
      "id": "8jK6LavOGgbF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gX04ZTmfz-pK",
      "metadata": {
        "id": "gX04ZTmfz-pK"
      },
      "outputs": [],
      "source": [
        "raw_feat_5min = pd.read_csv(root_data+'Results/'+symbol+'_M5_Raw_Features.csv')\n",
        "raw_feat_5min[\"Date\"] = pd.to_datetime(raw_feat_5min[\"Date\"])\n",
        "print(raw_feat_5min.shape)\n",
        "#raw_feat_5min.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5gxf0ke0KUx",
      "metadata": {
        "id": "D5gxf0ke0KUx"
      },
      "outputs": [],
      "source": [
        "scale_feat_5min = pd.read_csv(root_data+'Results/'+symbol+'_M5_Scale_Features.csv')\n",
        "scale_feat_5min = scale_feat_5min.drop('Unnamed: 0', axis=1)\n",
        "scale_feat_5min[\"Date\"] = pd.to_datetime(scale_feat_5min[\"Date\"])\n",
        "print(scale_feat_5min.shape)\n",
        "#scale_feat_5min.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cHlD0eRb0scj",
      "metadata": {
        "id": "cHlD0eRb0scj"
      },
      "outputs": [],
      "source": [
        "raw_feat_10min = pd.read_csv(root_data+'Results/'+symbol+'_M10_Raw_Features.csv')\n",
        "raw_feat_10min[\"Date\"] = pd.to_datetime(raw_feat_10min[\"Date\"])\n",
        "\n",
        "# Add \"10min_\" prefix to all column names except 'Date'\n",
        "cols_to_prefix = [col for col in raw_feat_10min.columns if col != 'Date']\n",
        "raw_feat_10min.rename(columns={col: col for col in cols_to_prefix}, inplace=True)\n",
        "\n",
        "print(raw_feat_10min.shape)\n",
        "#raw_feat_10min.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O7xrmOA90sck",
      "metadata": {
        "id": "O7xrmOA90sck"
      },
      "outputs": [],
      "source": [
        "scale_feat_10min = pd.read_csv(root_data+'Results/'+symbol+'_M10_Scale_Features.csv')\n",
        "scale_feat_10min = scale_feat_10min.drop('Unnamed: 0', axis=1)\n",
        "scale_feat_10min[\"Date\"] = pd.to_datetime(scale_feat_10min[\"Date\"])\n",
        "\n",
        "# Add \"10min_\" prefix to all column names except 'Date'\n",
        "cols_to_prefix = [col for col in scale_feat_10min.columns if col != 'Date']\n",
        "scale_feat_10min.rename(columns={col:col for col in cols_to_prefix}, inplace=True)\n",
        "\n",
        "print(scale_feat_10min.shape)\n",
        "#scale_feat_10min.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hv5bsrpc03z7",
      "metadata": {
        "id": "Hv5bsrpc03z7"
      },
      "source": [
        "## Merge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_type = 'Scale'"
      ],
      "metadata": {
        "id": "Mc1y2NbRM_f1"
      },
      "id": "Mc1y2NbRM_f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I0gkNVT60Ka5",
      "metadata": {
        "id": "I0gkNVT60Ka5"
      },
      "outputs": [],
      "source": [
        "# First merge scale_feat_5min with scale_feat_10min and apply forward fill\n",
        "merged_features = scale_feat_5min.merge(scale_feat_10min, on='Date', how='left').ffill()\n",
        "#merged_features = raw_feat_5min.merge(raw_feat_10min, on='Date', how='left').ffill()\n",
        "\n",
        "# Then merge with the 'label' column from lab using the lab rows and 'Date' as key\n",
        "df = pd.merge(lab[['Date', 'label']], merged_features, on='Date', how='left')\n",
        "\n",
        "cols = df.columns.tolist()\n",
        "cols.remove('label')\n",
        "cols.insert(1, 'label')\n",
        "df = df[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jXcXWty506Ur",
      "metadata": {
        "id": "jXcXWty506Ur"
      },
      "outputs": [],
      "source": [
        "print(df.columns, '\\n')\n",
        "print(df.shape,'\\n')\n",
        "print('Label_Counts : ',df.label.value_counts(),'\\n')\n",
        "#df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lBtQecOO08zB",
      "metadata": {
        "id": "lBtQecOO08zB"
      },
      "source": [
        "## ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8rlkmypd9DJj",
      "metadata": {
        "id": "8rlkmypd9DJj"
      },
      "outputs": [],
      "source": [
        "# ===================== 1. ENTRENAR Y OBTENER IMPORTANCIAS =====================\n",
        "def compute_xgb_importance(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    task: str = \"classification\",\n",
        "    random_state: int = 42,\n",
        "    **xgb_params: Any\n",
        ") -> Tuple[pd.DataFrame, Any]:\n",
        "    \"\"\"\n",
        "    Entrena un modelo XGBoost y devuelve:\n",
        "      - imp_df: DataFrame con 'feature', 'importance' y 'cum_importance'.\n",
        "      - model : modelo ya entrenado.\n",
        "\n",
        "    Soporta:\n",
        "      • Clasificación binaria o multiclase (detecta nº de clases).\n",
        "      • Regresión (si task != 'classification').\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "        Matriz de características (sin la columna objetivo).\n",
        "    y : pd.Series\n",
        "        Etiquetas objetivo. Puede ser binaria (0/1) o multiclase (0..K-1).\n",
        "    task : str, opcional\n",
        "        \"classification\" (default) o \"regression\".\n",
        "    random_state : int, opcional\n",
        "        Semilla para reproducibilidad.\n",
        "    **xgb_params : dict\n",
        "        Parámetros adicionales para el estimador de XGBoost.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (imp_df, model)\n",
        "        imp_df : DataFrame con importancias y su acumulado.\n",
        "        model  : instancia entrenada de XGBClassifier / XGBRegressor.\n",
        "    \"\"\"\n",
        "    default_params: Dict[str, Any] = dict(\n",
        "        n_estimators=500,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        tree_method=\"hist\",\n",
        "    )\n",
        "    default_params.update(xgb_params)\n",
        "\n",
        "    if task == \"classification\":\n",
        "        # Detectar nº de clases\n",
        "        classes = np.unique(y)\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        # XGBClassifier ajusta objetivo automáticamente, pero lo explicitamos:\n",
        "        if n_classes > 2:\n",
        "            default_params.setdefault(\"objective\", \"multi:softprob\")\n",
        "            default_params.setdefault(\"num_class\", n_classes)\n",
        "            eval_metric = \"mlogloss\"\n",
        "        else:\n",
        "            default_params.setdefault(\"objective\", \"binary:logistic\")\n",
        "            eval_metric = \"logloss\"\n",
        "\n",
        "        model = XGBClassifier(eval_metric=eval_metric, **default_params)\n",
        "\n",
        "    else:\n",
        "        model = XGBRegressor(**default_params)\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\n",
        "            \"feature\": X.columns,\n",
        "            \"importance\": model.feature_importances_\n",
        "        })\n",
        "        .sort_values(\"importance\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    total_imp = imp_df[\"importance\"].sum()\n",
        "    if total_imp == 0:\n",
        "        # Evitar división por cero si el modelo devuelve todo cero (raro, pero posible)\n",
        "        imp_df[\"cum_importance\"] = 0.0\n",
        "    else:\n",
        "        imp_df[\"cum_importance\"] = imp_df[\"importance\"].cumsum() / total_imp\n",
        "\n",
        "    return imp_df, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3r9Di4Xx9DOU",
      "metadata": {
        "id": "3r9Di4Xx9DOU"
      },
      "outputs": [],
      "source": [
        "# ===================== 2. SELECCIÓN DE FEATURES =====================\n",
        "def select_features_with_importance(\n",
        "    X: pd.DataFrame,\n",
        "    imp_df: pd.DataFrame,\n",
        "    top_n: Optional[int] = None,\n",
        "    threshold: Optional[str | float] = None,\n",
        "    cum_threshold: Optional[float] = 0.8\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Selección flexible de variables a partir de importancias de XGBoost.\n",
        "\n",
        "    Reglas:\n",
        "      - Si top_n no es None           => usa el top_n.\n",
        "      - Else si cum_threshold no None => usa importancia acumulada (p.ej. 0.8 = 80%).\n",
        "      - Else usa threshold ('median', 'mean' o valor numérico).\n",
        "\n",
        "    Devuelve (X_reducido, lista_de_features).\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "        Matriz de características original.\n",
        "    imp_df : pd.DataFrame\n",
        "        DataFrame devuelto por compute_xgb_importance.\n",
        "    top_n : int | None\n",
        "        Número fijo de variables a conservar.\n",
        "    threshold : str | float | None\n",
        "        Umbral de importancia. Si str, usar 'median' o 'mean'.\n",
        "    cum_threshold : float | None\n",
        "        Porcentaje acumulado de importancia (0-1). Si None, se ignora.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (X_sel, keep)\n",
        "        X_sel : subset de X con columnas seleccionadas.\n",
        "        keep  : lista de nombres de columnas seleccionadas.\n",
        "    \"\"\"\n",
        "    if top_n is not None:\n",
        "        keep = imp_df.head(top_n)[\"feature\"].tolist()\n",
        "\n",
        "    elif cum_threshold is not None:\n",
        "        keep_mask = imp_df[\"cum_importance\"] <= float(cum_threshold)\n",
        "        keep = imp_df.loc[keep_mask, \"feature\"].tolist()\n",
        "        # asegurar que haya al menos una más para no quedarnos exactamente en el corte\n",
        "        if len(keep) < len(imp_df):\n",
        "            keep.append(imp_df.iloc[len(keep)][\"feature\"])\n",
        "\n",
        "    else:\n",
        "        if threshold is None:\n",
        "            threshold = \"median\"\n",
        "        if isinstance(threshold, str):\n",
        "            thr_val = imp_df[\"importance\"].agg(threshold)\n",
        "        else:\n",
        "            thr_val = float(threshold)\n",
        "        keep = imp_df.loc[imp_df[\"importance\"] >= thr_val, \"feature\"].tolist()\n",
        "\n",
        "    return X[keep], keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tCDZCTz2_v9z",
      "metadata": {
        "id": "tCDZCTz2_v9z"
      },
      "outputs": [],
      "source": [
        "# ===================== 3. BÚSQUEDA DEL MEJOR UMBRAL ACUMULADO =====================\n",
        "def find_best_cum_threshold(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_valid: pd.DataFrame,\n",
        "    y_valid: pd.Series,\n",
        "    task: str = \"classification\",\n",
        "    thresholds: Tuple[float, ...] = (0.6, 0.7, 0.8, 0.9),\n",
        "    random_state: int = 42,\n",
        "    metric: str = \"auto\",\n",
        "    **xgb_params: Any\n",
        ") -> Tuple[float, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Entrena un XGB en train, calcula importancias y prueba varios umbrales\n",
        "    acumulados para ver cuál da la mejor métrica en valid.\n",
        "\n",
        "    Para CLASIFICACIÓN:\n",
        "        - Detecta nº de clases.\n",
        "        - Métrica por defecto (metric=\"auto\"):\n",
        "            • Binaria: ROC-AUC (probabilidades de la clase positiva).\n",
        "            • Multiclase: ROC-AUC macro OVR (usa predict_proba).\n",
        "          Alternativas: metric=\"f1_macro\", \"accuracy\", \"logloss\" (se MINIMIZA).\n",
        "    Para REGRESIÓN:\n",
        "        - Usa R^2.\n",
        "\n",
        "    Devuelve:\n",
        "        best_thr, res_df_ordenado_por_score_desc, imp_df\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    X_train, y_train, X_valid, y_valid : pd.DataFrame / pd.Series\n",
        "        Particiones de entrenamiento y validación.\n",
        "    task : str\n",
        "        \"classification\" (default) o \"regression\".\n",
        "    thresholds : tuple[float, ...]\n",
        "        Valores de umbral de importancia acumulada a evaluar (0-1).\n",
        "    random_state : int\n",
        "        Semilla para reproducibilidad.\n",
        "    metric : str\n",
        "        \"auto\" (default), \"roc_auc\", \"f1_macro\", \"accuracy\", \"logloss\" (clasif) o \"r2\" (regresión).\n",
        "    **xgb_params : dict\n",
        "        Parámetros extra para el estimador de XGBoost (pasan a compute y a los modelos internos).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (best_thr, res_df, imp_df)\n",
        "        best_thr : float\n",
        "            Umbral con mejor score (o menor logloss si metric='logloss').\n",
        "        res_df : pd.DataFrame\n",
        "            Tabla con resultados por umbral (n_features, score).\n",
        "        imp_df : pd.DataFrame\n",
        "            Importancias calculadas en X_train / y_train.\n",
        "    \"\"\"\n",
        "    imp_df, _ = compute_xgb_importance(\n",
        "        X_train, y_train, task=task, random_state=random_state, **xgb_params\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Detectar nº de clases si es clasificación\n",
        "    if task == \"classification\":\n",
        "        classes = np.unique(y_train)\n",
        "        n_classes = len(classes)\n",
        "        if metric == \"auto\":\n",
        "            metric_to_use = \"roc_auc\" if n_classes == 2 else \"roc_auc\"\n",
        "        else:\n",
        "            metric_to_use = metric\n",
        "    else:\n",
        "        metric_to_use = \"r2\" if metric == \"auto\" else metric\n",
        "\n",
        "    for thr in thresholds:\n",
        "        X_tr_sel, cols = select_features_with_importance(\n",
        "            X_train, imp_df, cum_threshold=thr, top_n=None, threshold=None\n",
        "        )\n",
        "        X_va_sel = X_valid[cols]\n",
        "\n",
        "        if task == \"classification\":\n",
        "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
        "            params.update(xgb_params)\n",
        "\n",
        "            if n_classes > 2:\n",
        "                params.setdefault(\"objective\", \"multi:softprob\")\n",
        "                params.setdefault(\"num_class\", n_classes)\n",
        "                eval_metric = \"mlogloss\"\n",
        "            else:\n",
        "                params.setdefault(\"objective\", \"binary:logistic\")\n",
        "                eval_metric = \"logloss\"\n",
        "\n",
        "            model_sel = XGBClassifier(eval_metric=eval_metric, **params)\n",
        "            model_sel.fit(X_tr_sel, y_train)\n",
        "\n",
        "            # Probabilidades y predicciones\n",
        "            proba = model_sel.predict_proba(X_va_sel)\n",
        "            pred  = np.argmax(proba, axis=1) if n_classes > 2 else (proba[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "            # Calcular métrica\n",
        "            if metric_to_use == \"roc_auc\":\n",
        "                if n_classes == 2:\n",
        "                    score = roc_auc_score(y_valid, proba[:, 1])\n",
        "                else:\n",
        "                    # AUC macro One-vs-Rest\n",
        "                    score = roc_auc_score(y_valid, proba, multi_class=\"ovr\", average=\"macro\")\n",
        "            elif metric_to_use == \"f1_macro\":\n",
        "                score = f1_score(y_valid, pred, average=\"macro\")\n",
        "            elif metric_to_use == \"accuracy\":\n",
        "                score = accuracy_score(y_valid, pred)\n",
        "            elif metric_to_use == \"logloss\":\n",
        "                # En este caso, menor es mejor. Guardamos negativo para mantener criterio \"mayor mejor\".\n",
        "                score = -log_loss(y_valid, proba, labels=np.unique(y_train))\n",
        "            else:\n",
        "                raise ValueError(f\"Métrica no soportada: {metric_to_use}\")\n",
        "\n",
        "        else:\n",
        "            # REGRESIÓN\n",
        "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
        "            params.update(xgb_params)\n",
        "            model_sel = XGBRegressor(**params)\n",
        "            model_sel.fit(X_tr_sel, y_train)\n",
        "            pred = model_sel.predict(X_va_sel)\n",
        "\n",
        "            if metric_to_use == \"r2\":\n",
        "                score = r2_score(y_valid, pred)\n",
        "            else:\n",
        "                raise ValueError(f\"Métrica de regresión no soportada: {metric_to_use}\")\n",
        "\n",
        "        results.append({\"cum_threshold\": thr, \"n_features\": len(cols), \"score\": score})\n",
        "\n",
        "    # Ordenar (si usamos logloss negado, mayor sigue siendo mejor)\n",
        "    res_df = pd.DataFrame(results).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
        "    best_thr = float(res_df.iloc[0][\"cum_threshold\"])\n",
        "    return best_thr, res_df, imp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WodcQEBJ_wAW",
      "metadata": {
        "id": "WodcQEBJ_wAW"
      },
      "outputs": [],
      "source": [
        "# ===================== 3. PIPELINE PRINCIPAL =====================\n",
        "df = df.dropna()\n",
        "y = df[\"label\"]\n",
        "X = df.iloc[:, 2:]\n",
        "\n",
        "# --- 3.3 Split temporal (ejemplo simple 80/20) ---\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "# --- 3.4 Importancias con XGBoost ---\n",
        "imp_df, xgb_model = compute_xgb_importance(X_train, y_train, task=\"classification\")\n",
        "\n",
        "print(\"=== Importancias XGBoost ===\")\n",
        "print(imp_df.head(20))\n",
        "print(f\"Total features: {len(imp_df)}\")\n",
        "\n",
        "# --- 3.5 Selección (elige una opción) ---\n",
        "X_train_sel, keep_cols = select_features_with_importance(X_train, imp_df, cum_threshold=0.8)\n",
        "X_test_sel = X_test[keep_cols]\n",
        "\n",
        "print(f\"Features seleccionadas: {len(keep_cols)}\")\n",
        "pd.Series(keep_cols).to_csv(root_data+'Results/'+symbol+'_'+direction+'_M5M10_'+data_type+'_ImportantCols.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FZNBrkDHgfNk",
      "metadata": {
        "id": "FZNBrkDHgfNk"
      },
      "outputs": [],
      "source": [
        "keep_cols.insert(0, 'Date')\n",
        "#keep_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KxL6YRcB-Mu4",
      "metadata": {
        "id": "KxL6YRcB-Mu4"
      },
      "outputs": [],
      "source": [
        "df = df.loc[:,keep_cols]\n",
        "#df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(root_data+'Results/'+symbol+'_'+direction+'_M5M10_'+data_type+'_Features.csv')"
      ],
      "metadata": {
        "id": "oP7E7UEQGNxh"
      },
      "id": "oP7E7UEQGNxh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2RFfhHT2AwAJ",
      "metadata": {
        "id": "2RFfhHT2AwAJ"
      },
      "source": [
        "# Encode_Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jRl6BZWUZ-WI",
      "metadata": {
        "id": "jRl6BZWUZ-WI"
      },
      "outputs": [],
      "source": [
        "### Encode Features\n",
        "\n",
        "cols_to_scale = df.columns[1:]\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(len(cols_to_scale),)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(cols_to_scale), activation='linear'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3hmEdGGclTxU",
      "metadata": {
        "id": "3hmEdGGclTxU"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the autoencoder using the scaled features\n",
        "cols_to_scale = df.columns[1:]\n",
        "autoencoder = model.fit(df[cols_to_scale], df[cols_to_scale], epochs=100, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gfyyEElYwPQ_",
      "metadata": {
        "id": "gfyyEElYwPQ_"
      },
      "outputs": [],
      "source": [
        "# Plot the graph of Loss versus Epoch\n",
        "plt.plot(autoencoder.history[\"loss\"])\n",
        "plt.plot(figsize=(15, 7))\n",
        "plt.title('Loss vs. Epoch', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DmJ8a4TuwPRA",
      "metadata": {
        "id": "DmJ8a4TuwPRA"
      },
      "outputs": [],
      "source": [
        "# Select the columns to be scaled (from index 6 onwards)\n",
        "cols_to_scale = df.columns[1:]\n",
        "\n",
        "reconstruction_error = np.square(df[cols_to_scale] - model.predict(df[cols_to_scale]))\n",
        "feature_reconstruction_error = np.mean(reconstruction_error, axis=0)\n",
        "feature_reconstruction_error_df = pd.DataFrame(feature_reconstruction_error, index=cols_to_scale).T\n",
        "\n",
        "overall_reconstruction_error = feature_reconstruction_error_df.mean().mean()\n",
        "print('\\n','\\n', f\"\\033[1mOverall Reconstruction Error: {overall_reconstruction_error:.4f}\\033[0m\", '\\n','\\n')\n",
        "\n",
        "# Print the individual features error in a horizontal format\n",
        "display(feature_reconstruction_error_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nr2MdqXlwPRA",
      "metadata": {
        "id": "Nr2MdqXlwPRA"
      },
      "source": [
        "You can see that certain features have very low error, but some might have an error as high as 0.2. However, overall the error rate is 0.04 and thus, we can move forward.\n",
        "\n",
        "Let us now move towards finding the reduced features. This is done in the following steps.\n",
        "\n",
        "**Extract the encoder part of the autoencoder**:\n",
        "You will extract the encoder part of the autoencoder, which compresses the data into a reduced-dimensional representation. This part includes the first three layers of the model.\n",
        "\n",
        "Use the encoder to obtain the reduced-dimensional representation:\n",
        "We use the encoder to transform the input test data `(X_test)` into a reduced-dimensional representation `(X_encoded_test)`. This represents an encoded version of the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pvDj7316wPRA",
      "metadata": {
        "id": "pvDj7316wPRA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Re-create the encoder from the layers of the successfully trained model\n",
        "encoder = Sequential(model.layers[:3])\n",
        "X_encoded_test = encoder.predict(X[cols_to_scale]) # Use the scaled features for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmIW_EmxwPRB",
      "metadata": {
        "id": "bmIW_EmxwPRB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ── NUEVO: añade la columna Date al DataFrame codificado ──\n",
        "dates = df['Date'].reset_index(drop=True)                 # 1) copia la fecha\n",
        "features_enc = pd.DataFrame(                              # 2) crea el DF codificado\n",
        "    X_encoded_test,\n",
        "    columns=[f'Encoded_{i}' for i in range(X_encoded_test.shape[1])]\n",
        ")\n",
        "features_enc.insert(0, 'Date', dates)\n",
        "features_enc.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B-uYkk3_e3wF",
      "metadata": {
        "id": "B-uYkk3_e3wF"
      },
      "outputs": [],
      "source": [
        "features_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3614702",
      "metadata": {
        "id": "d3614702"
      },
      "outputs": [],
      "source": [
        "### Save the encoder model\n",
        "\n",
        "encoder_save_path = root_data+'Models/'+symbol+'_'+direction+'_'+data_type+'_ahm_encoder_model.keras'\n",
        "os.makedirs(os.path.dirname(encoder_save_path), exist_ok=True)\n",
        "encoder.save(encoder_save_path)\n",
        "print(f\"Encoder model saved successfully at: {encoder_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lHzi4304z00U",
      "metadata": {
        "id": "lHzi4304z00U"
      },
      "outputs": [],
      "source": [
        "features_enc.to_csv(root_data+'Results/'+symbol+'_'+direction+'_M5M10_'+data_type+'_Enc_Features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kEzuAKPE3KBb",
      "metadata": {
        "id": "kEzuAKPE3KBb"
      },
      "source": [
        "# Varios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1079c7b1",
      "metadata": {
        "id": "1079c7b1"
      },
      "outputs": [],
      "source": [
        "df.tail(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ysa-7eLvEMpE",
        "m8CIB8vltFfH",
        "y6QRdBKwrX98",
        "zhTndYVi1TEV",
        "jh9-mi26wsya",
        "Hv5bsrpc03z7",
        "2RFfhHT2AwAJ"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}