{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf83164c",
   "metadata": {
    "id": "bf83164c"
   },
   "source": [
    "# Pendientes\n",
    "\n",
    "Nada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ysa-7eLvEMpE",
   "metadata": {
    "id": "Ysa-7eLvEMpE"
   },
   "source": [
    "# Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9GJAW8qAEM8f",
   "metadata": {
    "id": "9GJAW8qAEM8f"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcIdAmrNERw-",
   "metadata": {
    "id": "FcIdAmrNERw-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m8CIB8vltFfH",
   "metadata": {
    "id": "m8CIB8vltFfH"
   },
   "source": [
    "# Set_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EB5RqjoAtFwl",
   "metadata": {
    "id": "EB5RqjoAtFwl"
   },
   "outputs": [],
   "source": [
    "\n",
    "root_data = f'/content/drive/MyDrive/Course Folder/Forex/XAUUSD/'\n",
    "print(root_data)\n",
    "\n",
    "direction = 'Short'\n",
    "direction_number = -1\n",
    "\n",
    "symbol = 'XAUUSD'\n",
    "strategy = 'Kalman'\n",
    "time_frame = 'M5'\n",
    "\n",
    "trade_evolution = 'st_Max'\n",
    "result_field = 'st_PnL'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6QRdBKwrX98",
   "metadata": {
    "id": "y6QRdBKwrX98"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1a01c",
   "metadata": {
    "id": "bda1a01c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import math\n",
    "import time\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from typing import Tuple, List, Optional, Dict, Any, Union\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import (roc_auc_score, f1_score, accuracy_score, log_loss, r2_score)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7JweuZy755ym",
   "metadata": {
    "id": "7JweuZy755ym"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KFfn45ty82qv",
   "metadata": {
    "id": "KFfn45ty82qv"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa65d54",
   "metadata": {
    "id": "9aa65d54"
   },
   "source": [
    "# Calculate_Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I23127P7lBq_",
   "metadata": {
    "id": "I23127P7lBq_"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kp4yJdGAjoeA",
   "metadata": {
    "id": "kp4yJdGAjoeA"
   },
   "outputs": [],
   "source": [
    "def kalman_line(source, kalman_length: int, smooth: int):\n",
    "    \"\"\"\n",
    "    Pine -> Python (solo 'kalman_line'), replicando la EMA de TradingView con\n",
    "    *semilla SMA* (como ta.ema) sobre el núcleo Kalman kf_c.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    source : pd.Series o array-like de floats (precio crudo, sin diff/returns)\n",
    "    kalman_length : int   (equivale a length_kal en Pine)\n",
    "    smooth : int          (equivale a smooth_kal en Pine -> ta.ema(kf_c, smooth))\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    Mismo tipo que `source`: pd.Series o np.ndarray con la línea Kalman suavizada.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # normalizamos tipos\n",
    "    is_series = hasattr(source, \"index\")\n",
    "    idx = source.index if is_series else None\n",
    "    x = np.asarray(source, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if n == 0:\n",
    "        return source\n",
    "\n",
    "    # ---------- núcleo Kalman idéntico al Pine ----------\n",
    "    sqrt_term   = np.sqrt((kalman_length / 10000.0) * 2.0)\n",
    "    length_term = kalman_length / 10000.0\n",
    "\n",
    "    kf_c   = np.empty(n, dtype=np.float64)\n",
    "    velo_c = np.empty(n, dtype=np.float64)\n",
    "\n",
    "    # bar 0 (nz(kf_c[1], source) y nz(velo_c[1], 0))\n",
    "    kf_c[0] = x[0]\n",
    "    velo_c[0] = 0.0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        prev_kf = kf_c[i - 1]\n",
    "        dk_c = x[i] - prev_kf\n",
    "        smooth_c = prev_kf + dk_c * sqrt_term\n",
    "        velo_c[i] = velo_c[i - 1] + length_term * dk_c\n",
    "        kf_c[i] = smooth_c + velo_c[i]\n",
    "\n",
    "    # ---------- EMA con semilla SMA (comportamiento ta.ema de TV) ----------\n",
    "    L = int(max(1, smooth))\n",
    "    alpha = 2.0 / (L + 1.0)\n",
    "    ema = np.full(n, np.nan, dtype=np.float64)\n",
    "\n",
    "    if n < L:\n",
    "        # con pocas barras, igualamos al promedio simple disponible\n",
    "        ema[-1] = np.nanmean(kf_c)\n",
    "    else:\n",
    "        # seed = SMA de las primeras L barras\n",
    "        seed = np.mean(kf_c[:L])\n",
    "        ema[L - 1] = seed\n",
    "        for i in range(L, n):\n",
    "            ema[i] = alpha * kf_c[i] + (1.0 - alpha) * ema[i - 1]\n",
    "\n",
    "    return (pd.Series(ema, index=idx) if is_series else ema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e51fb",
   "metadata": {
    "id": "e07e51fb"
   },
   "outputs": [],
   "source": [
    "def create_features(\n",
    "    stock_data: pd.DataFrame,\n",
    "    return_components: bool = False\n",
    ") -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n",
    "\n",
    "    kalman_periods = [300, 600, 900]\n",
    "\n",
    "    component_frames: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    def _scale_block(block: pd.DataFrame, window: int = 200) -> pd.DataFrame:\n",
    "        \"\"\"Apply rolling z-score scaling to a feature block.\"\"\"\n",
    "        if block.empty:\n",
    "            return block.copy()\n",
    "        scaled = block.copy()\n",
    "        rolling = scaled.rolling(window, min_periods=1)\n",
    "        centered = scaled - rolling.mean()\n",
    "        std = rolling.std(ddof=0).replace(0, np.nan)\n",
    "        scaled = centered / std\n",
    "        return scaled.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # ───────────────────────── Kalman filters ───────────────────────\n",
    "    t0 = time.time()\n",
    "    base_features = pd.DataFrame(index=stock_data.index)\n",
    "    base_features['Close'] = stock_data['Close']\n",
    "\n",
    "    kalman_900_series: Optional[pd.Series] = None\n",
    "\n",
    "    for period in tqdm(kalman_periods, desc=\"Kalman on Close\"):\n",
    "        kal_series = pd.Series(\n",
    "            kalman_line(stock_data['Close'], kalman_length=period, smooth=3),\n",
    "            index=stock_data.index,\n",
    "            name=f'Kal_{period}'\n",
    "        )\n",
    "        base_features[kal_series.name] = kal_series\n",
    "        if period == 900:\n",
    "            kalman_900_series = kal_series\n",
    "\n",
    "    if kalman_900_series is None:\n",
    "        raise ValueError(\"Kalman series for period 900 could not be computed.\")\n",
    "\n",
    "    close_minus_kal900 = stock_data['Close'] - kalman_900_series\n",
    "    base_features['Close_minus_Kal_900'] = close_minus_kal900\n",
    "\n",
    "    # ───── Kalman filters on (Close - Kal_900) ─────\n",
    "    kalman_close_diff = pd.DataFrame(index=stock_data.index)\n",
    "    for period in tqdm(kalman_periods, desc=\"Kalman on Close-Kal900\", leave=False):\n",
    "        diff_series = pd.Series(\n",
    "            kalman_line(close_minus_kal900, kalman_length=period, smooth=3),\n",
    "            index=stock_data.index,\n",
    "            name=f'Kal_CloseDiff_{period}'\n",
    "        )\n",
    "        kalman_close_diff[diff_series.name] = diff_series\n",
    "\n",
    "    # ───── Scaling and derivatives for Kalman indicators ─────\n",
    "    scaling_inputs = pd.concat(\n",
    "        [base_features[[f'Kal_{p}' for p in kalman_periods]], kalman_close_diff],\n",
    "        axis=1\n",
    "    )\n",
    "    scaled_kalman = _scale_block(scaling_inputs)\n",
    "    scaled_kalman.columns = [f\"{col}_scaled\" for col in scaled_kalman.columns]\n",
    "\n",
    "    scaled_diff = scaled_kalman.diff().add_suffix('_diff')\n",
    "    scaled_pct = scaled_kalman.pct_change().replace([np.inf, -np.inf], np.nan).add_suffix('_pct_change')\n",
    "\n",
    "    rolling_min_frames = []\n",
    "    for window in (2, 5, 7):\n",
    "        rolling_min = scaled_kalman.rolling(window, min_periods=1).min()\n",
    "        rolling_min.columns = [f\"{col}_rollmin_{window}\" for col in scaled_kalman.columns]\n",
    "        rolling_min_frames.append(rolling_min)\n",
    "\n",
    "    tqdm.write(f\"[Timing] Kalman block: {time.time()-t0:.2f}s\")\n",
    "\n",
    "    component_frames['Kalman_Close'] = base_features[[f'Kal_{p}' for p in kalman_periods]].copy()\n",
    "    component_frames['Close_minus_Kal_900'] = base_features[['Close_minus_Kal_900']].copy()\n",
    "    component_frames['Kalman_CloseDiff'] = kalman_close_diff.copy()\n",
    "    component_frames['Kalman_Scaled'] = scaled_kalman.copy()\n",
    "    component_frames['Kalman_Scaled_Derivatives'] = pd.concat([scaled_diff, scaled_pct], axis=1)\n",
    "    for window, frame in zip((2, 5, 7), rolling_min_frames):\n",
    "        component_frames[f'Kalman_Scaled_RollMin_{window}'] = frame.copy()\n",
    "\n",
    "    features = pd.concat(\n",
    "        [base_features, kalman_close_diff, scaled_kalman, scaled_diff, scaled_pct] + rolling_min_frames,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    if return_components:\n",
    "        component_frames['Create_Features'] = features.copy()\n",
    "        return features, component_frames\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c0f79",
   "metadata": {
    "id": "1e0c0f79"
   },
   "outputs": [],
   "source": [
    "def scale_feature_block(features: pd.DataFrame, window: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Scale features using a rolling window standardization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    window : int, optional\n",
    "        Rolling window size, by default 200.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Scaled feature block preserving the original index.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return features.copy()\n",
    "\n",
    "    scaled = features.copy()\n",
    "    rolling = scaled.rolling(window, min_periods=1)\n",
    "    centered = scaled - rolling.mean()\n",
    "    scaled = centered / rolling.std(ddof=0).replace(0, np.nan)\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DoqqPJxPxnBF",
   "metadata": {
    "id": "DoqqPJxPxnBF"
   },
   "source": [
    "## 5_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7l7vt7QxvyC",
   "metadata": {
    "id": "d7l7vt7QxvyC"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df_5min = pd.read_csv(root_data + 'Data/'+symbol+'_M5.csv', index_col=0)\n",
    "df_5min.index = pd.to_datetime(df_5min.index)\n",
    "#df_5min = df_5min.iloc[-50000:,]\n",
    "\n",
    "print('Min_Date : ', df_5min.index.min())\n",
    "print('Min_Date : ', df_5min.index.max())\n",
    "print('Number_Rows = ',len(df_5min.index))\n",
    "print('\\n')\n",
    "\n",
    "df_5min.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HBicVZcDx2CF",
   "metadata": {
    "id": "HBicVZcDx2CF"
   },
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XxU4wllsEUcN",
   "metadata": {
    "id": "XxU4wllsEUcN"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "features_5min_raw, components_5min = create_features(df_5min, return_components=True)\n",
    "features_5min_raw = features_5min_raw.dropna()\n",
    "valid_index_5min = features_5min_raw.index\n",
    "\n",
    "# Align every component to the valid index coming from the full feature set\n",
    "m5_raw_blocks: Dict[str, pd.DataFrame] = {name: frame.loc[valid_index_5min].copy()\n",
    "                                           for name, frame in components_5min.items()}\n",
    "m5_raw_blocks['Create_Features'] = features_5min_raw\n",
    "\n",
    "m5_scaled_blocks: Dict[str, pd.DataFrame] = {}\n",
    "m5_raw_paths: Dict[str, str] = {}\n",
    "m5_scaled_paths: Dict[str, str] = {}\n",
    "\n",
    "for name, frame in m5_raw_blocks.items():\n",
    "    filename_raw = f\"{symbol}_M5_{name}_Raw_Features.csv\" if name != 'Create_Features' else f\"{symbol}_M5_Raw_Features.csv\"\n",
    "    raw_path = os.path.join(root_data, 'Results', filename_raw)\n",
    "    frame.to_csv(raw_path)\n",
    "    m5_raw_paths[name] = raw_path\n",
    "\n",
    "    scaled_frame = scale_feature_block(frame)\n",
    "    m5_scaled_blocks[name] = scaled_frame\n",
    "    filename_scale = f\"{symbol}_M5_{name}_Scale_Features.csv\" if name != 'Create_Features' else f\"{symbol}_M5_Scale_Features.csv\"\n",
    "    scale_path = os.path.join(root_data, 'Results', filename_scale)\n",
    "    scaled_frame.to_csv(scale_path)\n",
    "    m5_scaled_paths[name] = scale_path\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Number of features are: {features_5min_raw.shape[1]}\")\n",
    "print(features_5min_raw.shape)\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "print(f\"Saved {len(m5_raw_blocks) * 2} DataFrames for the M5 timeframe.\")\n",
    "features_5min_raw.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Vw_2C-ZEO57",
   "metadata": {
    "id": "7Vw_2C-ZEO57"
   },
   "outputs": [],
   "source": [
    "print(\"NaN counts per column (sorted):\")\n",
    "print(m5_raw_blocks['Create_Features'].isnull().sum().sort_values(ascending=False), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4AwNJic6icv",
   "metadata": {
    "id": "j4AwNJic6icv"
   },
   "outputs": [],
   "source": [
    "print(\"Saved raw feature files:\")\n",
    "for name, path in m5_raw_paths.items():\n",
    "    print(f\" - {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edtpuJ4Y-E",
   "metadata": {
    "id": "71edtpuJ4Y-E"
   },
   "outputs": [],
   "source": [
    "print(list(features_5min_raw.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QX4cSq3Ftqhm",
   "metadata": {
    "id": "QX4cSq3Ftqhm"
   },
   "source": [
    "**Scale_features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qEb1Cz1vzj6",
   "metadata": {
    "id": "1qEb1Cz1vzj6"
   },
   "outputs": [],
   "source": [
    "print(\"Saved scaled feature files:\")\n",
    "for name, path in m5_scaled_paths.items():\n",
    "    print(f\" - {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eJcGUESttqhm",
   "metadata": {
    "id": "eJcGUESttqhm"
   },
   "outputs": [],
   "source": [
    "m5_scaled_blocks['Create_Features'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BFVUCAe3GWFy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFVUCAe3GWFy",
    "outputId": "e159ed54-c793-417c-efc5-cee3cf0ebba8"
   },
   "outputs": [],
   "source": [
    "print(\"NaN counts per column (scaled, sorted):\")\n",
    "print(m5_scaled_blocks['Create_Features'].isnull().sum().sort_values(ascending=False), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aXMGezVftqhn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXMGezVftqhn",
    "outputId": "47394118-0f87-40ad-fcb4-09964531ea21"
   },
   "outputs": [],
   "source": [
    "print(\"M5 feature blocks:\", list(m5_raw_blocks.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1Pk4Hkfjhvt",
   "metadata": {
    "id": "W1Pk4Hkfjhvt"
   },
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67453c9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "67453c9e",
    "outputId": "9b2012d6-f4f6-4167-9956-800e8b6ed102"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "last_200_features_5min = m5_scaled_blocks['Create_Features'].tail(200)\n",
    "last_200_df_5min = df_5min.tail(200)\n",
    "\n",
    "indicators_to_plot = ['Close','5min_Kal_300','5min_Kal_600']\n",
    "\n",
    "# Separate Close from other indicators\n",
    "close_to_plot = 'Close' if 'Close' in indicators_to_plot else None\n",
    "other_indicators_to_plot = [col for col in indicators_to_plot if col != 'Close' and col in last_200_features_5min.columns]\n",
    "\n",
    "num_plots = len(other_indicators_to_plot) + (1 if close_to_plot else 0)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(15, 2.5 * num_plots), sharex=True)\n",
    "\n",
    "# Ensure axes is an array even if only one plot\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "current_plot_index = 0\n",
    "\n",
    "# Plot Close price if requested\n",
    "if close_to_plot:\n",
    "    axes[current_plot_index].plot(last_200_df_5min.index, last_200_df_5min['Close'])\n",
    "    axes[current_plot_index].set_title('Close Price')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    current_plot_index += 1\n",
    "\n",
    "# Plot each selected indicator (excluding 'Close')\n",
    "for col in other_indicators_to_plot:\n",
    "    axes[current_plot_index].plot(last_200_features_5min.index, last_200_features_5min[col])\n",
    "    axes[current_plot_index].set_title(col)\n",
    "    axes[current_plot_index].grid(True)\n",
    "    current_plot_index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5c__w6s_dtY",
   "metadata": {
    "id": "A5c__w6s_dtY"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4j7J8tk_f3o-",
   "metadata": {
    "id": "4j7J8tk_f3o-"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6XOsumPycYz3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "6XOsumPycYz3",
    "outputId": "9482225b-2764-4223-ec8f-971e9d26ac91"
   },
   "outputs": [],
   "source": [
    "lab = pd.read_csv(root_data + 'Results/'+symbol+'_'+strategy+'_'+time_frame+'_Strategy_Gen_Labels.csv', index_col=0)\n",
    "lab['Date'] = pd.to_datetime(lab['Date'])\n",
    "\n",
    "print('Min_Date    : ',lab['Date'].min())\n",
    "print('Min_Date    : ',lab['Date'].max(),'\\n')\n",
    "print('Number_Rows : ',lab.shape,'\\n')\n",
    "print('Columns     : ',lab.columns)\n",
    "\n",
    "lab['Open_Trade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apV2tezPsbux",
   "metadata": {
    "id": "apV2tezPsbux"
   },
   "outputs": [],
   "source": [
    "#analyse_column = 'st_atr_max_PnL'\n",
    "analyse_column = 'st_Max'\n",
    "\n",
    "st_max_0  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
    "st_max_1  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
    "st_max_2  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].sum()\n",
    "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].sum()\n",
    "\n",
    "\n",
    "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 0)),analyse_column].sum()\n",
    "\n",
    "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 0.7) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].sum()\n",
    "\n",
    "st_max_5 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 1) &\n",
    "                   (lab['st_atr_max_PnL'] <= 1.5)),analyse_column].sum()\n",
    "\n",
    "st_max_6 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 1.5) &\n",
    "                   (lab['st_atr_max_PnL'] <= 2)),analyse_column].sum()\n",
    "\n",
    "st_max_7 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 2)),analyse_column].sum()\n",
    "\n",
    "\n",
    "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
    "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
    "print(f'Above_Mean = {st_max_2:,.2f}')\n",
    "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
    "\n",
    "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
    "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
    "print(f'> 1 & <= 1.5 = {st_max_5:,.2f}')\n",
    "print(f'> 1.5 & <= 2 = {st_max_6:,.2f}')\n",
    "print(f'> 2 = {st_max_7:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2fjs4Si792v",
   "metadata": {
    "id": "p2fjs4Si792v"
   },
   "outputs": [],
   "source": [
    "analyse_column = 'st_atr_max_PnL'\n",
    "\n",
    "st_max_0 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
    "st_max_1 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
    "st_max_2 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].count()\n",
    "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].count()\n",
    "\n",
    "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                    (lab['st_atr_max_PnL'] <= 0.5)),analyse_column].count()\n",
    "\n",
    "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                     (lab['st_atr_max_PnL'] >= 0.5) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].count()\n",
    "\n",
    "st_max_5 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] >= 1), analyse_column].count()\n",
    "\n",
    "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
    "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
    "print(f'Above_Mean = {st_max_2:,.2f}')\n",
    "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
    "\n",
    "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
    "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
    "print(f'> 1 = {st_max_5:,.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85DATjQqZd66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85DATjQqZd66",
    "outputId": "7025ab16-8427-43fc-bef8-07c54b2c5264"
   },
   "outputs": [],
   "source": [
    "# --- Parámetros / campos\n",
    "result_field = 'st_atr_max_PnL'\n",
    "\n",
    "#valid = (\n",
    "#    (lab['Type'] == direction) &\n",
    "#    (lab['Open_Trade'].isin([1, -1])) &\n",
    "#    (lab[result_field].notna()))\n",
    "\n",
    "valid = (\n",
    "    (lab['Open_Trade'].isin([1, -1])) &\n",
    "    (lab[result_field].notna()))\n",
    "\n",
    "\n",
    "# --- Etiquetado en la columna \"label\" con valores 4/5/6\n",
    "lab['label'] = np.nan\n",
    "lab.loc[valid & (lab[result_field] <= 1), 'label'] = 0\n",
    "lab.loc[valid & (lab[result_field] >= 1), 'label'] = 1\n",
    "\n",
    "\n",
    "# --- Mantener solo filas válidas y con label\n",
    "lab = lab.loc[valid & lab['label'].notna()].copy()\n",
    "lab['label'] = lab['label'].astype('int8')\n",
    "\n",
    "# --- Ver distribución de labels 4/5/6\n",
    "print('\\nValue counts de label 4/5/6:')\n",
    "print(lab['label'].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_BCdqt3Y0Fjm",
   "metadata": {
    "id": "_BCdqt3Y0Fjm"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gX04ZTmfz-pK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gX04ZTmfz-pK",
    "outputId": "a7838052-f4b6-4a1c-a0a7-a90cffc2b3ea"
   },
   "outputs": [],
   "source": [
    "raw_feat_5min = pd.read_csv(root_data+'Results/'+symbol+'_M5_Raw_Features.csv')\n",
    "raw_feat_5min[\"Date\"] = pd.to_datetime(raw_feat_5min[\"Date\"])\n",
    "print(raw_feat_5min.shape)\n",
    "#raw_feat_5min.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5gxf0ke0KUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5gxf0ke0KUx",
    "outputId": "664f38c5-5cef-4055-ca65-55b2806d86f2"
   },
   "outputs": [],
   "source": [
    "scale_feat_5min = pd.read_csv(root_data+'Results/'+symbol+'_M5_Scale_Features.csv')\n",
    "#scale_feat_5min = scale_feat_5min.drop('Unnamed: 0', axis=1)\n",
    "scale_feat_5min[\"Date\"] = pd.to_datetime(scale_feat_5min[\"Date\"])\n",
    "print(scale_feat_5min.shape)\n",
    "#scale_feat_5min.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hv5bsrpc03z7",
   "metadata": {
    "id": "Hv5bsrpc03z7"
   },
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mc1y2NbRM_f1",
   "metadata": {
    "id": "Mc1y2NbRM_f1"
   },
   "outputs": [],
   "source": [
    "data_type = 'Scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I0gkNVT60Ka5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0gkNVT60Ka5",
    "outputId": "09725b8c-2fb3-480f-bc9e-a3d0240817d6"
   },
   "outputs": [],
   "source": [
    "def prepare_feature_block(frame: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of the feature block with a consistent prefix.\"\"\"\n",
    "    block = frame.copy()\n",
    "    if 'Unnamed: 0' in block.columns:\n",
    "        block = block.drop(columns='Unnamed: 0')\n",
    "    block = block.drop_duplicates(subset='Date').sort_values('Date')\n",
    "    rename_map = {col: f\"{prefix}{col}\" for col in block.columns if col != 'Date'}\n",
    "    block = block.rename(columns=rename_map)\n",
    "    return block\n",
    "\n",
    "feature_blocks = {\n",
    "    'M5_raw': prepare_feature_block(raw_feat_5min, 'M5_raw_'),\n",
    "    'M5_scale': prepare_feature_block(scale_feat_5min, 'M5_scale_'),\n",
    "}\n",
    "\n",
    "feature_df = lab[['Date']].drop_duplicates().copy()\n",
    "\n",
    "print('Feature blocks merged:')\n",
    "for name, block in feature_blocks.items():\n",
    "    feature_df = feature_df.merge(block, on='Date', how='left')\n",
    "    print(f\" - {name}: {block.shape[1] - 1} columns\")\n",
    "\n",
    "feature_df = feature_df.sort_values('Date').set_index('Date').ffill().reset_index()\n",
    "\n",
    "df = lab[['Date', 'Close','label', 'Open_Trade','kal_1', 'kal_2', 'kal_3', 'kal_4']].merge(feature_df, on='Date', how='left')\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('label')\n",
    "cols.insert(1, 'label')\n",
    "df = df[cols]\n",
    "\n",
    "combined_feature_path = os.path.join(root_data, 'Results', f\"{symbol}_{direction}_AllFeatures.csv\")\n",
    "df.to_csv(combined_feature_path, index=False)\n",
    "\n",
    "print(f\"Combined feature dataframe shape: {df.shape}\")\n",
    "print(f\"Saved combined feature dataframe to: {combined_feature_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQPfQroWYLRM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQPfQroWYLRM",
    "outputId": "978085e0-5f33-4a38-a769-5f4dc47c43be"
   },
   "outputs": [],
   "source": [
    "expected_prefixes = ['M5_raw_', 'M5_scale_']\n",
    "for prefix in expected_prefixes:\n",
    "    matching_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    print(f\"{prefix}: {len(matching_cols)} columns\")\n",
    "\n",
    "duplicated_columns = df.columns[df.columns.duplicated()].tolist()\n",
    "if duplicated_columns:\n",
    "    print('Duplicated feature columns detected:', duplicated_columns)\n",
    "else:\n",
    "    print('No duplicated feature columns detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jXcXWty506Ur",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXcXWty506Ur",
    "outputId": "8c8024b5-b147-4d3c-f9cc-696fee6058fd"
   },
   "outputs": [],
   "source": [
    "print(df.shape,'\\n')\n",
    "print('Label_Counts : ',df.label.value_counts(),'\\n')\n",
    "print(list(df.columns), '\\n')\n",
    "\n",
    "# Add NaN count per column, sorted\n",
    "print(\"NaN counts per column (sorted):\")\n",
    "print(df.isnull().sum().sort_values(ascending=False), '\\n')\n",
    "\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F-S1XcoyKbjY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-S1XcoyKbjY",
    "outputId": "03134025-5570-4aa8-831d-5bddeb24e334"
   },
   "outputs": [],
   "source": [
    "print(list(df.columns), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d67f8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "06d67f8e",
    "outputId": "7de079fb-ff04-4255-993b-2e98a96b23eb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kalman_periods = [300, 600, 900]\n",
    "\n",
    "# Define the row range for plotting\n",
    "start_row = 200\n",
    "end_row = 250\n",
    "\n",
    "# Select the data for the specified row range from df\n",
    "plot_df = df.iloc[start_row:end_row].copy()\n",
    "\n",
    "chart_groups = [\n",
    "    (\"Close & Close_minus_Kal_900\", [col for col in ['Close', 'Close_minus_Kal_900'] if col in plot_df.columns]),\n",
    "    (\"Scaled Kalman on Close\", [col for col in [f'Kal_{p}_scaled' for p in kalman_periods] if col in plot_df.columns]),\n",
    "    (\"Scaled Kalman on Close-Kal900\", [col for col in [f'Kal_CloseDiff_{p}_scaled' for p in kalman_periods] if col in plot_df.columns]),\n",
    "    (\"Scaled Kalman diff\", [col for col in [f'Kal_{p}_scaled_diff' for p in kalman_periods] + [f'Kal_CloseDiff_{p}_scaled_diff' for p in kalman_periods] if col in plot_df.columns]),\n",
    "    (\"Scaled Kalman pct change\", [col for col in [f'Kal_{p}_scaled_pct_change' for p in kalman_periods] + [f'Kal_CloseDiff_{p}_scaled_pct_change' for p in kalman_periods] if col in plot_df.columns])\n",
    "]\n",
    "\n",
    "# Keep only chart groups that have at least one column to plot\n",
    "chart_groups = [(title, cols) for title, cols in chart_groups if cols]\n",
    "\n",
    "if not chart_groups:\n",
    "    raise ValueError(\"No matching columns found for plotting in the specified range.\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(chart_groups), ncols=1, figsize=(15, 4 * len(chart_groups)), sharex=True)\n",
    "\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "for ax, (title, columns) in zip(axes, chart_groups):\n",
    "    ax.plot(plot_df['Date'], plot_df[columns])\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    ax.legend(columns)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4TPpba7UChUY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "4TPpba7UChUY",
    "outputId": "d6a16411-2abe-4652-e882-a410b98d0358"
   },
   "outputs": [],
   "source": [
    "### Align feature directions so that shorts mirror longs\n",
    "if 'Open_Trade' not in df.columns:\n",
    "    raise KeyError(\"'Open_Trade' column is required in df to flip feature signs.\")\n",
    "\n",
    "# Drop intermediate columns that are no longer needed\n",
    "cols_to_drop = [col for col in df.columns if ('raw' in col or 'RSI' in col)]\n",
    "if cols_to_drop:\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    print(f\"Dropped {len(cols_to_drop)} columns containing 'raw' or 'RSI'.\")\n",
    "else:\n",
    "    print(\"No 'raw' or 'RSI' columns found to drop.\")\n",
    "\n",
    "# Identify feature columns to flip (exclude identifiers/targets and Close)\n",
    "value_columns = [col for col in df.columns if col not in ['Date', 'label', 'Open_Trade']]\n",
    "feature_cols = [col for col in value_columns if col != 'Close']\n",
    "short_mask = df['Open_Trade'] == -1\n",
    "\n",
    "if short_mask.any() and feature_cols:\n",
    "    df.loc[short_mask, feature_cols] = df.loc[short_mask, feature_cols] * -1\n",
    "    print(f\"Flipped {short_mask.sum()} rows with Open_Trade = -1.\")\n",
    "else:\n",
    "    print(\"No rows with Open_Trade = -1 were found or no feature columns to flip.\")\n",
    "\n",
    "# Reorder columns so Open_Trade stays next to the label for downstream steps.\n",
    "ordered_cols = ['Date', 'label', 'Open_Trade']\n",
    "if 'Close' in df.columns:\n",
    "    ordered_cols.append('Close')\n",
    "ordered_cols.extend([col for col in df.columns if col not in ordered_cols])\n",
    "\n",
    "missing_cols = [col for col in ordered_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"The following expected columns are missing from df: {missing_cols}\")\n",
    "\n",
    "df = df[ordered_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pQa8_4GpvRRf",
   "metadata": {
    "id": "pQa8_4GpvRRf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lBtQecOO08zB",
   "metadata": {
    "id": "lBtQecOO08zB"
   },
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rlkmypd9DJj",
   "metadata": {
    "id": "8rlkmypd9DJj"
   },
   "outputs": [],
   "source": [
    "# ===================== 1. ENTRENAR Y OBTENER IMPORTANCIAS =====================\n",
    "def compute_xgb_importance(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    random_state: int = 42,\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[pd.DataFrame, Any]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost y devuelve:\n",
    "      - imp_df: DataFrame con 'feature', 'importance' y 'cum_importance'.\n",
    "      - model : modelo ya entrenado.\n",
    "\n",
    "    Soporta:\n",
    "      • Clasificación binaria o multiclase (detecta nº de clases).\n",
    "      • Regresión (si task != 'classification').\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de características (sin la columna objetivo).\n",
    "    y : pd.Series\n",
    "        Etiquetas objetivo. Puede ser binaria (0/1) o multiclase (0..K-1).\n",
    "    task : str, opcional\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    random_state : int, opcional\n",
    "        Semilla para reproducibilidad.\n",
    "    **xgb_params : dict\n",
    "        Parámetros adicionales para el estimador de XGBoost.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (imp_df, model)\n",
    "        imp_df : DataFrame con importancias y su acumulado.\n",
    "        model  : instancia entrenada de XGBClassifier / XGBRegressor.\n",
    "    \"\"\"\n",
    "    default_params: Dict[str, Any] = dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    default_params.update(xgb_params)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        # Detectar nº de clases\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        # XGBClassifier ajusta objetivo automáticamente, pero lo explicitamos:\n",
    "        if n_classes > 2:\n",
    "            default_params.setdefault(\"objective\", \"multi:softprob\")\n",
    "            default_params.setdefault(\"num_class\", n_classes)\n",
    "            eval_metric = \"mlogloss\"\n",
    "        else:\n",
    "            default_params.setdefault(\"objective\", \"binary:logistic\")\n",
    "            eval_metric = \"logloss\"\n",
    "\n",
    "        model = XGBClassifier(eval_metric=eval_metric, **default_params)\n",
    "\n",
    "    else:\n",
    "        model = XGBRegressor(**default_params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance\": model.feature_importances_\n",
    "        })\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    total_imp = imp_df[\"importance\"].sum()\n",
    "    if total_imp == 0:\n",
    "        # Evitar división por cero si el modelo devuelve todo cero (raro, pero posible)\n",
    "        imp_df[\"cum_importance\"] = 0.0\n",
    "    else:\n",
    "        imp_df[\"cum_importance\"] = imp_df[\"importance\"].cumsum() / total_imp\n",
    "\n",
    "    return imp_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3r9Di4Xx9DOU",
   "metadata": {
    "id": "3r9Di4Xx9DOU"
   },
   "outputs": [],
   "source": [
    "# ===================== 2. SELECCIÓN DE FEATURES =====================\n",
    "def select_features_with_importance(\n",
    "    X: pd.DataFrame,\n",
    "    imp_df: pd.DataFrame,\n",
    "    top_n: Optional[int] = None,\n",
    "    threshold: Optional[str | float] = None,\n",
    "    cum_threshold: Optional[float] = 0.8\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Selección flexible de variables a partir de importancias de XGBoost.\n",
    "\n",
    "    Reglas:\n",
    "      - Si top_n no es None           => usa el top_n.\n",
    "      - Else si cum_threshold no None => usa importancia acumulada (p.ej. 0.8 = 80%).\n",
    "      - Else usa threshold ('median', 'mean' o valor numérico).\n",
    "\n",
    "    Devuelve (X_reducido, lista_de_features).\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de características original.\n",
    "    imp_df : pd.DataFrame\n",
    "        DataFrame devuelto por compute_xgb_importance.\n",
    "    top_n : int | None\n",
    "        Número fijo de variables a conservar.\n",
    "    threshold : str | float | None\n",
    "        Umbral de importancia. Si str, usar 'median' o 'mean'.\n",
    "    cum_threshold : float | None\n",
    "        Porcentaje acumulado de importancia (0-1). Si None, se ignora.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_sel, keep)\n",
    "        X_sel : subset de X con columnas seleccionadas.\n",
    "        keep  : lista de nombres de columnas seleccionadas.\n",
    "    \"\"\"\n",
    "    if top_n is not None:\n",
    "        keep = imp_df.head(top_n)[\"feature\"].tolist()\n",
    "\n",
    "    elif cum_threshold is not None:\n",
    "        keep_mask = imp_df[\"cum_importance\"] <= float(cum_threshold)\n",
    "        keep = imp_df.loc[keep_mask, \"feature\"].tolist()\n",
    "        # asegurar que haya al menos una más para no quedarnos exactamente en el corte\n",
    "        if len(keep) < len(imp_df):\n",
    "            keep.append(imp_df.iloc[len(keep)][\"feature\"])\n",
    "\n",
    "    else:\n",
    "        if threshold is None:\n",
    "            threshold = \"median\"\n",
    "        if isinstance(threshold, str):\n",
    "            thr_val = imp_df[\"importance\"].agg(threshold)\n",
    "        else:\n",
    "            thr_val = float(threshold)\n",
    "        keep = imp_df.loc[imp_df[\"importance\"] >= thr_val, \"feature\"].tolist()\n",
    "\n",
    "    return X[keep], keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tCDZCTz2_v9z",
   "metadata": {
    "id": "tCDZCTz2_v9z"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. BÚSQUEDA DEL MEJOR UMBRAL ACUMULADO =====================\n",
    "def find_best_cum_threshold(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    thresholds: Tuple[float, ...] = (0.6, 0.7, 0.8, 0.9),\n",
    "    random_state: int = 42,\n",
    "    metric: str = \"auto\",\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[float, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Entrena un XGB en train, calcula importancias y prueba varios umbrales\n",
    "    acumulados para ver cuál da la mejor métrica en valid.\n",
    "\n",
    "    Para CLASIFICACIÓN:\n",
    "        - Detecta nº de clases.\n",
    "        - Métrica por defecto (metric=\"auto\"):\n",
    "            • Binaria: ROC-AUC (probabilidades de la clase positiva).\n",
    "            • Multiclase: ROC-AUC macro OVR (usa predict_proba).\n",
    "          Alternativas: metric=\"f1_macro\", \"accuracy\", \"logloss\" (se MINIMIZA).\n",
    "    Para REGRESIÓN:\n",
    "        - Usa R^2.\n",
    "\n",
    "    Devuelve:\n",
    "        best_thr, res_df_ordenado_por_score_desc, imp_df\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X_train, y_train, X_valid, y_valid : pd.DataFrame / pd.Series\n",
    "        Particiones de entrenamiento y validación.\n",
    "    task : str\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    thresholds : tuple[float, ...]\n",
    "        Valores de umbral de importancia acumulada a evaluar (0-1).\n",
    "    random_state : int\n",
    "        Semilla para reproducibilidad.\n",
    "    metric : str\n",
    "        \"auto\" (default), \"roc_auc\", \"f1_macro\", \"accuracy\", \"logloss\" (clasif) o \"r2\" (regresión).\n",
    "    **xgb_params : dict\n",
    "        Parámetros extra para el estimador de XGBoost (pasan a compute y a los modelos internos).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (best_thr, res_df, imp_df)\n",
    "        best_thr : float\n",
    "            Umbral con mejor score (o menor logloss si metric='logloss').\n",
    "        res_df : pd.DataFrame\n",
    "            Tabla con resultados por umbral (n_features, score).\n",
    "        imp_df : pd.DataFrame\n",
    "            Importancias calculadas en X_train / y_train.\n",
    "    \"\"\"\n",
    "    imp_df, _ = compute_xgb_importance(\n",
    "        X_train, y_train, task=task, random_state=random_state, **xgb_params\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Detectar nº de clases si es clasificación\n",
    "    if task == \"classification\":\n",
    "        classes = np.unique(y_train)\n",
    "        n_classes = len(classes)\n",
    "        if metric == \"auto\":\n",
    "            metric_to_use = \"roc_auc\" if n_classes == 2 else \"roc_auc\"\n",
    "        else:\n",
    "            metric_to_use = metric\n",
    "    else:\n",
    "        metric_to_use = \"r2\" if metric == \"auto\" else metric\n",
    "\n",
    "    for thr in thresholds:\n",
    "        X_tr_sel, cols = select_features_with_importance(\n",
    "            X_train, imp_df, cum_threshold=thr, top_n=None, threshold=None\n",
    "        )\n",
    "        X_va_sel = X_valid[cols]\n",
    "\n",
    "        if task == \"classification\":\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "\n",
    "            if n_classes > 2:\n",
    "                params.setdefault(\"objective\", \"multi:softprob\")\n",
    "                params.setdefault(\"num_class\", n_classes)\n",
    "                eval_metric = \"mlogloss\"\n",
    "            else:\n",
    "                params.setdefault(\"objective\", \"binary:logistic\")\n",
    "                eval_metric = \"logloss\"\n",
    "\n",
    "            model_sel = XGBClassifier(eval_metric=eval_metric, **params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "\n",
    "            # Probabilidades y predicciones\n",
    "            proba = model_sel.predict_proba(X_va_sel)\n",
    "            pred  = np.argmax(proba, axis=1) if n_classes > 2 else (proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "            # Calcular métrica\n",
    "            if metric_to_use == \"roc_auc\":\n",
    "                if n_classes == 2:\n",
    "                    score = roc_auc_score(y_valid, proba[:, 1])\n",
    "                else:\n",
    "                    # AUC macro One-vs-Rest\n",
    "                    score = roc_auc_score(y_valid, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "            elif metric_to_use == \"f1_macro\":\n",
    "                score = f1_score(y_valid, pred, average=\"macro\")\n",
    "            elif metric_to_use == \"accuracy\":\n",
    "                score = accuracy_score(y_valid, pred)\n",
    "            elif metric_to_use == \"logloss\":\n",
    "                # En este caso, menor es mejor. Guardamos negativo para mantener criterio \"mayor mejor\".\n",
    "                score = -log_loss(y_valid, proba, labels=np.unique(y_train))\n",
    "            else:\n",
    "                raise ValueError(f\"Métrica no soportada: {metric_to_use}\")\n",
    "\n",
    "        else:\n",
    "            # REGRESIÓN\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "            model_sel = XGBRegressor(**params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "            pred = model_sel.predict(X_va_sel)\n",
    "\n",
    "            if metric_to_use == \"r2\":\n",
    "                score = r2_score(y_valid, pred)\n",
    "            else:\n",
    "                raise ValueError(f\"Métrica de regresión no soportada: {metric_to_use}\")\n",
    "\n",
    "        results.append({\"cum_threshold\": thr, \"n_features\": len(cols), \"score\": score})\n",
    "\n",
    "    # Ordenar (si usamos logloss negado, mayor sigue siendo mejor)\n",
    "    res_df = pd.DataFrame(results).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    best_thr = float(res_df.iloc[0][\"cum_threshold\"])\n",
    "    return best_thr, res_df, imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0Dm8ZPGcBr7",
   "metadata": {
    "id": "R0Dm8ZPGcBr7"
   },
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "\n",
    "    # Solo numéricos para evitar errores y acelerar\n",
    "    corr_matrix = df.corr(numeric_only=True).abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1))\n",
    "\n",
    "    to_drop = []\n",
    "    for col in tqdm(upper.columns, desc=f\"Pruning corr > {threshold}\", unit=\"col\", leave=False):\n",
    "        if (upper[col] > threshold).any():\n",
    "            to_drop.append(col)\n",
    "\n",
    "    return df.drop(columns=to_drop, errors=\"ignore\"), to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WodcQEBJ_wAW",
   "metadata": {
    "id": "WodcQEBJ_wAW"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. PIPELINE PRINCIPAL =====================\n",
    "df = df.dropna()\n",
    "y = df['label']\n",
    "X = df.iloc[:, 2:]\n",
    "\n",
    "# --- 3.3 Split temporal (ejemplo simple 80/20) ---\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# --- 3.4 Remove correlated features ---\n",
    "X_train_filtered, dropped_features = remove_highly_correlated_features(X_train, threshold=0.9)\n",
    "X_test_filtered = X_test.drop(columns=dropped_features)\n",
    "\n",
    "# Baseline logistic regression with time-series CV\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train_filtered)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "baseline = cross_val_score(LogisticRegression(max_iter=1000), X_scaled, y_train, cv=tscv).mean()\n",
    "print('Logistic regression CV accuracy:', baseline)\n",
    "\n",
    "# --- 3.5 Importancias con XGBoost ---\n",
    "imp_df, xgb_model = compute_xgb_importance(X_train_filtered, y_train, task='classification')\n",
    "\n",
    "print('=== Importancias XGBoost ===')\n",
    "print(imp_df.head(20))\n",
    "print(f'Total features: {len(imp_df)}')\n",
    "\n",
    "# --- 3.6 Selección (elige una opción) ---\n",
    "X_train_sel, keep_cols = select_features_with_importance(X_train_filtered, imp_df, cum_threshold=0.8)\n",
    "X_test_sel = X_test_filtered[keep_cols]\n",
    "\n",
    "print(f'Features seleccionadas: {len(keep_cols)}')\n",
    "importance_map = imp_df.set_index(\"feature\")[\"importance\"]\n",
    "selected_importances = pd.DataFrame({\n",
    "    \"feature\": keep_cols,\n",
    "    \"importance\": importance_map.reindex(keep_cols).values\n",
    "})\n",
    "selected_importances.to_csv(root_data+'Results/'+symbol+'_'+direction+'_M5M10_'+data_type+'_ImportantCols.csv', index=False)\n",
    "\n",
    "# Save dataset with selected features\n",
    "df_selected = df[['Date', 'label'] + keep_cols]\n",
    "df_selected.to_csv(root_data+'Results/'+symbol+'_'+direction+'_M5M10_'+data_type+'_Features.csv', index=False)\n",
    "\n",
    "# Time-series cross-validation with XGBoost\n",
    "xgb_cv = XGBClassifier(eval_metric='logloss', n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "xgb_scores = cross_val_score(xgb_cv, X_train_sel, y_train, cv=tscv, scoring='accuracy')\n",
    "print('XGBoost CV accuracy:', xgb_scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ysa-7eLvEMpE",
    "m8CIB8vltFfH",
    "y6QRdBKwrX98",
    "zhTndYVi1TEV",
    "jh9-mi26wsya",
    "Hv5bsrpc03z7",
    "2RFfhHT2AwAJ"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
