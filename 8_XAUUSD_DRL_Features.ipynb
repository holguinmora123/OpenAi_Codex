{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf83164c",
   "metadata": {
    "id": "bf83164c"
   },
   "source": [
    "# Pendientes\n",
    "\n",
    "Nada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ysa-7eLvEMpE",
   "metadata": {
    "id": "Ysa-7eLvEMpE"
   },
   "source": [
    "# Gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m8CIB8vltFfH",
   "metadata": {
    "id": "m8CIB8vltFfH"
   },
   "source": [
    "# Set_Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6QRdBKwrX98",
   "metadata": {
    "id": "y6QRdBKwrX98"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1a01c",
   "metadata": {
    "id": "bda1a01c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, r2_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EB5RqjoAtFwl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB5RqjoAtFwl",
    "outputId": "22f9d4bc-f030-4a39-a382-54063ace02db"
   },
   "outputs": [],
   "source": [
    "DEFAULT_DATA_ROOT = os.environ.get(\n",
    "    \"XAUUSD_DATA_ROOT\",\n",
    "    \"/content/drive/MyDrive/Course Folder/Forex/XAUUSD\"\n",
    ")\n",
    "root_data = Path(DEFAULT_DATA_ROOT).expanduser()\n",
    "results_dir = root_data / \"Results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {root_data}\")\n",
    "\n",
    "direction = 'Short'\n",
    "direction_number = -1\n",
    "\n",
    "symbol = 'XAUUSD'\n",
    "strategy = 'Kalman'\n",
    "time_frame = 'M5'\n",
    "\n",
    "trade_evolution = 'st_Max'\n",
    "result_field = 'st_PnL'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7JweuZy755ym",
   "metadata": {
    "id": "7JweuZy755ym"
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa65d54",
   "metadata": {
    "id": "9aa65d54"
   },
   "source": [
    "# Calculate_Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I23127P7lBq_",
   "metadata": {
    "id": "I23127P7lBq_"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kp4yJdGAjoeA",
   "metadata": {
    "id": "kp4yJdGAjoeA"
   },
   "outputs": [],
   "source": [
    "def kalman_line(source, kalman_length: int, smooth: int):\n",
    "    \"\"\"\n",
    "    Pine -> Python (solo 'kalman_line'), replicando la EMA de TradingView con\n",
    "    *semilla SMA* (como ta.ema) sobre el n\u00facleo Kalman kf_c.\n",
    "\n",
    "    Par\u00e1metros\n",
    "    ----------\n",
    "    source : pd.Series o array-like de floats (precio crudo, sin diff/returns)\n",
    "    kalman_length : int   (equivale a length_kal en Pine)\n",
    "    smooth : int          (equivale a smooth_kal en Pine -> ta.ema(kf_c, smooth))\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    Mismo tipo que `source`: pd.Series o np.ndarray con la l\u00ednea Kalman suavizada.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # normalizamos tipos\n",
    "    is_series = hasattr(source, \"index\")\n",
    "    idx = source.index if is_series else None\n",
    "    x = np.asarray(source, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if n == 0:\n",
    "        return source\n",
    "\n",
    "    # ---------- n\u00facleo Kalman id\u00e9ntico al Pine ----------\n",
    "    sqrt_term   = np.sqrt((kalman_length / 10000.0) * 2.0)\n",
    "    length_term = kalman_length / 10000.0\n",
    "\n",
    "    kf_c   = np.empty(n, dtype=np.float64)\n",
    "    velo_c = np.empty(n, dtype=np.float64)\n",
    "\n",
    "    # bar 0 (nz(kf_c[1], source) y nz(velo_c[1], 0))\n",
    "    kf_c[0] = x[0]\n",
    "    velo_c[0] = 0.0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        prev_kf = kf_c[i - 1]\n",
    "        dk_c = x[i] - prev_kf\n",
    "        smooth_c = prev_kf + dk_c * sqrt_term\n",
    "        velo_c[i] = velo_c[i - 1] + length_term * dk_c\n",
    "        kf_c[i] = smooth_c + velo_c[i]\n",
    "\n",
    "    # ---------- EMA con semilla SMA (comportamiento ta.ema de TV) ----------\n",
    "    L = int(max(1, smooth))\n",
    "    alpha = 2.0 / (L + 1.0)\n",
    "    ema = np.full(n, np.nan, dtype=np.float64)\n",
    "\n",
    "    if n < L:\n",
    "        # con pocas barras, igualamos al promedio simple disponible\n",
    "        ema[-1] = np.nanmean(kf_c)\n",
    "    else:\n",
    "        # seed = SMA de las primeras L barras\n",
    "        seed = np.mean(kf_c[:L])\n",
    "        ema[L - 1] = seed\n",
    "        for i in range(L, n):\n",
    "            ema[i] = alpha * kf_c[i] + (1.0 - alpha) * ema[i - 1]\n",
    "\n",
    "    return (pd.Series(ema, index=idx) if is_series else ema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e51fb",
   "metadata": {
    "id": "e07e51fb"
   },
   "outputs": [],
   "source": [
    "def create_features(\n",
    "    stock_data: pd.DataFrame,\n",
    "    return_components: bool = False\n",
    ") -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n",
    "\n",
    "    kalman_periods = [300, 600, 900]\n",
    "    kalman_smooth_kal = 3\n",
    "\n",
    "    component_frames: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    def _unique_pairwise(columns: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Return ordered unique column pairs without self-pairings.\"\"\"\n",
    "        unique_columns = list(dict.fromkeys(columns))\n",
    "        return list(combinations(unique_columns, 2))\n",
    "\n",
    "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Kalman y derivados \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    t0 = time.time()\n",
    "    kal_cols = []\n",
    "    kalman_features = pd.DataFrame(index=stock_data.index)\n",
    "    kalman_900_series: Optional[pd.Series] = None\n",
    "    for period in tqdm(kalman_periods, desc=\"Kalman & Derivatives\"):\n",
    "        kal = pd.Series(\n",
    "            kalman_line(stock_data['Close'], kalman_length=period, smooth=kalman_smooth_kal),\n",
    "            index=stock_data.index\n",
    "        )\n",
    "\n",
    "        if kal.isna().any():\n",
    "            kal = kal.ffill()\n",
    "            if kal.isna().any():\n",
    "                kal = kal.bfill()\n",
    "        kname = f'Kal_{period}'\n",
    "        kal_cols.append(kname)\n",
    "\n",
    "        kalman_features[kname] = kal\n",
    "        if period == 900:\n",
    "            kalman_900_series = kal\n",
    "\n",
    "    tqdm.write(f\"[Timing] Kalman block: {time.time()-t0:.2f}s\")\n",
    "    component_frames['Kalman'] = kalman_features.copy()\n",
    "    features = kalman_features.copy()\n",
    "\n",
    "    if return_components:\n",
    "        component_frames['Create_Features'] = features.copy()\n",
    "        return features, component_frames\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24wl8snRkxUz",
   "metadata": {
    "id": "24wl8snRkxUz"
   },
   "outputs": [],
   "source": [
    "def scale_minmax(features: pd.DataFrame, window: int) -> pd.DataFrame:\n",
    "    \"\"\"Apply Min-Max scaling using a rolling window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    window : int\n",
    "        Rolling window size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Min-Max scaled feature block preserving the original index.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=np.number).columns\n",
    "    if numeric_cols.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    scaled_features = pd.DataFrame(index=features.index)\n",
    "    for col in numeric_cols:\n",
    "        rolling = features[col].rolling(window=window, min_periods=window)\n",
    "        rolling_min = rolling.min()\n",
    "        rolling_max = rolling.max()\n",
    "\n",
    "        range_ = (rolling_max - rolling_min).replace(0, np.nan)\n",
    "        scaled_features[col] = (features[col] - rolling_min) / range_\n",
    "\n",
    "    return scaled_features\n",
    "\n",
    "def apply_minmax_scaling(\n",
    "    features: pd.DataFrame,\n",
    "    windows: Tuple[int, ...] = (3,),\n",
    "    prefix: str = \"a_minimal_\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply Min-Max scaling using multiple rolling window lengths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    windows : Tuple[int, ...], optional\n",
    "        Collection of rolling window sizes to apply, by default (3).\n",
    "    prefix : str, optional\n",
    "        Prefix to add to each generated Min-Max column, by default \"a_minimal_\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with Min-Max scaled features for the requested windows.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    scaled_blocks = []\n",
    "    for window in windows:\n",
    "        if window <= 1:\n",
    "            continue\n",
    "        window_scaled = scale_minmax(features, window=window)\n",
    "        if window_scaled.empty:\n",
    "            continue\n",
    "        rename_map = {col: f\"{prefix}{col}_minmax_{window}\" for col in window_scaled.columns}\n",
    "        scaled_blocks.append(window_scaled.rename(columns=rename_map))\n",
    "\n",
    "    if not scaled_blocks:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    return pd.concat(scaled_blocks, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c0f79",
   "metadata": {
    "id": "1e0c0f79"
   },
   "outputs": [],
   "source": [
    "def scale_feature_block(features: pd.DataFrame, window: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Scale features using a rolling window standardization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    window : int, optional\n",
    "        Rolling window size, by default 200.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Scaled feature block preserving the original index.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return features.copy()\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=np.number).columns\n",
    "    scaled_features = features.copy()\n",
    "\n",
    "    if not numeric_cols.empty:\n",
    "        rolling = features[numeric_cols].rolling(window=window, min_periods=window)\n",
    "        mean = rolling.mean()\n",
    "        std = rolling.std()\n",
    "\n",
    "        std = std.replace(0, np.nan)\n",
    "\n",
    "        scaled_features[numeric_cols] = (features[numeric_cols] - mean) / std\n",
    "\n",
    "    return scaled_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DoqqPJxPxnBF",
   "metadata": {
    "id": "DoqqPJxPxnBF"
   },
   "source": [
    "## 5_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7l7vt7QxvyC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "d7l7vt7QxvyC",
    "outputId": "8c339745-3782-49af-b418-4e3b3950026f"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df_5min = pd.read_csv(root_data / 'Data' / f\"{symbol}_M5.csv\", index_col=0)\n",
    "df_5min.index = pd.to_datetime(df_5min.index)\n",
    "#df_5min = df_5min.iloc[-10000:,]\n",
    "\n",
    "print('Min_Date : ', df_5min.index.min())\n",
    "print('Min_Date : ', df_5min.index.max())\n",
    "print('Number_Rows = ',len(df_5min.index))\n",
    "print('\\n')\n",
    "\n",
    "df_5min.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HBicVZcDx2CF",
   "metadata": {
    "id": "HBicVZcDx2CF"
   },
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XxU4wllsEUcN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "dc38b92328b143beb0ca760d8c8e6859",
      "e14a885c7dc4465fbef1959b6b719147",
      "d406ea993e074aceb7c9997d4e6afa1b",
      "37471a2d01464d7884e2bb52ac2645c7",
      "3dd3464d252b4559ae36b1f71964ca8d",
      "4ae877086206461e9f54280a50304361",
      "93b234553dd24e6f82efea30c21f6063",
      "93acf3715d864a2cbf793b0bdafd96d4",
      "3713d638e82c414291abacb4508c375e",
      "3460b91edd1f41acbced602c41d85dce",
      "cc9c9572ab9b4e258ab7396f930c8e95"
     ]
    },
    "id": "XxU4wllsEUcN",
    "outputId": "b59d116b-dfb0-4e09-b2af-b3e7b6224ea0"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "features_5min_raw, components_5min = create_features(df_5min, return_components=True)\n",
    "valid_index_5min = features_5min_raw.index\n",
    "\n",
    "m5_raw_blocks: Dict[str, pd.DataFrame] = {name: frame.loc[valid_index_5min].copy()\n",
    "                                           for name, frame in components_5min.items()}\n",
    "m5_raw_blocks['Create_Features'] = features_5min_raw\n",
    "\n",
    "m5_scaled_blocks: Dict[str, pd.DataFrame] = {}\n",
    "m5_raw_paths: Dict[str, str] = {}\n",
    "m5_scaled_paths: Dict[str, str] = {}\n",
    "\n",
    "for name, frame in m5_raw_blocks.items():\n",
    "    filename_raw = f\"{symbol}_M5_{name}_Raw_Features.csv\" if name != 'Create_Features' else f\"{symbol}_M5_Raw_Features.csv\"\n",
    "    raw_path = results_dir / filename_raw\n",
    "    frame.to_csv(raw_path)\n",
    "    m5_raw_paths[name] = raw_path\n",
    "\n",
    "    frame_filled = frame.ffill()\n",
    "\n",
    "    # Apply specific scaling and min-max scaling to Kalman features\n",
    "    kalman_features_to_scale = frame_filled[['Kal_300', 'Kal_600', 'Kal_900']].copy()\n",
    "    scaled_kalman = scale_feature_block(kalman_features_to_scale, window=900)\n",
    "    minmax_kalman = apply_minmax_scaling(kalman_features_to_scale, windows=(3,), prefix=\"minmax_\")\n",
    "\n",
    "\n",
    "    # Rename columns with specified prefixes\n",
    "    scaled_kalman = scaled_kalman.rename(columns={col: f\"scale_{col}\" for col in scaled_kalman.columns})\n",
    "    minmax_kalman = minmax_kalman.rename(columns={col: f\"{col}\" for col in minmax_kalman.columns})\n",
    "\n",
    "    # Calculate and add the difference for scale_Kal_300\n",
    "    if 'scale_Kal_300' in scaled_kalman.columns:\n",
    "        scaled_kalman['scale_Kal_300_diff'] = scaled_kalman['scale_Kal_300'].diff()\n",
    "        # ******** Apply Kalman filter to the difference ********\n",
    "        scaled_kalman['kalman_scale_Kal_300_diff'] = pd.Series(\n",
    "            kalman_line(scaled_kalman['scale_Kal_300_diff'], kalman_length=300, smooth=3), # Using example parameters\n",
    "            index=scaled_kalman.index\n",
    "        )\n",
    "        # Forward fill any NaNs introduced by the Kalman filter\n",
    "        scaled_kalman['kalman_scale_Kal_300_diff'] = scaled_kalman['kalman_scale_Kal_300_diff'].ffill()\n",
    "\n",
    "\n",
    "    # Combine scaled and min-max features with the rest of the frame\n",
    "    # Add the original 'Close' column to the combined_scaled DataFrame\n",
    "    combined_scaled = frame_filled.drop(columns=['Kal_300', 'Kal_600', 'Kal_900']).copy() # Drop original Kalman\n",
    "    combined_scaled = pd.concat([combined_scaled, scaled_kalman, minmax_kalman, df_5min[['Close']].loc[frame_filled.index]], axis=1) # Add scaled, minmax, and Close\n",
    "\n",
    "\n",
    "    m5_scaled_blocks[name] = combined_scaled\n",
    "    filename_scale = f\"{symbol}_M5_{name}_Scale_Features.csv\" if name != 'Create_Features' else f\"{symbol}_M5_Scale_Features.csv\"\n",
    "    scale_path = results_dir / filename_scale\n",
    "    combined_scaled.to_csv(scale_path)\n",
    "    m5_scaled_paths[name] = scale_path\n",
    "\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Number of features are: {features_5min_raw.shape[1]}\")\n",
    "print(features_5min_raw.shape)\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "print(f\"Saved {len(m5_raw_blocks) * 2} DataFrames for the M5 timeframe.\")\n",
    "features_5min_raw.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Vw_2C-ZEO57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Vw_2C-ZEO57",
    "outputId": "57c48eb1-76ad-4639-c8dd-4f937510ab2f"
   },
   "outputs": [],
   "source": [
    "print(\"NaN counts per column (sorted):\")\n",
    "print(m5_raw_blocks['Create_Features'].isnull().sum().sort_values(ascending=False), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4AwNJic6icv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4AwNJic6icv",
    "outputId": "2ff1a751-7eaf-402d-e9c0-56e1012d355c"
   },
   "outputs": [],
   "source": [
    "print(\"Saved raw feature files:\")\n",
    "for name, path in m5_raw_paths.items():\n",
    "    print(f\" - {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edtpuJ4Y-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71edtpuJ4Y-E",
    "outputId": "7ddf2634-57b9-4245-c2c5-a62a87ab545a"
   },
   "outputs": [],
   "source": [
    "print(list(features_5min_raw.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QX4cSq3Ftqhm",
   "metadata": {
    "id": "QX4cSq3Ftqhm"
   },
   "source": [
    "**Scale_features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qEb1Cz1vzj6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qEb1Cz1vzj6",
    "outputId": "bef02d47-fb72-40f7-bf7a-1140d6c08a1b"
   },
   "outputs": [],
   "source": [
    "print(\"Saved scaled feature files:\")\n",
    "for name, path in m5_scaled_paths.items():\n",
    "    print(f\" - {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eJcGUESttqhm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "eJcGUESttqhm",
    "outputId": "35134ca6-52c6-4fb7-9fb8-8e29b6d89361"
   },
   "outputs": [],
   "source": [
    "m5_scaled_blocks['Create_Features'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BFVUCAe3GWFy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFVUCAe3GWFy",
    "outputId": "74429f84-f3ef-413c-85d1-f7a2f9658aa6"
   },
   "outputs": [],
   "source": [
    "print(\"NaN counts per column (scaled, sorted):\")\n",
    "print(m5_scaled_blocks['Create_Features'].isnull().sum().sort_values(ascending=False), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aXMGezVftqhn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXMGezVftqhn",
    "outputId": "1a850c96-ea9e-449d-acb7-f2d247ec0c14"
   },
   "outputs": [],
   "source": [
    "print(\"M5 feature blocks:\", list(m5_raw_blocks.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1Pk4Hkfjhvt",
   "metadata": {
    "id": "W1Pk4Hkfjhvt"
   },
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n# Define the row range for plotting\nstart_row = 8000\nend_row = 8500\n\n# Select the data for the specified row range from the scaled features\nplot_df = m5_scaled_blocks['Create_Features'].iloc[start_row:end_row].copy()\nplot_df['Date'] = plot_df.index # Add Date column from index for plotting\n\n# Define the columns to plot in each chart\nchart1_cols = ['Close']\nchart2_cols = ['scale_Kal_300', 'scale_Kal_600', 'scale_Kal_900']\nchart3_cols = ['minmax_Kal_300_minmax_3', 'minmax_Kal_600_minmax_3', 'minmax_Kal_900_minmax_3']\nchart4_cols = ['scale_Kal_300_diff']\n\n\n# Determine the number of subplots (only include charts with available columns in plot_df)\nnum_plots = 0\nif any(col in plot_df.columns for col in chart1_cols): num_plots += 1\nif any(col in plot_df.columns for col in chart2_cols): num_plots += 1\nif any(col in plot_df.columns for col in chart3_cols): num_plots += 1\nif any(col in plot_df.columns for col in chart4_cols): num_plots += 1\n\n\n# Create the subplots\nfig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(15, 4 * num_plots), sharex=True)\n\n# Ensure axes is an array even if only one plot\nif not isinstance(axes, np.ndarray):\n    axes = np.array([axes])\n\ncurrent_plot_index = 0\n\n# Plot Chart 1: Close Price\nif any(col in plot_df.columns for col in chart1_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart1_cols])\n    axes[current_plot_index].set_title('Close Price')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart1_cols)\n    current_plot_index += 1\n\n\n# Plot Chart 2: M5 Scale Kalman indicators\nif any(col in plot_df.columns for col in chart2_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart2_cols])\n    axes[current_plot_index].set_title('M5 Scale Kalman')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart2_cols)\n    current_plot_index += 1\n\n# Plot Chart 4: M5 Scale Kalman Diff indicator\nif any(col in plot_df.columns for col in chart4_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart4_cols])\n    axes[current_plot_index].set_title('M5 Scale Kalman Diff (scale_Kal_300_diff)')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart4_cols)\n    current_plot_index += 1\n\n\n# Plot Chart 3: M5 Scale Kalman Change indicators\nif any(col in plot_df.columns for col in chart3_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart3_cols])\n    axes[current_plot_index].set_title('M5 Scale Kalman Change (MinMax 3)')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart3_cols)\n    current_plot_index += 1\n\nplt.tight_layout()\nplt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wYpp7GEBEing",
    "outputId": "11dcfec9-c53c-4819-ad31-5b60a7028258"
   },
   "id": "wYpp7GEBEing",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "A5c__w6s_dtY",
   "metadata": {
    "id": "A5c__w6s_dtY"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4j7J8tk_f3o-",
   "metadata": {
    "id": "4j7J8tk_f3o-"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6XOsumPycYz3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "6XOsumPycYz3",
    "outputId": "50edb87e-e3a1-4c6f-82dc-23a53b93d0c1"
   },
   "outputs": [],
   "source": [
    "lab = pd.read_csv(results_dir / f\"{symbol}_{strategy}_{time_frame}_Strategy_Gen_Labels.csv\", index_col=0)\n",
    "lab['Date'] = pd.to_datetime(lab['Date'])\n",
    "\n",
    "print('Min_Date    : ',lab['Date'].min())\n",
    "print('Min_Date    : ',lab['Date'].max(),'\\n')\n",
    "print('Number_Rows : ',lab.shape,'\\n')\n",
    "print('Columns     : ',lab.columns)\n",
    "\n",
    "lab['Open_Trade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apV2tezPsbux",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apV2tezPsbux",
    "outputId": "b24f897b-9a54-412d-8456-af9b4efa506f"
   },
   "outputs": [],
   "source": [
    "#analyse_column = 'st_atr_max_PnL'\n",
    "analyse_column = 'st_Max'\n",
    "\n",
    "st_max_0  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
    "st_max_1  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
    "st_max_2  = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].sum()\n",
    "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].sum()\n",
    "\n",
    "\n",
    "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 0)),analyse_column].sum()\n",
    "\n",
    "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 0.7) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].sum()\n",
    "\n",
    "st_max_5 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 1) &\n",
    "                   (lab['st_atr_max_PnL'] <= 1.5)),analyse_column].sum()\n",
    "\n",
    "st_max_6 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 1.5) &\n",
    "                   (lab['st_atr_max_PnL'] <= 2)),analyse_column].sum()\n",
    "\n",
    "st_max_7 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] > 2)),analyse_column].sum()\n",
    "\n",
    "\n",
    "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
    "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
    "print(f'Above_Mean = {st_max_2:,.2f}')\n",
    "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
    "\n",
    "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
    "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
    "print(f'> 1 & <= 1.5 = {st_max_5:,.2f}')\n",
    "print(f'> 1.5 & <= 2 = {st_max_6:,.2f}')\n",
    "print(f'> 2 = {st_max_7:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2fjs4Si792v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2fjs4Si792v",
    "outputId": "e9bcf045-242b-4e55-c6e4-68246d1422ec"
   },
   "outputs": [],
   "source": [
    "analyse_column = 'st_atr_max_PnL'\n",
    "\n",
    "st_max_0 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),:]['Open_Trade'].count()\n",
    "st_max_1 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)),analyse_column].mean()\n",
    "st_max_2 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] >= 1.93),analyse_column].count()\n",
    "st_max_25 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) & (lab['st_atr_max_PnL'] <= 1.93),analyse_column].count()\n",
    "\n",
    "st_max_3 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                    (lab['st_atr_max_PnL'] <= 0.5)),analyse_column].count()\n",
    "\n",
    "st_max_4 = lab.loc[(((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                     (lab['st_atr_max_PnL'] >= 0.5) & (lab['st_atr_max_PnL'] <= 1)),analyse_column].count()\n",
    "\n",
    "st_max_5 = lab.loc[((lab['Open_Trade']==1) |(lab['Open_Trade']==-1)) &\n",
    "                   (lab['st_atr_max_PnL'] >= 1), analyse_column].count()\n",
    "\n",
    "print(f'Total_Trades = {st_max_0:,.0f}\\n')\n",
    "print(f'Mean st_atr_max_PnL = {st_max_1:,.2f}\\n')\n",
    "print(f'Above_Mean = {st_max_2:,.2f}')\n",
    "print(f'Below_Mean = {st_max_25:,.2f}\\n')\n",
    "\n",
    "print(f'<= 0.5 = {st_max_3:,.2f}')\n",
    "print(f'> 0.5 & <= 1 = {st_max_4:,.2f}')\n",
    "print(f'> 1 = {st_max_5:,.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85DATjQqZd66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85DATjQqZd66",
    "outputId": "6bf48cea-2d20-4a1b-9d5c-acd3f3ab7edd"
   },
   "outputs": [],
   "source": [
    "# --- Par\u00e1metros / campos\n",
    "result_field = 'st_atr_max_PnL'\n",
    "\n",
    "#valid = (\n",
    "#    (lab['Type'] == direction) &\n",
    "#    (lab['Open_Trade'].isin([1, -1])) &\n",
    "#    (lab[result_field].notna()))\n",
    "\n",
    "valid = (\n",
    "    (lab['Open_Trade'].isin([1, -1])) &\n",
    "    (lab[result_field].notna()))\n",
    "\n",
    "\n",
    "# --- Etiquetado en la columna \"label\" con valores 4/5/6\n",
    "lab['label'] = np.nan\n",
    "lab.loc[valid & (lab[result_field] <= 1), 'label'] = 0\n",
    "lab.loc[valid & (lab[result_field] >= 1), 'label'] = 1\n",
    "\n",
    "\n",
    "# --- Mantener solo filas v\u00e1lidas y con label\n",
    "lab = lab.loc[valid & lab['label'].notna()].copy()\n",
    "lab['label'] = lab['label'].astype('int8')\n",
    "\n",
    "# --- Ver distribuci\u00f3n de labels 4/5/6\n",
    "print('\\nValue counts de label 4/5/6:')\n",
    "print(lab['label'].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_BCdqt3Y0Fjm",
   "metadata": {
    "id": "_BCdqt3Y0Fjm"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gX04ZTmfz-pK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gX04ZTmfz-pK",
    "outputId": "8ceb3939-8d03-4bea-b65c-7b7a890f5df0"
   },
   "outputs": [],
   "source": [
    "raw_feat_5min = pd.read_csv(results_dir / f\"{symbol}_M5_Raw_Features.csv\")\n",
    "raw_feat_5min[\"Date\"] = pd.to_datetime(raw_feat_5min[\"Date\"])\n",
    "print(raw_feat_5min.shape)\n",
    "#raw_feat_5min.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5gxf0ke0KUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5gxf0ke0KUx",
    "outputId": "355c5b0b-e225-4420-c199-cb88f596b5b5"
   },
   "outputs": [],
   "source": [
    "scale_feat_5min = pd.read_csv(results_dir / f\"{symbol}_M5_Scale_Features.csv\")\n",
    "#scale_feat_5min = scale_feat_5min.drop('Unnamed: 0', axis=1)\n",
    "scale_feat_5min[\"Date\"] = pd.to_datetime(scale_feat_5min[\"Date\"])\n",
    "print(scale_feat_5min.shape)\n",
    "#scale_feat_5min.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hv5bsrpc03z7",
   "metadata": {
    "id": "Hv5bsrpc03z7"
   },
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mc1y2NbRM_f1",
   "metadata": {
    "id": "Mc1y2NbRM_f1"
   },
   "outputs": [],
   "source": [
    "data_type = 'Scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I0gkNVT60Ka5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0gkNVT60Ka5",
    "outputId": "3e512c2d-82fb-43cd-8ee0-a3ce50cd3146"
   },
   "outputs": [],
   "source": [
    "def prepare_feature_block(frame: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of the feature block with a consistent prefix.\"\"\"\n",
    "    block = frame.copy()\n",
    "    if 'Unnamed: 0' in block.columns:\n",
    "        block = block.drop(columns='Unnamed: 0')\n",
    "    block = block.drop_duplicates(subset='Date').sort_values('Date')\n",
    "    rename_map = {col: f\"{prefix}{col}\" for col in block.columns if col != 'Date'}\n",
    "    block = block.rename(columns=rename_map)\n",
    "    return block\n",
    "\n",
    "feature_blocks = {\n",
    "    'M5_raw': prepare_feature_block(raw_feat_5min, 'M5_raw_'),\n",
    "    'M5_scale': prepare_feature_block(scale_feat_5min, 'M5_scale_'),\n",
    "}\n",
    "\n",
    "feature_df = lab[['Date']].drop_duplicates().copy()\n",
    "\n",
    "print('Feature blocks merged:')\n",
    "for name, block in feature_blocks.items():\n",
    "    feature_df = feature_df.merge(block, on='Date', how='left')\n",
    "    print(f\" - {name}: {block.shape[1] - 1} columns\")\n",
    "\n",
    "feature_df = feature_df.sort_values('Date').set_index('Date').ffill().reset_index()\n",
    "\n",
    "df = lab[['Date', 'Close','label', 'Open_Trade','kal_1', 'kal_2', 'kal_3', 'kal_4']].merge(feature_df, on='Date', how='left')\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('label')\n",
    "cols.insert(1, 'label')\n",
    "df = df[cols]\n",
    "\n",
    "combined_feature_path = results_dir / f\"{symbol}_{direction}_AllFeatures.csv\"\n",
    "df.to_csv(combined_feature_path, index=False)\n",
    "\n",
    "print(f\"Combined feature dataframe shape: {df.shape}\")\n",
    "print(f\"Saved combined feature dataframe to: {combined_feature_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQPfQroWYLRM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQPfQroWYLRM",
    "outputId": "e92ec6d4-53b4-4dd5-c017-436eb7dc9031"
   },
   "outputs": [],
   "source": [
    "expected_prefixes = ['M5_raw_', 'M5_scale_']\n",
    "for prefix in expected_prefixes:\n",
    "    matching_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    print(f\"{prefix}: {len(matching_cols)} columns\")\n",
    "\n",
    "duplicated_columns = df.columns[df.columns.duplicated()].tolist()\n",
    "if duplicated_columns:\n",
    "    print('Duplicated feature columns detected:', duplicated_columns)\n",
    "else:\n",
    "    print('No duplicated feature columns detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jXcXWty506Ur",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXcXWty506Ur",
    "outputId": "45e59bdd-d63a-4b9e-df5b-c239900c0750"
   },
   "outputs": [],
   "source": [
    "print(df.shape,'\\n')\n",
    "print('Label_Counts : ',df.label.value_counts(),'\\n')\n",
    "print(list(df.columns), '\\n')\n",
    "\n",
    "# Add NaN count per column, sorted\n",
    "print(\"NaN counts per column (sorted):\")\n",
    "print(df.isnull().sum().sort_values(ascending=False), '\\n')\n",
    "\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F-S1XcoyKbjY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-S1XcoyKbjY",
    "outputId": "27818672-9d56-4617-d796-29567713bd85"
   },
   "outputs": [],
   "source": [
    "print(list(df.columns), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d67f8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "06d67f8e",
    "outputId": "858ff676-2ee0-4469-9c7e-54a8b4f6f31c"
   },
   "outputs": [],
   "source": [
    "\n# Define the row range for plotting\nstart_row = 9200\nend_row = 9300\n\n# Select the data for the specified row range from df\nplot_df = df.iloc[start_row:end_row].copy()\n\n# Define the columns to plot in each chart\nchart1_cols = ['Close'] # Assuming 'Close' is now in df\nchart2_cols = ['M5_scale_Kal_300', 'M5_scale_Kal_600', 'M5_scale_Kal_900']\nchart3_cols = ['M5_scale_Close_Kal_Kal_300', 'M5_scale_Close_Kal_Kal_600', 'M5_scale_Close_Kal_Kal_900']\n\n\n# Determine the number of subplots (only include charts with available columns in plot_df)\nnum_plots = 0\nif any(col in plot_df.columns for col in chart1_cols): num_plots += 1\nif any(col in plot_df.columns for col in chart2_cols): num_plots += 1\nif any(col in plot_df.columns for col in chart3_cols): num_plots += 1\n\n\n# Create the subplots\nfig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(15, 4 * num_plots), sharex=True)\n\n# Ensure axes is an array even if only one plot\nif not isinstance(axes, np.ndarray):\n    axes = np.array([axes])\n\ncurrent_plot_index = 0\n\n# Plot Chart 1: Close Price\nif any(col in plot_df.columns for col in chart1_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart1_cols])\n    axes[current_plot_index].set_title('Close Price')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart1_cols)\n    current_plot_index += 1\n\n\n# Plot Chart 2: M5 Scale Kalman indicators\nif any(col in plot_df.columns for col in chart2_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart2_cols])\n    axes[current_plot_index].set_title('M5 Scale Kalman')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart2_cols)\n    current_plot_index += 1\n\n# Plot Chart 3: M5 Scale Kalman Change indicators\nif any(col in plot_df.columns for col in chart3_cols):\n    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart3_cols])\n    axes[current_plot_index].set_title('M5 Scale Kalman Change')\n    axes[current_plot_index].grid(True)\n    axes[current_plot_index].legend(chart3_cols)\n    current_plot_index += 1\n\n\nplt.tight_layout()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4TPpba7UChUY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TPpba7UChUY",
    "outputId": "c78f063c-91c0-43ed-d078-81beceb54b02"
   },
   "outputs": [],
   "source": [
    "### Align feature directions so that shorts mirror longs\n",
    "if 'Open_Trade' not in df.columns:\n",
    "    raise KeyError(\"'Open_Trade' column is required in df to flip feature signs.\")\n",
    "\n",
    "### Identify feature columns to flip (exclude identifiers/targets).\n",
    "feature_cols = [col for col in df.columns if col not in ['Date', 'label', 'Open_Trade', 'Close']] # Keep 'Close'\n",
    "short_mask = df['Open_Trade'] == -1\n",
    "\n",
    "if short_mask.any():\n",
    "    df.loc[short_mask, feature_cols] = df.loc[short_mask, feature_cols] * -1\n",
    "    print(f\"Flipped {short_mask.sum()} rows with Open_Trade = -1.\")\n",
    "else:\n",
    "    print(\"No rows with Open_Trade = -1 were found.\")\n",
    "\n",
    "### Reorder columns so Open_Trade stays next to the label for downstream steps.\n",
    "ordered_cols = ['Date', 'label', 'Open_Trade'] + [col for col in feature_cols]\n",
    "df = df[ordered_cols]\n",
    "\n",
    "# Remove columns with 'raw' or 'RSI' in their name from df, but keep 'Close'\n",
    "cols_to_drop = [col for col in df.columns if ('raw' in col or 'RSI' in col) and col != 'Close']\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "print(f\"Dropped {len(cols_to_drop)} columns containing 'raw' or 'RSI' (keeping 'Close').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pQa8_4GpvRRf",
   "metadata": {
    "id": "pQa8_4GpvRRf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lBtQecOO08zB",
   "metadata": {
    "id": "lBtQecOO08zB"
   },
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rlkmypd9DJj",
   "metadata": {
    "id": "8rlkmypd9DJj"
   },
   "outputs": [],
   "source": [
    "# ===================== 1. ENTRENAR Y OBTENER IMPORTANCIAS =====================\n",
    "def compute_xgb_importance(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    random_state: int = 42,\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[pd.DataFrame, Any]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost y devuelve:\n",
    "      - imp_df: DataFrame con 'feature', 'importance' y 'cum_importance'.\n",
    "      - model : modelo ya entrenado.\n",
    "\n",
    "    Soporta:\n",
    "      \u2022 Clasificaci\u00f3n binaria o multiclase (detecta n\u00ba de clases).\n",
    "      \u2022 Regresi\u00f3n (si task != 'classification').\n",
    "\n",
    "    Par\u00e1metros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de caracter\u00edsticas (sin la columna objetivo).\n",
    "    y : pd.Series\n",
    "        Etiquetas objetivo. Puede ser binaria (0/1) o multiclase (0..K-1).\n",
    "    task : str, opcional\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    random_state : int, opcional\n",
    "        Semilla para reproducibilidad.\n",
    "    **xgb_params : dict\n",
    "        Par\u00e1metros adicionales para el estimador de XGBoost.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (imp_df, model)\n",
    "        imp_df : DataFrame con importancias y su acumulado.\n",
    "        model  : instancia entrenada de XGBClassifier / XGBRegressor.\n",
    "    \"\"\"\n",
    "    default_params: Dict[str, Any] = dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    default_params.update(xgb_params)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        # Detectar n\u00ba de clases\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        # XGBClassifier ajusta objetivo autom\u00e1ticamente, pero lo explicitamos:\n",
    "        if n_classes > 2:\n",
    "            default_params.setdefault(\"objective\", \"multi:softprob\")\n",
    "            default_params.setdefault(\"num_class\", n_classes)\n",
    "            eval_metric = \"mlogloss\"\n",
    "        else:\n",
    "            default_params.setdefault(\"objective\", \"binary:logistic\")\n",
    "            eval_metric = \"logloss\"\n",
    "\n",
    "        model = XGBClassifier(eval_metric=eval_metric, **default_params)\n",
    "\n",
    "    else:\n",
    "        model = XGBRegressor(**default_params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance\": model.feature_importances_\n",
    "        })\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    total_imp = imp_df[\"importance\"].sum()\n",
    "    if total_imp == 0:\n",
    "        # Evitar divisi\u00f3n por cero si el modelo devuelve todo cero (raro, pero posible)\n",
    "        imp_df[\"cum_importance\"] = 0.0\n",
    "    else:\n",
    "        imp_df[\"cum_importance\"] = imp_df[\"importance\"].cumsum() / total_imp\n",
    "\n",
    "    return imp_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3r9Di4Xx9DOU",
   "metadata": {
    "id": "3r9Di4Xx9DOU"
   },
   "outputs": [],
   "source": [
    "# ===================== 2. SELECCI\u00d3N DE FEATURES =====================\n",
    "def select_features_with_importance(\n",
    "    X: pd.DataFrame,\n",
    "    imp_df: pd.DataFrame,\n",
    "    top_n: Optional[int] = None,\n",
    "    threshold: Optional[str | float] = None,\n",
    "    cum_threshold: Optional[float] = 0.8\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Selecci\u00f3n flexible de variables a partir de importancias de XGBoost.\n",
    "\n",
    "    Reglas:\n",
    "      - Si top_n no es None           => usa el top_n.\n",
    "      - Else si cum_threshold no None => usa importancia acumulada (p.ej. 0.8 = 80%).\n",
    "      - Else usa threshold ('median', 'mean' o valor num\u00e9rico).\n",
    "\n",
    "    Devuelve (X_reducido, lista_de_features).\n",
    "\n",
    "    Par\u00e1metros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de caracter\u00edsticas original.\n",
    "    imp_df : pd.DataFrame\n",
    "        DataFrame devuelto por compute_xgb_importance.\n",
    "    top_n : int | None\n",
    "        N\u00famero fijo de variables a conservar.\n",
    "    threshold : str | float | None\n",
    "        Umbral de importancia. Si str, usar 'median' o 'mean'.\n",
    "    cum_threshold : float | None\n",
    "        Porcentaje acumulado de importancia (0-1). Si None, se ignora.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_sel, keep)\n",
    "        X_sel : subset de X con columnas seleccionadas.\n",
    "        keep  : lista de nombres de columnas seleccionadas.\n",
    "    \"\"\"\n",
    "    if top_n is not None:\n",
    "        keep = imp_df.head(top_n)[\"feature\"].tolist()\n",
    "\n",
    "    elif cum_threshold is not None:\n",
    "        keep_mask = imp_df[\"cum_importance\"] <= float(cum_threshold)\n",
    "        keep = imp_df.loc[keep_mask, \"feature\"].tolist()\n",
    "        # asegurar que haya al menos una m\u00e1s para no quedarnos exactamente en el corte\n",
    "        if len(keep) < len(imp_df):\n",
    "            keep.append(imp_df.iloc[len(keep)][\"feature\"])\n",
    "\n",
    "    else:\n",
    "        if threshold is None:\n",
    "            threshold = \"median\"\n",
    "        if isinstance(threshold, str):\n",
    "            thr_val = imp_df[\"importance\"].agg(threshold)\n",
    "        else:\n",
    "            thr_val = float(threshold)\n",
    "        keep = imp_df.loc[imp_df[\"importance\"] >= thr_val, \"feature\"].tolist()\n",
    "\n",
    "    return X[keep], keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tCDZCTz2_v9z",
   "metadata": {
    "id": "tCDZCTz2_v9z"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. B\u00daSQUEDA DEL MEJOR UMBRAL ACUMULADO =====================\n",
    "def find_best_cum_threshold(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    thresholds: Tuple[float, ...] = (0.6, 0.7, 0.8, 0.9),\n",
    "    random_state: int = 42,\n",
    "    metric: str = \"auto\",\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[float, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Entrena un XGB en train, calcula importancias y prueba varios umbrales\n",
    "    acumulados para ver cu\u00e1l da la mejor m\u00e9trica en valid.\n",
    "\n",
    "    Para CLASIFICACI\u00d3N:\n",
    "        - Detecta n\u00ba de clases.\n",
    "        - M\u00e9trica por defecto (metric=\"auto\"):\n",
    "            \u2022 Binaria: ROC-AUC (probabilidades de la clase positiva).\n",
    "            \u2022 Multiclase: ROC-AUC macro OVR (usa predict_proba).\n",
    "          Alternativas: metric=\"f1_macro\", \"accuracy\", \"logloss\" (se MINIMIZA).\n",
    "    Para REGRESI\u00d3N:\n",
    "        - Usa R^2.\n",
    "\n",
    "    Devuelve:\n",
    "        best_thr, res_df_ordenado_por_score_desc, imp_df\n",
    "\n",
    "    Par\u00e1metros\n",
    "    ----------\n",
    "    X_train, y_train, X_valid, y_valid : pd.DataFrame / pd.Series\n",
    "        Particiones de entrenamiento y validaci\u00f3n.\n",
    "    task : str\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    thresholds : tuple[float, ...]\n",
    "        Valores de umbral de importancia acumulada a evaluar (0-1).\n",
    "    random_state : int\n",
    "        Semilla para reproducibilidad.\n",
    "    metric : str\n",
    "        \"auto\" (default), \"roc_auc\", \"f1_macro\", \"accuracy\", \"logloss\" (clasif) o \"r2\" (regresi\u00f3n).\n",
    "    **xgb_params : dict\n",
    "        Par\u00e1metros extra para el estimador de XGBoost (pasan a compute y a los modelos internos).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (best_thr, res_df, imp_df)\n",
    "        best_thr : float\n",
    "            Umbral con mejor score (o menor logloss si metric='logloss').\n",
    "        res_df : pd.DataFrame\n",
    "            Tabla con resultados por umbral (n_features, score).\n",
    "        imp_df : pd.DataFrame\n",
    "            Importancias calculadas en X_train / y_train.\n",
    "    \"\"\"\n",
    "    imp_df, _ = compute_xgb_importance(\n",
    "        X_train, y_train, task=task, random_state=random_state, **xgb_params\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Detectar n\u00ba de clases si es clasificaci\u00f3n\n",
    "    if task == \"classification\":\n",
    "        classes = np.unique(y_train)\n",
    "        n_classes = len(classes)\n",
    "        if metric == \"auto\":\n",
    "            metric_to_use = \"roc_auc\" if n_classes == 2 else \"roc_auc\"\n",
    "        else:\n",
    "            metric_to_use = metric\n",
    "    else:\n",
    "        metric_to_use = \"r2\" if metric == \"auto\" else metric\n",
    "\n",
    "    for thr in thresholds:\n",
    "        X_tr_sel, cols = select_features_with_importance(\n",
    "            X_train, imp_df, cum_threshold=thr, top_n=None, threshold=None\n",
    "        )\n",
    "        X_va_sel = X_valid[cols]\n",
    "\n",
    "        if task == \"classification\":\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "\n",
    "            if n_classes > 2:\n",
    "                params.setdefault(\"objective\", \"multi:softprob\")\n",
    "                params.setdefault(\"num_class\", n_classes)\n",
    "                eval_metric = \"mlogloss\"\n",
    "            else:\n",
    "                params.setdefault(\"objective\", \"binary:logistic\")\n",
    "                eval_metric = \"logloss\"\n",
    "\n",
    "            model_sel = XGBClassifier(eval_metric=eval_metric, **params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "\n",
    "            # Probabilidades y predicciones\n",
    "            proba = model_sel.predict_proba(X_va_sel)\n",
    "            pred  = np.argmax(proba, axis=1) if n_classes > 2 else (proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "            # Calcular m\u00e9trica\n",
    "            if metric_to_use == \"roc_auc\":\n",
    "                if n_classes == 2:\n",
    "                    score = roc_auc_score(y_valid, proba[:, 1])\n",
    "                else:\n",
    "                    # AUC macro One-vs-Rest\n",
    "                    score = roc_auc_score(y_valid, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "            elif metric_to_use == \"f1_macro\":\n",
    "                score = f1_score(y_valid, pred, average=\"macro\")\n",
    "            elif metric_to_use == \"accuracy\":\n",
    "                score = accuracy_score(y_valid, pred)\n",
    "            elif metric_to_use == \"logloss\":\n",
    "                # En este caso, menor es mejor. Guardamos negativo para mantener criterio \"mayor mejor\".\n",
    "                score = -log_loss(y_valid, proba, labels=np.unique(y_train))\n",
    "            else:\n",
    "                raise ValueError(f\"M\u00e9trica no soportada: {metric_to_use}\")\n",
    "\n",
    "        else:\n",
    "            # REGRESI\u00d3N\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "            model_sel = XGBRegressor(**params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "            pred = model_sel.predict(X_va_sel)\n",
    "\n",
    "            if metric_to_use == \"r2\":\n",
    "                score = r2_score(y_valid, pred)\n",
    "            else:\n",
    "                raise ValueError(f\"M\u00e9trica de regresi\u00f3n no soportada: {metric_to_use}\")\n",
    "\n",
    "        results.append({\"cum_threshold\": thr, \"n_features\": len(cols), \"score\": score})\n",
    "\n",
    "    # Ordenar (si usamos logloss negado, mayor sigue siendo mejor)\n",
    "    res_df = pd.DataFrame(results).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    best_thr = float(res_df.iloc[0][\"cum_threshold\"])\n",
    "    return best_thr, res_df, imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0Dm8ZPGcBr7",
   "metadata": {
    "id": "R0Dm8ZPGcBr7"
   },
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "\n",
    "    # Solo num\u00e9ricos para evitar errores y acelerar\n",
    "    corr_matrix = df.corr(numeric_only=True).abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1))\n",
    "\n",
    "    to_drop = []\n",
    "    for col in tqdm(upper.columns, desc=f\"Pruning corr > {threshold}\", unit=\"col\", leave=False):\n",
    "        if (upper[col] > threshold).any():\n",
    "            to_drop.append(col)\n",
    "\n",
    "    return df.drop(columns=to_drop, errors=\"ignore\"), to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WodcQEBJ_wAW",
   "metadata": {
    "id": "WodcQEBJ_wAW"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. PIPELINE PRINCIPAL =====================\n",
    "df = df.dropna()\n",
    "y = df['label']\n",
    "X = df.iloc[:, 2:]\n",
    "\n",
    "# --- 3.3 Split temporal (ejemplo simple 80/20) ---\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# --- 3.4 Remove correlated features ---\n",
    "X_train_filtered, dropped_features = remove_highly_correlated_features(X_train, threshold=0.9)\n",
    "X_test_filtered = X_test.drop(columns=dropped_features)\n",
    "\n",
    "# Baseline logistic regression with time-series CV\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train_filtered)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "baseline = cross_val_score(LogisticRegression(max_iter=1000), X_scaled, y_train, cv=tscv).mean()\n",
    "print('Logistic regression CV accuracy:', baseline)\n",
    "\n",
    "# --- 3.5 Importancias con XGBoost ---\n",
    "imp_df, xgb_model = compute_xgb_importance(X_train_filtered, y_train, task='classification')\n",
    "\n",
    "print('=== Importancias XGBoost ===')\n",
    "print(imp_df.head(20))\n",
    "print(f'Total features: {len(imp_df)}')\n",
    "\n",
    "# --- 3.6 Selecci\u00f3n (elige una opci\u00f3n) ---\n",
    "X_train_sel, keep_cols = select_features_with_importance(X_train_filtered, imp_df, cum_threshold=0.8)\n",
    "X_test_sel = X_test_filtered[keep_cols]\n",
    "\n",
    "print(f'Features seleccionadas: {len(keep_cols)}')\n",
    "importance_map = imp_df.set_index(\"feature\")[\"importance\"]\n",
    "selected_importances = pd.DataFrame({\n",
    "    \"feature\": keep_cols,\n",
    "    \"importance\": importance_map.reindex(keep_cols).values\n",
    "})\n",
    "selected_importances.to_csv(results_dir / f\"{symbol}_{direction}_M5M10_{data_type}_ImportantCols.csv\", index=False)\n",
    "\n",
    "# Save dataset with selected features\n",
    "df_selected = df[['Date', 'label'] + keep_cols]\n",
    "df_selected.to_csv(results_dir / f\"{symbol}_{direction}_M5M10_{data_type}_Features.csv\", index=False)\n",
    "\n",
    "# Time-series cross-validation with XGBoost\n",
    "xgb_cv = XGBClassifier(eval_metric='logloss', n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "xgb_scores = cross_val_score(xgb_cv, X_train_sel, y_train, cv=tscv, scoring='accuracy')\n",
    "print('XGBoost CV accuracy:', xgb_scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ysa-7eLvEMpE",
    "m8CIB8vltFfH",
    "y6QRdBKwrX98",
    "zhTndYVi1TEV",
    "jh9-mi26wsya",
    "Hv5bsrpc03z7",
    "2RFfhHT2AwAJ"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "dc38b92328b143beb0ca760d8c8e6859": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e14a885c7dc4465fbef1959b6b719147",
       "IPY_MODEL_d406ea993e074aceb7c9997d4e6afa1b",
       "IPY_MODEL_37471a2d01464d7884e2bb52ac2645c7"
      ],
      "layout": "IPY_MODEL_3dd3464d252b4559ae36b1f71964ca8d"
     }
    },
    "e14a885c7dc4465fbef1959b6b719147": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ae877086206461e9f54280a50304361",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_93b234553dd24e6f82efea30c21f6063",
      "value": "Kalman\u2007&amp;\u2007Derivatives:\u2007100%"
     }
    },
    "d406ea993e074aceb7c9997d4e6afa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93acf3715d864a2cbf793b0bdafd96d4",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3713d638e82c414291abacb4508c375e",
      "value": 3
     }
    },
    "37471a2d01464d7884e2bb52ac2645c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3460b91edd1f41acbced602c41d85dce",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_cc9c9572ab9b4e258ab7396f930c8e95",
      "value": "\u20073/3\u2007[00:01&lt;00:00,\u2007\u20071.55it/s]"
     }
    },
    "3dd3464d252b4559ae36b1f71964ca8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ae877086206461e9f54280a50304361": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93b234553dd24e6f82efea30c21f6063": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93acf3715d864a2cbf793b0bdafd96d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3713d638e82c414291abacb4508c375e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3460b91edd1f41acbced602c41d85dce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9c9572ab9b4e258ab7396f930c8e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}