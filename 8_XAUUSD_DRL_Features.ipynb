{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf83164c",
   "metadata": {
    "id": "bf83164c"
   },
   "source": [
    "# Pendientes\n",
    "\n",
    "Nada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ysa-7eLvEMpE",
   "metadata": {
    "id": "Ysa-7eLvEMpE"
   },
   "source": [
    "# Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9GJAW8qAEM8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GJAW8qAEM8f",
    "outputId": "23d2551f-93fe-4eab-9b4d-ca6968966ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  8 21:41:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             12W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "FcIdAmrNERw-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcIdAmrNERw-",
    "outputId": "7f997355-43ca-46e8-9437-4ef1901cd961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6QRdBKwrX98",
   "metadata": {
    "id": "y6QRdBKwrX98"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bda1a01c",
   "metadata": {
    "id": "bda1a01c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, r2_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "EB5RqjoAtFwl",
   "metadata": {
    "id": "EB5RqjoAtFwl"
   },
   "outputs": [],
   "source": [
    "root_data = \"/content/gdrive/MyDrive/Course Folder/Forex/XAUUSD\"\n",
    "#/content/gdrive/MyDrive/Course Folder/Forex/XAUUSD/Data/XAUUSD_M5.csv\n",
    "\n",
    "direction = 'Short'\n",
    "direction_number = -1\n",
    "\n",
    "symbol = 'XAUUSD'\n",
    "strategy = 'Kalman'\n",
    "time_frame = 'M5'\n",
    "\n",
    "trade_evolution = 'st_Max'\n",
    "result_field = 'st_PnL'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7JweuZy755ym",
   "metadata": {
    "id": "7JweuZy755ym"
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "knFFHRT7JTVA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knFFHRT7JTVA",
    "outputId": "6e934286-88f7-433c-e556-3bff92c1b110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcD94EQwhf6l",
   "metadata": {
    "id": "dcD94EQwhf6l"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VsVEkJgcL9dY",
   "metadata": {
    "id": "VsVEkJgcL9dY"
   },
   "source": [
    "## Strategy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "DPrfPT3uB6HY",
   "metadata": {
    "id": "DPrfPT3uB6HY"
   },
   "outputs": [],
   "source": [
    "def st_pnl_close(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # 1) ensure Date is datetime & sorted\n",
    "    if not np.issubdtype(df['Date'].dtype, np.datetime64):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    df['st_row_PnL_close'] = np.nan\n",
    "\n",
    "    # 3) pick only true entry‐rows: longs open_trade==1, shorts open_trade==-1\n",
    "    entries = df[\n",
    "        ((df['Type'] == 'Long')  & (df['Open_Trade'] == 1)) |\n",
    "        ((df['Type'] == 'Short') & (df['Open_Trade'] == -1))\n",
    "    ]\n",
    "\n",
    "    for _, trade in entries.iterrows():\n",
    "        entry    = trade['Entry_Date']\n",
    "        exit_    = trade['st_Exit_Date'] if pd.notna(trade['st_Exit_Date']) else entry\n",
    "        base     = trade['Close']\n",
    "        mask     = (df['Date'] >= entry) & (df['Date'] <= exit_)\n",
    "\n",
    "        if trade['Type'] == 'Long':\n",
    "            df.loc[mask, 'st_row_PnL_close'] = (df.loc[mask, 'Close'] - base) * lot_size\n",
    "        else:\n",
    "            df.loc[mask, 'st_row_PnL_close'] = (base - df.loc[mask, 'Close']) * lot_size\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def st_pnl_low(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if not np.issubdtype(df['Date'].dtype, np.datetime64):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    df['st_row_PnL_Low'] = np.nan\n",
    "\n",
    "    opens = df[df['Type'].notna()]\n",
    "    for _, trade in opens.iterrows():\n",
    "        entry = trade['Entry_Date']\n",
    "        exit_ = trade['st_Exit_Date']\n",
    "\n",
    "        # Buscar el precio de cierre en la fecha de entrada\n",
    "        base_row = df[df['Date'] == entry]\n",
    "        if base_row.empty:\n",
    "            continue  # Saltar si no se encuentra esa fecha\n",
    "        base = base_row.iloc[0]['Close']\n",
    "\n",
    "        mask = (df['Date'] >= entry) & (df['Date'] <= exit_)\n",
    "\n",
    "        if trade['Type'] == 'Long':\n",
    "            df.loc[mask, 'st_row_PnL_low'] = (df.loc[mask, 'Low'] - base) * lot_size\n",
    "        elif trade['Type'] == 'Short':\n",
    "            df.loc[mask, 'st_row_PnL_low'] = (base - df.loc[mask, 'Low']) * lot_size\n",
    "\n",
    "    return df\n",
    "\n",
    "def st_pnl_high(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if not np.issubdtype(df['Date'].dtype, np.datetime64):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    df['st_row_PnL_high'] = np.nan\n",
    "\n",
    "    opens = df[df['Type'].notna()]\n",
    "    for _, trade in opens.iterrows():\n",
    "        entry = trade['Entry_Date']\n",
    "        exit_ = trade['st_Exit_Date']\n",
    "\n",
    "        # Buscar el precio de cierre en la fecha de entrada\n",
    "        base_row = df[df['Date'] == entry]\n",
    "        if base_row.empty:\n",
    "            continue  # Saltar si no se encuentra esa fecha\n",
    "        base = base_row.iloc[0]['Close']\n",
    "\n",
    "        mask = (df['Date'] >= entry) & (df['Date'] <= exit_)\n",
    "\n",
    "        if trade['Type'] == 'Long':\n",
    "            df.loc[mask, 'st_row_PnL_high'] = (df.loc[mask, 'High'] - base) * lot_size\n",
    "        elif trade['Type'] == 'Short':\n",
    "            df.loc[mask, 'st_row_PnL_high'] = (base - df.loc[mask, 'High']) * lot_size\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "tGzPMKFKXENq",
   "metadata": {
    "id": "tGzPMKFKXENq"
   },
   "outputs": [],
   "source": [
    "def st_exit_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['st_Exit_Date'] = pd.NaT\n",
    "\n",
    "    long_entries = df[df['Open_Trade'] == 1]\n",
    "    short_entries = df[df['Open_Trade'] == -1]\n",
    "    short_signals = df['Date'][df['Open_Trade'] == -1]\n",
    "    long_signals = df['Date'][df['Open_Trade'] == 1]\n",
    "\n",
    "    for index, row in long_entries.iterrows():\n",
    "        next_short_signal_date = short_signals[short_signals > row['Date']]\n",
    "        if not next_short_signal_date.empty:\n",
    "            exit_index = df[df['Date'] == next_short_signal_date.iloc[0]].index\n",
    "            if not exit_index.empty:\n",
    "                df.loc[index, 'st_Exit_Date'] = df.loc[exit_index[0], 'Date']\n",
    "\n",
    "    for index, row in short_entries.iterrows():\n",
    "        next_long_signal_date = long_signals[long_signals > row['Date']]\n",
    "        if not next_long_signal_date.empty:\n",
    "            exit_index = df[df['Date'] == next_long_signal_date.iloc[0]].index\n",
    "            if not exit_index.empty:\n",
    "                df.loc[index, 'st_Exit_Date'] = df.loc[exit_index[0], 'Date']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "S1I7ERNYkbi-",
   "metadata": {
    "id": "S1I7ERNYkbi-"
   },
   "outputs": [],
   "source": [
    "def st_duration(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['st_Duration'] = pd.NaT\n",
    "    trade_rows = df[df['Type'].isin(['Long', 'Short'])]\n",
    "    df.loc[trade_rows.index, 'st_Duration'] = (trade_rows['st_Exit_Date'] - trade_rows['Entry_Date']).dt.total_seconds() / 60\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "G087UbYEKj4F",
   "metadata": {
    "id": "G087UbYEKj4F"
   },
   "outputs": [],
   "source": [
    "def st_max_min(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Entry_Date'] = pd.to_datetime(df['Entry_Date'])\n",
    "    df['st_Exit_Date']  = pd.to_datetime(df['st_Exit_Date'])\n",
    "\n",
    "    df['st_Max'] = np.nan\n",
    "    df['st_Min'] = np.nan\n",
    "\n",
    "    opens = df[df['Type'].notna()]\n",
    "    for idx, trade in opens.iterrows():\n",
    "        entry    = trade['Entry_Date']\n",
    "        exit_dt  = trade['st_Exit_Date']\n",
    "        # hallar la fila de exit_dt y retroceder una\n",
    "        exit_rows = df.index[df['Date'] == exit_dt]\n",
    "        if not exit_rows.empty and exit_rows[0] > 0:\n",
    "            prev_date = df.at[exit_rows[0] - 1, 'Date']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        mask = (df['Date'] >= entry) & (df['Date'] <= prev_date)\n",
    "        # Consider max/min across the three specified columns within the mask\n",
    "        subset_df = df.loc[mask, ['st_row_PnL_close', 'st_row_PnL_low', 'st_row_PnL_high']]\n",
    "\n",
    "        if not subset_df.empty:\n",
    "            # Calculate max and min across the relevant columns for the masked rows\n",
    "            df.at[idx, 'st_Max'] = subset_df.values.max()\n",
    "            df.at[idx, 'st_Min'] = subset_df.values.min()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "LpdtMDMJMDQh",
   "metadata": {
    "id": "LpdtMDMJMDQh"
   },
   "outputs": [],
   "source": [
    "def st_trade_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure datetime columns are in datetime format for comparisons\n",
    "    for col in [\"Date\", \"Entry_Date\", \"st_Exit_Date\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    atr_candidates = [col for col in df.columns if isinstance(col, str)]\n",
    "    atr_col = next((col for col in atr_candidates if col.strip().lower() == 'atr'), None)\n",
    "    if atr_col is None:\n",
    "        fallback_names = {'atr14', 'atr_14', 'atr(14)'}\n",
    "        atr_col = next((col for col in atr_candidates if col.strip().lower() in fallback_names), None)\n",
    "\n",
    "    # Prepare/clear destination columns\n",
    "    df['trade type'] = np.nan\n",
    "    df['consecutive_trade_period'] = np.nan\n",
    "    df['atr_base_change_abs'] = np.nan\n",
    "    df['atr_base_change_%'] = np.nan\n",
    "    df['atr_entry_close_to_high_%'] = np.nan\n",
    "    df['atr_entry_close_to_low_%'] = np.nan\n",
    "\n",
    "    trade_entries = df[df['Type'].isin(['Long', 'Short'])].copy()\n",
    "\n",
    "    for entry_idx, trade in trade_entries.iterrows():\n",
    "        trade_type = trade.get('Type')\n",
    "        entry_date = trade.get('Entry_Date', trade.get('Date'))\n",
    "        exit_date = trade.get('st_Exit_Date')\n",
    "\n",
    "        if pd.isna(entry_date) or trade_type not in ('Long', 'Short'):\n",
    "            continue\n",
    "\n",
    "        # Determine the slice of rows that belong to the trade block\n",
    "        if pd.notna(exit_date):\n",
    "            mask = (df['Date'] >= entry_date) & (df['Date'] < exit_date)\n",
    "        else:\n",
    "            mask = df['Date'] >= entry_date\n",
    "        trade_indices = df.index[mask]\n",
    "        if len(trade_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        trade_slice = df.loc[trade_indices]\n",
    "        expected_flag = 1 if trade_type == 'Long' else -1\n",
    "        entry_row_index = trade_indices[0]\n",
    "        if 'Open_Trade' in trade_slice.columns:\n",
    "            entry_rows = trade_slice.loc[trade_slice['Open_Trade'] == expected_flag]\n",
    "            if not entry_rows.empty:\n",
    "                entry_row_index = entry_rows.index[0]\n",
    "\n",
    "        # Trade direction encoded as 1 (Long) / 0 (Short)\n",
    "        df.loc[trade_indices, 'trade type'] = 1 if trade_type == 'Long' else 0\n",
    "\n",
    "        # Consecutive period counter within the trade\n",
    "        df.loc[trade_indices, 'consecutive_trade_period'] = np.arange(1, len(trade_indices) + 1, dtype=float)\n",
    "\n",
    "        if atr_col is not None:\n",
    "            base_atr = pd.to_numeric(pd.Series([df.loc[entry_row_index, atr_col]]), errors='coerce').iloc[0]\n",
    "        else:\n",
    "            base_atr = np.nan\n",
    "        entry_close = pd.to_numeric(pd.Series([df.loc[entry_row_index, 'Close']]), errors='coerce').iloc[0]\n",
    "\n",
    "        if atr_col is not None and np.isfinite(base_atr):\n",
    "            current_atr = pd.to_numeric(df.loc[trade_indices, atr_col], errors='coerce')\n",
    "            delta = current_atr - base_atr\n",
    "            df.loc[trade_indices, 'atr_base_change_abs'] = delta\n",
    "            if base_atr != 0:\n",
    "                df.loc[trade_indices, 'atr_base_change_%'] = (delta / base_atr) * 100\n",
    "            else:\n",
    "                df.loc[trade_indices, 'atr_base_change_%'] = np.nan\n",
    "        else:\n",
    "            df.loc[trade_indices, ['atr_base_change_abs', 'atr_base_change_%']] = np.nan\n",
    "\n",
    "        if atr_col is not None and np.isfinite(base_atr) and base_atr != 0 and np.isfinite(entry_close):\n",
    "            highs = pd.to_numeric(df.loc[trade_indices, 'High'], errors='coerce')\n",
    "            lows = pd.to_numeric(df.loc[trade_indices, 'Low'], errors='coerce')\n",
    "\n",
    "            if trade_type == 'Long':\n",
    "                high_pct = (highs - entry_close) / base_atr * 100\n",
    "                low_pct = (lows - entry_close) / base_atr * 100\n",
    "            else:\n",
    "                high_pct = (entry_close - highs) / base_atr * 100\n",
    "                low_pct = (entry_close - lows) / base_atr * 100\n",
    "\n",
    "            df.loc[trade_indices, 'atr_entry_close_to_high_%'] = high_pct\n",
    "            df.loc[trade_indices, 'atr_entry_close_to_low_%'] = low_pct\n",
    "        else:\n",
    "            df.loc[trade_indices, ['atr_entry_close_to_high_%', 'atr_entry_close_to_low_%']] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "Yf1or5PPMDW6",
   "metadata": {
    "id": "Yf1or5PPMDW6"
   },
   "outputs": [],
   "source": [
    "def st_pnl(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['st_Exit_Date'] = pd.to_datetime(df['st_Exit_Date'])\n",
    "    df['st_PnL'] = np.nan\n",
    "\n",
    "    pnl_by_date = df.set_index('Date')['st_row_PnL_close']\n",
    "    entries = df[df['Type'].isin(['Long', 'Short'])]\n",
    "    for idx, trade in entries.iterrows():\n",
    "        exit_dt = trade['st_Exit_Date']\n",
    "        exit_rows = df.index[df['Date'] == exit_dt]\n",
    "        if not exit_rows.empty and exit_rows[0] > 0:\n",
    "            prev_date = df.at[exit_rows[0] - 1, 'Date']\n",
    "            df.at[idx, 'st_PnL'] = pnl_by_date.get(prev_date, np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "qYC7Jizty6P7",
   "metadata": {
    "id": "qYC7Jizty6P7"
   },
   "outputs": [],
   "source": [
    "def PnL_trade_analytics_table(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calcula métricas de rendimiento de trades con los siguientes formatos:\n",
    "      • Total PnL, Avg Profit y Avg Loss → $ con separador de miles, 0 decimales\n",
    "      • Win (%) y Loss (%) → 2 decimales + símbolo %\n",
    "      • Profit Factor → 2 decimales\n",
    "      • # Days y conteos → separador de miles\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # ──────────────────── Cálculo numérico ────────────────────\n",
    "    analytics = pd.DataFrame(index=['Strategy'])\n",
    "    analytics['# Days'] = (df['Date'].max() - df['Date'].min()).days\n",
    "    analytics['Total PnL']  = round(df.st_PnL.sum(), 0)\n",
    "    analytics['Total Trades'] = df.st_PnL.count()\n",
    "    analytics['Number of Winners'] = (df.st_PnL > 0).sum()\n",
    "    analytics['Number of Losers']  = (df.st_PnL <= 0).sum()\n",
    "    analytics['Win (%)']  = round(100 * analytics.at['Strategy','Number of Winners'] /\n",
    "                                  analytics.at['Strategy','Total Trades'], 2)\n",
    "    analytics['Loss (%)'] = round(100 * analytics.at['Strategy','Number of Losers']  /\n",
    "                                  analytics.at['Strategy','Total Trades'], 2)\n",
    "    analytics['Average Profit of Winning Trade'] = round(df.loc[df.st_PnL > 0, 'st_PnL'].mean(), 0)\n",
    "    analytics['Average Loss of Losing Trade']    = round(abs(df.loc[df.st_PnL < 0, 'st_PnL'].mean()), 0)\n",
    "\n",
    "    if {'Entry_Date','st_Exit_Date'}.issubset(df.columns):\n",
    "        df['Entry_Date']   = pd.to_datetime(df['Entry_Date'])\n",
    "        df['st_Exit_Date'] = pd.to_datetime(df['st_Exit_Date'])\n",
    "        analytics['Average Holding Time'] = (df['st_Exit_Date'] - df['Entry_Date']).mean()\n",
    "\n",
    "    tot_profit   = df.loc[df.st_PnL > 0, 'st_PnL'].sum()\n",
    "    tot_loss_abs = abs(df.loc[df.st_PnL < 0, 'st_PnL'].sum())\n",
    "    analytics['Profit Factor'] = round(tot_profit / tot_loss_abs, 2) if tot_loss_abs else np.nan\n",
    "\n",
    "    # ──────────────────── Formato visual ────────────────────\n",
    "    def _fmt(val, metric):\n",
    "        if metric in ('Win (%)', 'Loss (%)'):\n",
    "            return f'{val:,.2f}%'\n",
    "        if metric in ('Total PnL',\n",
    "                      'Average Profit of Winning Trade',\n",
    "                      'Average Loss of Losing Trade'):\n",
    "            return f'$ {val:,.0f}'\n",
    "        if metric in ('# Days', 'Total Trades', 'Number of Winners', 'Number of Losers'):\n",
    "            return f'{val:,.0f}'\n",
    "        if metric in ('Profit Factor',):\n",
    "            return f'{val:,.2f}'\n",
    "        return val\n",
    "\n",
    "    styled = analytics.T.copy()\n",
    "    styled['Strategy'] = [ _fmt(v, idx) for idx, v in styled['Strategy'].items() ]\n",
    "\n",
    "    return (styled.style\n",
    "                  .set_caption('')\n",
    "                  .set_table_styles([\n",
    "                      {'selector':'th','props':[('font-weight','bold'), ('text-align','center')]},\n",
    "                      {'selector':'td','props':[('text-align','center')]},\n",
    "                      {'selector':'thead th', 'props':[('text-align','center')]}\n",
    "                  ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eXkJb2DngBBW",
   "metadata": {
    "id": "eXkJb2DngBBW"
   },
   "outputs": [],
   "source": [
    "def st_add_atr_scaled_pnls(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    atr_col: str = \"ATR\",\n",
    "    usd_per_point: float = 100.0,\n",
    "    atr_len: int = 14\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Elimina ÚNICAMENTE la columna 'trade_type' si existe.\n",
    "    - Crea 'st_atr_PnL' y 'st_atr_max_PnL' en TODAS las filas del bloque de cada trade,\n",
    "      expresando st_PnL y st_Max en múltiplos de ATR (usando el ATR de la fila de apertura).\n",
    "    - Si no existe 'ATR', lo calcula (ATR(14) EWM).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Asegurar tipos y orden\n",
    "    for c in (\"Date\", \"Entry_Date\", \"st_Exit_Date\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # Eliminar SOLO 'trade_type'\n",
    "    if \"trade_type\" in df.columns:\n",
    "        df.drop(columns=[\"trade_type\"], inplace=True)\n",
    "\n",
    "    # Asegurar ATR\n",
    "    if (atr_col not in df.columns) or (df[atr_col].isna().all()):\n",
    "        high = pd.to_numeric(df[\"High\"], errors=\"coerce\")\n",
    "        low  = pd.to_numeric(df[\"Low\"],  errors=\"coerce\")\n",
    "        close = pd.to_numeric(df[\"Close\"], errors=\"coerce\")\n",
    "        prev_close = close.shift(1)\n",
    "        tr = pd.concat([(high - low), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "        df[atr_col] = tr.ewm(alpha=1.0/atr_len, adjust=False, min_periods=atr_len).mean()\n",
    "\n",
    "    # Crear columnas destino\n",
    "    df['st_atr_PnL'] = np.nan\n",
    "    df['st_atr_max_PnL'] = np.nan\n",
    "\n",
    "    # Recorrer entradas\n",
    "    entries = df[df[\"Type\"].isin([\"Long\", \"Short\"])].copy()\n",
    "    if entries.empty:\n",
    "        return df\n",
    "\n",
    "    for _, trade in entries.iterrows():\n",
    "        entry_dt = trade[\"Entry_Date\"]\n",
    "        exit_dt  = trade[\"st_Exit_Date\"]\n",
    "\n",
    "        base_row = df.loc[df[\"Date\"] == entry_dt]\n",
    "        if base_row.empty or pd.isna(base_row.iloc[0][atr_col]):\n",
    "            continue\n",
    "        atr_base = float(base_row.iloc[0][atr_col])\n",
    "        atr_usd = atr_base * usd_per_point if np.isfinite(atr_base) else np.nan\n",
    "        if not np.isfinite(atr_usd) or atr_usd == 0:\n",
    "            continue\n",
    "\n",
    "        # Barra previa al exit para delimitar bloque\n",
    "        exit_rows = df.index[df[\"Date\"] == exit_dt]\n",
    "        if not exit_rows.empty and exit_rows[0] > 0:\n",
    "            prev_date = df.at[exit_rows[0] - 1, \"Date\"]\n",
    "        else:\n",
    "            prev_date = df[\"Date\"].max()\n",
    "\n",
    "        mask = (df[\"Date\"] >= entry_dt) & (df[\"Date\"] <= prev_date)\n",
    "\n",
    "        # st_atr_pnl: usar st_PnL del trade (USD) / (ATR_base * usd_per_point)\n",
    "        pnl_usd = trade.get(\"st_PnL\", np.nan)\n",
    "        # reconstrucción si falta st_PnL\n",
    "        if not np.isfinite(pnl_usd):\n",
    "            base_close = float(base_row.iloc[0][\"Close\"]) if \"Close\" in base_row.columns else np.nan\n",
    "            row_prev = df.loc[df[\"Date\"] == prev_date]\n",
    "            if row_prev.empty or not np.isfinite(base_close):\n",
    "                continue\n",
    "            close_prev = float(row_prev.iloc[0][\"Close\"])\n",
    "            if trade[\"Type\"] == \"Long\":\n",
    "                pnl_usd = (close_prev - base_close) * usd_per_point\n",
    "            else:\n",
    "                pnl_usd = (base_close - close_prev) * usd_per_point\n",
    "        atr_pnl = pnl_usd / atr_usd if np.isfinite(pnl_usd) else np.nan\n",
    "\n",
    "        # st_atr_max_pnl: usar st_Max (USD) / (ATR_base * usd_per_point)\n",
    "        max_usd = trade.get(\"st_Max\", np.nan)\n",
    "        if not np.isfinite(max_usd):\n",
    "            cols = [c for c in (\"st_row_PnL_close\", \"st_row_PnL_low\", \"st_row_PnL_high\") if c in df.columns]\n",
    "            if cols:\n",
    "                subset = df.loc[mask, cols]\n",
    "                if not subset.empty:\n",
    "                    max_usd = np.nanmax(subset.values)\n",
    "        atr_max_pnl = (max_usd / atr_usd) if np.isfinite(max_usd) else np.nan\n",
    "\n",
    "        # Rellenar en todas las filas del bloque\n",
    "        df.loc[mask, 'st_atr_PnL'] = atr_pnl\n",
    "        df.loc[mask, 'st_atr_max_PnL'] = atr_max_pnl\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "oMJsnHtJLJXj",
   "metadata": {
    "id": "oMJsnHtJLJXj"
   },
   "outputs": [],
   "source": [
    "def st_add_atr_multipliers_from_prices(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    atr_col: str = \"ATR\",\n",
    "    atr_len: int = 14\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ATR-multiplier columns based on PRICES (not USD PnL), per your formulas:\n",
    "\n",
    "      LONG:\n",
    "        price_max_profit  = entry_close + (ATR_entry * atr_multiplier_max)\n",
    "        price_close_trade = entry_close + (ATR_entry * atr_multiplier_close)\n",
    "\n",
    "      SHORT:\n",
    "        price_max_profit  = entry_close - (ATR_entry * atr_multiplier_max)\n",
    "        price_close_trade = entry_close - (ATR_entry * atr_multiplier_close)\n",
    "\n",
    "    Definitions used:\n",
    "      • entry_close  := Close at Entry_Date\n",
    "      • ATR_entry    := ATR value at Entry_Date\n",
    "      • price_close_trade := Close at the bar immediately BEFORE st_Exit_Date\n",
    "      • price_max_profit  := For Long → max High within [Entry_Date, bar_before_exit]\n",
    "                             For Short → min Low  within [Entry_Date, bar_before_exit]\n",
    "\n",
    "    The resulting multipliers are written across every row of each trade block:\n",
    "      • st_atr_PnL      → multiplier to reach the close-trade price\n",
    "      • st_atr_max_PnL  → multiplier to reach the max-profit price\n",
    "\n",
    "    If ATR is missing, it is computed as an EWM (Wilder-style) of True Range.\n",
    "    \"\"\"\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # Ensure types and order\n",
    "    for c in (\"Date\", \"Entry_Date\", \"st_Exit_Date\"):\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_datetime(out[c], errors=\"coerce\")\n",
    "    out = out.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # Ensure numeric price columns\n",
    "    for c in (\"Close\", \"High\", \"Low\"):\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # Ensure ATR exists (EWM of True Range if missing or all-NaN)\n",
    "    if (atr_col not in out.columns) or (out[atr_col].isna().all()):\n",
    "        high = out[\"High\"]\n",
    "        low  = out[\"Low\"]\n",
    "        close = out[\"Close\"]\n",
    "        prev_close = close.shift(1)\n",
    "        tr = pd.concat([(high - low),\n",
    "                        (high - prev_close).abs(),\n",
    "                        (low  - prev_close).abs()], axis=1).max(axis=1)\n",
    "        out[atr_col] = tr.ewm(alpha=1.0/atr_len, adjust=False, min_periods=atr_len).mean()\n",
    "\n",
    "    # Prepare destination columns\n",
    "    out[\"st_atr_PnL\"] = np.nan\n",
    "    out[\"st_atr_max_PnL\"] = np.nan\n",
    "\n",
    "    # Iterate over trades (rows where Type is 'Long' or 'Short')\n",
    "    if \"Type\" not in out.columns:\n",
    "        return out\n",
    "\n",
    "    trades = out[out[\"Type\"].isin([\"Long\", \"Short\"])].copy()\n",
    "    for _, tr in trades.iterrows():\n",
    "        entry_dt = tr.get(\"Entry_Date\", pd.NaT)\n",
    "        exit_dt  = tr.get(\"st_Exit_Date\", pd.NaT)\n",
    "        side     = tr.get(\"Type\", None)\n",
    "\n",
    "        if pd.isna(entry_dt) or side not in (\"Long\", \"Short\"):\n",
    "            continue\n",
    "\n",
    "        # Base row at entry\n",
    "        base_row = out.loc[out[\"Date\"] == entry_dt]\n",
    "        if base_row.empty:\n",
    "            continue\n",
    "\n",
    "        entry_close = float(base_row.iloc[0][\"Close\"])\n",
    "        atr_entry   = float(base_row.iloc[0][atr_col]) if atr_col in base_row.columns else np.nan\n",
    "        if not np.isfinite(entry_close) or not np.isfinite(atr_entry) or atr_entry <= 0:\n",
    "            continue\n",
    "\n",
    "        # Determine bar_before_exit to delimit the trade block\n",
    "        if pd.notna(exit_dt):\n",
    "            exit_idx = out.index[out[\"Date\"] == exit_dt]\n",
    "            if len(exit_idx) > 0 and exit_idx[0] > 0:\n",
    "                bar_before_exit_date = out.at[exit_idx[0] - 1, \"Date\"]\n",
    "            else:\n",
    "                # If exit is the first row or not found, consider up to the last available row\n",
    "                bar_before_exit_date = out[\"Date\"].max()\n",
    "        else:\n",
    "            # If no exit, consider up to the last available row\n",
    "            bar_before_exit_date = out[\"Date\"].max()\n",
    "\n",
    "        # Mask for the trade block [Entry_Date, bar_before_exit]\n",
    "        mask = (out[\"Date\"] >= entry_dt) & (out[\"Date\"] <= bar_before_exit_date)\n",
    "        block = out.loc[mask, [\"High\", \"Low\", \"Close\"]].dropna()\n",
    "        if block.empty:\n",
    "            continue\n",
    "\n",
    "        # Prices of interest\n",
    "        price_close_trade = float(out.loc[out[\"Date\"] == bar_before_exit_date, \"Close\"].iloc[0])\n",
    "\n",
    "        if side == \"Long\":\n",
    "            # Max favorable price in block is the highest High\n",
    "            price_max_profit = float(block[\"High\"].max())\n",
    "            atr_mult_close = (price_close_trade - entry_close) / atr_entry\n",
    "            atr_mult_max   = (price_max_profit  - entry_close) / atr_entry\n",
    "        else:  # Short\n",
    "            # Max favorable price for Short is the lowest Low\n",
    "            price_max_profit = float(block[\"Low\"].min())\n",
    "            atr_mult_close = (entry_close - price_close_trade) / atr_entry\n",
    "            atr_mult_max   = (entry_close - price_max_profit ) / atr_entry\n",
    "\n",
    "        # Write the same multiplier across the trade block\n",
    "        out.loc[mask, \"st_atr_PnL\"] = atr_mult_close\n",
    "        out.loc[mask, \"st_atr_max_PnL\"] = atr_mult_max\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I23127P7lBq_",
   "metadata": {
    "id": "I23127P7lBq_"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "kp4yJdGAjoeA",
   "metadata": {
    "id": "kp4yJdGAjoeA"
   },
   "outputs": [],
   "source": [
    "def kalman_line(source, kalman_length: int, smooth: int):\n",
    "    \"\"\"\n",
    "    Pine -> Python (solo 'kalman_line'), replicando la EMA de TradingView con\n",
    "    *semilla SMA* (como ta.ema) sobre el núcleo Kalman kf_c.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    source : pd.Series o array-like de floats (precio crudo, sin diff/returns)\n",
    "    kalman_length : int   (equivale a length_kal en Pine)\n",
    "    smooth : int          (equivale a smooth_kal en Pine -> ta.ema(kf_c, smooth))\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    Mismo tipo que `source`: pd.Series o np.ndarray con la línea Kalman suavizada.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # normalizamos tipos\n",
    "    is_series = hasattr(source, \"index\")\n",
    "    idx = source.index if is_series else None\n",
    "    x = np.asarray(source, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if n == 0:\n",
    "        return source\n",
    "\n",
    "    # ---------- núcleo Kalman idéntico al Pine ----------\n",
    "    sqrt_term   = np.sqrt((kalman_length / 10000.0) * 2.0)\n",
    "    length_term = kalman_length / 10000.0\n",
    "\n",
    "    kf_c   = np.empty(n, dtype=np.float64)\n",
    "    velo_c = np.empty(n, dtype=np.float64)\n",
    "\n",
    "    # bar 0 (nz(kf_c[1], source) y nz(velo_c[1], 0))\n",
    "    kf_c[0] = x[0]\n",
    "    velo_c[0] = 0.0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        prev_kf = kf_c[i - 1]\n",
    "        dk_c = x[i] - prev_kf\n",
    "        smooth_c = prev_kf + dk_c * sqrt_term\n",
    "        velo_c[i] = velo_c[i - 1] + length_term * dk_c\n",
    "        kf_c[i] = smooth_c + velo_c[i]\n",
    "\n",
    "    # ---------- EMA con semilla SMA (comportamiento ta.ema de TV) ----------\n",
    "    L = int(max(1, smooth))\n",
    "    alpha = 2.0 / (L + 1.0)\n",
    "    ema = np.full(n, np.nan, dtype=np.float64)\n",
    "\n",
    "    if n < L:\n",
    "        # con pocas barras, igualamos al promedio simple disponible\n",
    "        ema[-1] = np.nanmean(kf_c)\n",
    "    else:\n",
    "        # seed = SMA de las primeras L barras\n",
    "        seed = np.mean(kf_c[:L])\n",
    "        ema[L - 1] = seed\n",
    "        for i in range(L, n):\n",
    "            ema[i] = alpha * kf_c[i] + (1.0 - alpha) * ema[i - 1]\n",
    "\n",
    "    return (pd.Series(ema, index=idx) if is_series else ema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e07e51fb",
   "metadata": {
    "id": "e07e51fb"
   },
   "outputs": [],
   "source": [
    "def create_features(\n",
    "    stock_data: pd.DataFrame,\n",
    "    return_components: bool = False\n",
    ") -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]]:\n",
    "\n",
    "    kalman_periods = [300, 600, 900]\n",
    "    kalman_smooth_kal = 3\n",
    "\n",
    "    component_frames: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    def _unique_pairwise(columns: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Return ordered unique column pairs without self-pairings.\"\"\"\n",
    "        unique_columns = list(dict.fromkeys(columns))\n",
    "        return list(combinations(unique_columns, 2))\n",
    "\n",
    "    # ───────────────────────── Kalman y derivados ───────────────────────\n",
    "    t0 = time.time()\n",
    "    kal_cols = []\n",
    "    kalman_features = pd.DataFrame(index=stock_data.index)\n",
    "    kalman_900_series: Optional[pd.Series] = None\n",
    "    for period in tqdm(kalman_periods, desc=\"Kalman & Derivatives\"):\n",
    "        kal = pd.Series(\n",
    "            kalman_line(stock_data['Close'], kalman_length=period, smooth=kalman_smooth_kal),\n",
    "            index=stock_data.index\n",
    "        )\n",
    "\n",
    "        if kal.isna().any():\n",
    "            kal = kal.ffill()\n",
    "            if kal.isna().any():\n",
    "                kal = kal.bfill()\n",
    "        kname = f'Kal_{period}'\n",
    "        kal_cols.append(kname)\n",
    "\n",
    "        kalman_features[kname] = kal\n",
    "        if period == 900:\n",
    "            kalman_900_series = kal\n",
    "\n",
    "    tqdm.write(f\"[Timing] Kalman block: {time.time()-t0:.2f}s\")\n",
    "    component_frames['Kalman'] = kalman_features.copy()\n",
    "    features = kalman_features.copy()\n",
    "\n",
    "    if return_components:\n",
    "        component_frames['Create_Features'] = features.copy()\n",
    "        return features, component_frames\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24wl8snRkxUz",
   "metadata": {
    "id": "24wl8snRkxUz"
   },
   "outputs": [],
   "source": [
    "def scale_minmax(features: pd.DataFrame, window: int) -> pd.DataFrame:\n",
    "    \"\"\"Apply Min-Max scaling using a rolling window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    window : int\n",
    "        Rolling window size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Min-Max scaled feature block preserving the original index.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=np.number).columns\n",
    "    if numeric_cols.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    scaled_features = pd.DataFrame(index=features.index)\n",
    "    for col in numeric_cols:\n",
    "        rolling = features[col].rolling(window=window, min_periods=window)\n",
    "        rolling_min = rolling.min()\n",
    "        rolling_max = rolling.max()\n",
    "\n",
    "        range_ = (rolling_max - rolling_min).replace(0, np.nan)\n",
    "        scaled_features[col] = (features[col] - rolling_min) / range_\n",
    "\n",
    "    return scaled_features\n",
    "\n",
    "def apply_minmax_scaling(\n",
    "    features: pd.DataFrame,\n",
    "    windows: Tuple[int, ...] = (3,),\n",
    "    prefix: str = \"a_minimal_\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply Min-Max scaling using multiple rolling window lengths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    windows : Tuple[int, ...], optional\n",
    "        Collection of rolling window sizes to apply, by default (3).\n",
    "    prefix : str, optional\n",
    "        Prefix to add to each generated Min-Max column, by default \"a_minimal_\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with Min-Max scaled features for the requested windows.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    scaled_blocks = []\n",
    "    for window in windows:\n",
    "        if window <= 1:\n",
    "            continue\n",
    "        window_scaled = scale_minmax(features, window=window)\n",
    "        if window_scaled.empty:\n",
    "            continue\n",
    "        rename_map = {col: f\"{prefix}{col}_minmax_{window}\" for col in window_scaled.columns}\n",
    "        scaled_blocks.append(window_scaled.rename(columns=rename_map))\n",
    "\n",
    "    if not scaled_blocks:\n",
    "        return pd.DataFrame(index=features.index)\n",
    "\n",
    "    return pd.concat(scaled_blocks, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e0c0f79",
   "metadata": {
    "id": "1e0c0f79"
   },
   "outputs": [],
   "source": [
    "def scale_feature_block(features: pd.DataFrame, window: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Scale features using a rolling window standardization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        Feature block to scale.\n",
    "    window : int, optional\n",
    "        Rolling window size, by default 200.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Scaled feature block preserving the original index.\n",
    "    \"\"\"\n",
    "    if features.empty:\n",
    "        return features.copy()\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=np.number).columns\n",
    "    scaled_features = features.copy()\n",
    "\n",
    "    if not numeric_cols.empty:\n",
    "        rolling = features[numeric_cols].rolling(window=window, min_periods=window)\n",
    "        mean = rolling.mean()\n",
    "        std = rolling.std()\n",
    "\n",
    "        std = std.replace(0, np.nan)\n",
    "\n",
    "        scaled_features[numeric_cols] = (features[numeric_cols] - mean) / std\n",
    "\n",
    "    return scaled_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DoqqPJxPxnBF",
   "metadata": {
    "id": "DoqqPJxPxnBF"
   },
   "source": [
    "# 5_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7l7vt7QxvyC",
   "metadata": {
    "id": "d7l7vt7QxvyC"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df_5min = pd.read_csv(root_data+'/Data/'+symbol+'_M5.csv')\n",
    "df_5min = df_5min.iloc[-50000:,]\n",
    "\n",
    "print('Min_Date : ', df_5min['Date'].min())\n",
    "print('Min_Date : ', df_5min['Date'].max())\n",
    "print('Number_Rows = ',len(df_5min))\n",
    "print('\\n')\n",
    "\n",
    "df_5min.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATIMrlP4L_zn",
   "metadata": {
    "id": "ATIMrlP4L_zn"
   },
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2l2awW7pT6dh",
   "metadata": {
    "id": "2l2awW7pT6dh"
   },
   "outputs": [],
   "source": [
    "\n",
    "length_1 = 300\n",
    "length_2 = 520\n",
    "length_3 = 710\n",
    "length_4 = 1130\n",
    "\n",
    "smooth_1 = 3\n",
    "smooth_2 = 3\n",
    "smooth_3 = 3\n",
    "smooth_4 = 7\n",
    "\n",
    "df_5min['kal_1'] = kalman_line(df_5min['Close'],length_1, smooth_1)\n",
    "df_5min['kal_2'] = kalman_line(df_5min['Close'],length_2, smooth_2)\n",
    "df_5min['kal_3'] = kalman_line(df_5min['Close'],length_3, smooth_3)\n",
    "df_5min['kal_4'] = kalman_line(df_5min['Close'],length_4, smooth_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gftXewm5ld_Q",
   "metadata": {
    "id": "gftXewm5ld_Q"
   },
   "outputs": [],
   "source": [
    "bullish_condition = (df_5min['kal_1'] > df_5min['kal_1'].shift()) & \\\n",
    "                    (df_5min['kal_2'] > df_5min['kal_2'].shift()) & \\\n",
    "                    (df_5min['kal_3'] > df_5min['kal_3'].shift())\n",
    "\n",
    "bearish_condition = (df_5min['kal_1'] < df_5min['kal_1'].shift()) & \\\n",
    "                    (df_5min['kal_2'] < df_5min['kal_2'].shift()) & \\\n",
    "                    (df_5min['kal_3'] < df_5min['kal_3'].shift())\n",
    "\n",
    "\n",
    "df_5min['Open_Trade_1'] = np.where(bullish_condition, 1, np.where(bearish_condition, -1, np.nan))\n",
    "df_5min['Open_Trade'] = np.where(df_5min['Open_Trade_1'] == df_5min['Open_Trade_1'].shift(1), np.nan, df_5min['Open_Trade_1'])\n",
    "\n",
    "close_bearish_condition = df_5min['Open_Trade'] == 1\n",
    "close_bullish_condition = df_5min['Open_Trade'] == -1\n",
    "\n",
    "df_5min['Close_Trade_1'] = np.where(df_5min['kal_4'] < df_5min['kal_4'].shift(), -1, np.where(df_5min['kal_4'] > df_5min['kal_4'].shift(), 1, np.nan))\n",
    "df_5min['Close_Trade'] = np.where(df_5min['Close_Trade_1'] == df_5min['Close_Trade_1'].shift(1), np.nan, df_5min['Close_Trade_1'])\n",
    "\n",
    "df_5min = df_5min.drop(columns=['Open_Trade_1', 'Close_Trade_1'])\n",
    "#df_5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0i7yrCpMyvtV",
   "metadata": {
    "id": "0i7yrCpMyvtV"
   },
   "outputs": [],
   "source": [
    "df_5min['Entry_Date'] = pd.NaT\n",
    "df_5min.loc[df_5min['Open_Trade'] ==  1, 'Entry_Date'] = df_5min['Date'][df_5min['Open_Trade'] == 1]\n",
    "df_5min.loc[df_5min['Open_Trade'] == -1, 'Entry_Date'] = df_5min['Date'][df_5min['Open_Trade'] == -1]\n",
    "\n",
    "df_5min['Type'] = np.nan\n",
    "df_5min.loc[df_5min['Open_Trade'] ==  1, 'Type'] = 'Long'\n",
    "df_5min.loc[df_5min['Open_Trade'] == -1, 'Type'] = 'Short'\n",
    "#df_5min.loc[df_5min['Open_Trade'] == -1, 'Type'] = 'Short'\n",
    "#df_5min['Type'] = df_5min['Type'].ffill()\n",
    "\n",
    "df_5min['Trade_Number'] = np.nan\n",
    "start_mask = df_5min[\"Entry_Date\"].notna() & df_5min[\"Type\"].notna()\n",
    "\n",
    "df_5min[\"Trade_Number\"] = start_mask.cumsum()\n",
    "first_trade = df_5min[\"Trade_Number\"].ne(0).idxmax()\n",
    "df_5min.loc[: first_trade - 1, \"Trade_Number\"] = np.nan\n",
    "df_5min.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MLKtgtS4ykqg",
   "metadata": {
    "id": "MLKtgtS4ykqg"
   },
   "outputs": [],
   "source": [
    "columns_to_see = ['Date', 'Close','Open_Trade', 'Type', 'Entry_Date','st_Exit_Date','Trade_Number']\n",
    "\n",
    "#df_5min.loc[df_5min['Open_Trade'].notna(), :].head(10)\n",
    "#df_5min.loc[((df_5min['Open_Trade'].notna()) & (df_5min['Type']=='Short')), :].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ZqV-TP95tLTA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "ZqV-TP95tLTA",
    "outputId": "533ca697-821d-4640-c66e-ef695803e1ac"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1530832145.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlot_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_5min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst_exit_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_5min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_5min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_5min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst_trade_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_5min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2195100099.py\u001b[0m in \u001b[0;36mst_exit_date\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshort_entries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnext_long_signal_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_signals\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnext_long_signal_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mexit_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnext_long_signal_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__gt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__gt__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ge__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lot_size = 100\n",
    "\n",
    "df_5min = st_exit_date(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_trade_type(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_duration(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_pnl_close(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_pnl_high(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_pnl_low(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_max_min(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_pnl(df_5min)\n",
    "print(df_5min.shape)\n",
    "df_5min = st_add_atr_scaled_pnls(df_5min, atr_col=\"ATR\", usd_per_point=100.0, atr_len=14)\n",
    "print(df_5min.columns)\n",
    "df_5min = st_add_atr_multipliers_from_prices(df_5min)\n",
    "print(df_5min.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JHSM5nVNbOHG",
   "metadata": {
    "id": "JHSM5nVNbOHG"
   },
   "outputs": [],
   "source": [
    "columns_to_see = ['Date', 'Close','ATR','trade type', 'Open_Trade',\n",
    "                  'st_Max', 'st_Min', 'st_PnL', 'Trade_Number', 'consecutive_trade_period', 'atr_base_change_abs', 'atr_base_change_%', 'atr_entry_close_to_high_%', 'atr_entry_close_to_low_%']\n",
    "\n",
    "df_5min.loc[df_5min['Trade_Number']==3, columns_to_see]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeEbY1u30zWk",
   "metadata": {
    "id": "aeEbY1u30zWk"
   },
   "outputs": [],
   "source": [
    "df_5min.to_csv(root_data+'/Results/'+symbol+'_'+strategy+'_'+time_frame+'_DRL_TradeResults.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1M3NMCpZmLVP",
   "metadata": {
    "id": "1M3NMCpZmLVP"
   },
   "outputs": [],
   "source": [
    "df_5min_st_pnl = df_5min.copy()\n",
    "df_5min_st_pnl = df_5min_st_pnl[df_5min_st_pnl['Entry_Date'].notna()]\n",
    "#df_5min_sl_pnl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zTtA7iyrmLVP",
   "metadata": {
    "id": "zTtA7iyrmLVP"
   },
   "outputs": [],
   "source": [
    "trade_analysis = PnL_trade_analytics_table(df_5min_st_pnl)\n",
    "trade_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rKPnZxavRB7v",
   "metadata": {
    "id": "rKPnZxavRB7v"
   },
   "outputs": [],
   "source": [
    "print(df_5min_st_pnl['st_PnL'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HBicVZcDx2CF",
   "metadata": {
    "id": "HBicVZcDx2CF"
   },
   "source": [
    "## ML_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XxU4wllsEUcN",
   "metadata": {
    "id": "XxU4wllsEUcN"
   },
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Inline feature creation logic from create_features function ---\n",
    "kalman_periods = [300, 600, 900]\n",
    "kalman_smooth_kal = 3\n",
    "\n",
    "# Kalman and Derivatives\n",
    "t0 = time.time()\n",
    "kal_cols = []\n",
    "for period in tqdm(kalman_periods, desc=\"Kalman & Derivatives\"):\n",
    "    kal = pd.Series(\n",
    "        kalman_line(df_5min['Close'], kalman_length=period, smooth=kalman_smooth_kal),index=df_5min.index)\n",
    "\n",
    "    if kal.isna().any():\n",
    "        kal = kal.ffill()\n",
    "        if kal.isna().any():\n",
    "            kal = kal.bfill()\n",
    "    kname = f'Kal_{period}'\n",
    "    kal_cols.append(kname)\n",
    "    df_5min[kname] = kal\n",
    "\n",
    "tqdm.write(f\"[Timing] Kalman block: {time.time()-t0:.2f}s\")\n",
    "\n",
    "# --- End of inlined feature creation logic ---\n",
    "\n",
    "# Build DRL-friendly Kalman derivative that flips with trade direction\n",
    "kal_900_diff = None\n",
    "if 'Kal_900_diff' in df_5min.columns:\n",
    "    kal_900_diff = df_5min['Kal_900_diff'].copy()\n",
    "elif 'Kal_900' in df_5min.columns:\n",
    "    kal_900_diff = df_5min['Kal_900'].diff()\n",
    "    df_5min['Kal_900_diff'] = kal_900_diff\n",
    "\n",
    "if kal_900_diff is not None:\n",
    "    kal_900_diff = kal_900_diff.fillna(0)\n",
    "    trade_type_series = df_5min.get('trade type')\n",
    "    if trade_type_series is None:\n",
    "        trade_type_series = pd.Series(np.nan, index=df_5min.index)\n",
    "\n",
    "    trade_type_filled = trade_type_series.fillna(-1)\n",
    "    trade_change_mask = trade_type_filled.ne(trade_type_filled.shift())\n",
    "    if not trade_change_mask.empty:\n",
    "        trade_change_mask.iloc[0] = True\n",
    "\n",
    "\n",
    "# Apply specific scaling and min-max scaling to Kalman features within df_5min\n",
    "kalman_features_to_scale = df_5min[['Kal_300', 'Kal_600', 'Kal_900']].copy()\n",
    "scaled_kalman = scale_feature_block(kalman_features_to_scale, window=900)\n",
    "minmax_kalman = apply_minmax_scaling(kalman_features_to_scale, windows=(3,), prefix=\"minmax_\")\n",
    "\n",
    "# Rename columns with specified prefixes and merge into df_5min\n",
    "scaled_kalman = scaled_kalman.rename(columns={col: f\"scale_{col}\" for col in scaled_kalman.columns})\n",
    "minmax_kalman = minmax_kalman.rename(columns={col: f\"{col}\" for col in minmax_kalman.columns})\n",
    "\n",
    "# Calculate and add the difference for scale_Kal_300\n",
    "if 'scale_Kal_300' in scaled_kalman.columns:\n",
    "    scale_kal_300 = scaled_kalman['scale_Kal_300'].copy()\n",
    "    scale_kal_300 = scale_kal_300.ffill().bfill()\n",
    "    scale_kal_300_diff = scale_kal_300.diff().fillna(0)\n",
    "    scaled_kalman['scale_Kal_300_diff'] = scale_kal_300_diff\n",
    "\n",
    "    kalman_input = scale_kal_300_diff.ffill().bfill()\n",
    "    scaled_kalman['kalman_scale_Kal_300_diff'] = pd.Series(\n",
    "        kalman_line(kalman_input.values, kalman_length=300, smooth=3),\n",
    "        index=scaled_kalman.index\n",
    "    )\n",
    "    scaled_kalman['kalman_scale_Kal_300_diff'] = scaled_kalman['kalman_scale_Kal_300_diff'].ffill().bfill()\n",
    "\n",
    "# Merge the scaled and minmax features into df_5min\n",
    "df_5min = df_5min.merge(scaled_kalman, left_index=True, right_index=True, how='left')\n",
    "df_5min = df_5min.merge(minmax_kalman, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Build Kal_900_ml indicator from scale_Kal_900 and trade type\n",
    "scale_kal_900_col = df_5min.get('scale_Kal_900')\n",
    "if scale_kal_900_col is not None:\n",
    "    scale_kal_900 = pd.to_numeric(scale_kal_900_col, errors='coerce')\n",
    "    scale_kal_900 = scale_kal_900.ffill().bfill()\n",
    "    trade_type_series = df_5min.get('trade type')\n",
    "    if trade_type_series is None:\n",
    "        trade_type_series = pd.Series(np.nan, index=df_5min.index)\n",
    "    trade_type_numeric = pd.to_numeric(trade_type_series, errors='coerce')\n",
    "    trade_type_filled = trade_type_numeric.ffill().fillna(0)\n",
    "    trade_change_mask = trade_type_filled.ne(trade_type_filled.shift())\n",
    "    if not trade_change_mask.empty:\n",
    "        trade_change_mask.iloc[0] = True\n",
    "    signed_scale_diff = scale_kal_900.diff().fillna(0)\n",
    "    trade_sign = np.where(trade_type_filled == 1, 1, np.where(trade_type_filled == 0, -1, 0))\n",
    "    trade_sign_series = pd.Series(trade_sign, index=df_5min.index)\n",
    "    signed_scale_diff = signed_scale_diff * trade_sign_series\n",
    "    signed_scale_diff.loc[trade_change_mask] = 0\n",
    "    segment_ids = trade_change_mask.cumsum()\n",
    "    kal_900_ml = signed_scale_diff.groupby(segment_ids).cumsum()\n",
    "    kal_900_ml.loc[trade_change_mask] = 0\n",
    "    df_5min['Kal_900_ml'] = kal_900_ml\n",
    "else:\n",
    "    df_5min['Kal_900_ml'] = np.nan\n",
    "\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Number of features are: {len(df_5min.columns)}\")\n",
    "print(df_5min.shape)\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "# Remove saving to separate files\n",
    "print(df_5min.columns)\n",
    "df_5min.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BFVUCAe3GWFy",
   "metadata": {
    "id": "BFVUCAe3GWFy"
   },
   "outputs": [],
   "source": [
    "print(\"NaN counts per column (scaled, sorted):\")\n",
    "print(df_5min.isnull().sum().sort_values(ascending=False), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Yfm6e8PvCuo",
   "metadata": {
    "id": "6Yfm6e8PvCuo"
   },
   "outputs": [],
   "source": [
    "print(df_5min.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jnNC6Sn8X6he",
   "metadata": {
    "id": "jnNC6Sn8X6he"
   },
   "outputs": [],
   "source": [
    "df_5min.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PjonaGiBX0e4",
   "metadata": {
    "id": "PjonaGiBX0e4"
   },
   "outputs": [],
   "source": [
    "df_5min.to_csv(root_data+'/Results/'+symbol+'_'+strategy+'_'+time_frame+'_ML_Features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1Pk4Hkfjhvt",
   "metadata": {
    "id": "W1Pk4Hkfjhvt"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYpp7GEBEing",
   "metadata": {
    "id": "wYpp7GEBEing"
   },
   "outputs": [],
   "source": [
    "# Define the row range for plotting\n",
    "start_row = 8000\n",
    "end_row = 8500\n",
    "\n",
    "# Select the data for the specified row range from the scaled features\n",
    "plot_df = df_5min.iloc[start_row:end_row].copy()\n",
    "plot_df['Date'] = plot_df.index # Add Date column from index for plotting\n",
    "\n",
    "# Define the columns to plot in each chart\n",
    "chart1_cols = ['Close','Kal_900']\n",
    "chart2_cols = ['Kal_900_diff']\n",
    "chart3_cols = ['Kal_900_ml']\n",
    "chart4_cols = ['trade type']\n",
    "\n",
    "\n",
    "# Determine the number of subplots (only include charts with available columns in plot_df)\n",
    "num_plots = 0\n",
    "if any(col in plot_df.columns for col in chart1_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart2_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart3_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart4_cols): num_plots += 1\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(15, 4 * num_plots), sharex=True)\n",
    "\n",
    "# Ensure axes is an array even if only one plot\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "current_plot_index = 0\n",
    "\n",
    "# Plot Chart 1: Close Price and Scale Kalman indicators\n",
    "if any(col in plot_df.columns for col in chart1_cols):\n",
    "    ax1 = axes[current_plot_index] # Assign the current axis to ax1\n",
    "    ax1.plot(plot_df['Date'], plot_df[chart1_cols])\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(chart1_cols, loc='upper left') # Add loc for clarity\n",
    "\n",
    "    # Create a secondary y-axis for the 'trade type'\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(plot_df['Date'], plot_df[chart4_cols[0]], color='red', linestyle='--', drawstyle='steps-post') # Plot trade type on ax2\n",
    "    ax2.set_ylabel(chart4_cols[0], color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.legend(chart4_cols, loc='upper right') # Add legend for the secondary axis\n",
    "\n",
    "    current_plot_index += 1\n",
    "\n",
    "\n",
    "# Plot Chart 2: Kalman Scale Kalman Diff indicator\n",
    "if any(col in plot_df.columns for col in chart2_cols):\n",
    "    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart2_cols])\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart2_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "# Plot Chart 3: M5 Scale Kalman Change indicators (MinMax 3)\n",
    "if any(col in plot_df.columns for col in chart3_cols):\n",
    "    axes[current_plot_index].plot(plot_df['Date'], plot_df[chart3_cols])\n",
    "    axes[current_plot_index].set_title('M5 Scale Kalman Change (MinMax 3)')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart3_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5c__w6s_dtY",
   "metadata": {
    "id": "A5c__w6s_dtY"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4j7J8tk_f3o-",
   "metadata": {
    "id": "4j7J8tk_f3o-"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FxoyVcmUnJQ6",
   "metadata": {
    "id": "FxoyVcmUnJQ6"
   },
   "outputs": [],
   "source": [
    "df_5min['Date'] = pd.to_datetime(df_5min['Date'])\n",
    "df_5min = df_5min.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZ1Skv4Icdrq",
   "metadata": {
    "id": "nZ1Skv4Icdrq"
   },
   "outputs": [],
   "source": [
    "#df_5min.loc[:,['Close','Open_Trade', 'Close_Trade', 'Entry_Date', 'Type', 'Trade_Number', 'st_Exit_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85DATjQqZd66",
   "metadata": {
    "id": "85DATjQqZd66"
   },
   "outputs": [],
   "source": [
    "# --- Parámetros / campos\n",
    "result_field = 'st_atr_max_PnL'\n",
    "\n",
    "valid = (\n",
    "    (df_5min['Open_Trade'].isin([1, -1])) &\n",
    "    (df_5min[result_field].notna()))\n",
    "\n",
    "# --- Etiquetado en la columna \"df_5minel\" con valores 4/5/6\n",
    "df_5min['label'] = np.nan\n",
    "df_5min.loc[valid & (df_5min[result_field] <= 1), 'label'] = 0\n",
    "df_5min.loc[valid & (df_5min[result_field] >= 1), 'label'] = 1\n",
    "\n",
    "\n",
    "# --- Mantener solo filas válidas y con label\n",
    "df_5min = df_5min.loc[valid & df_5min['label'].notna()].copy()\n",
    "df_5min['label'] = df_5min['label'].astype('int8')\n",
    "\n",
    "# --- Ver distribución de labels 4/5/6\n",
    "print('\\nValue counts de label 4/5/6:')\n",
    "print(df_5min['label'].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_BCdqt3Y0Fjm",
   "metadata": {
    "id": "_BCdqt3Y0Fjm"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gX04ZTmfz-pK",
   "metadata": {
    "id": "gX04ZTmfz-pK"
   },
   "outputs": [],
   "source": [
    "# Use the in-memory raw feature block instead of reading from disk\n",
    "if 'raw_feat_5min' not in globals():\n",
    "    raise RuntimeError('raw_feat_5min is not available. Run the feature generation cell first.')\n",
    "\n",
    "raw_feat_5min = raw_feat_5min.copy()\n",
    "raw_feat_5min['Date'] = pd.to_datetime(raw_feat_5min['Date'], errors='coerce')\n",
    "print(raw_feat_5min.shape)\n",
    "list(raw_feat_5min.columns)\n",
    "#raw_feat_5min.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5gxf0ke0KUx",
   "metadata": {
    "id": "D5gxf0ke0KUx"
   },
   "outputs": [],
   "source": [
    "# Use the in-memory scaled feature block instead of reading from disk\n",
    "if 'scale_feat_5min' not in globals():\n",
    "    raise RuntimeError('scale_feat_5min is not available. Run the feature generation cell first.')\n",
    "\n",
    "scale_feat_5min = scale_feat_5min.copy()\n",
    "scale_feat_5min['Date'] = pd.to_datetime(scale_feat_5min['Date'], errors='coerce')\n",
    "print(scale_feat_5min.shape)\n",
    "list(scale_feat_5min.columns)\n",
    "#scale_feat_5min.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hv5bsrpc03z7",
   "metadata": {
    "id": "Hv5bsrpc03z7"
   },
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I0gkNVT60Ka5",
   "metadata": {
    "id": "I0gkNVT60Ka5"
   },
   "outputs": [],
   "source": [
    "# Initialize feature_df with the first dataframe to merge\n",
    "df_base = df_5min.reset_index()\n",
    "df = df_base[['Date', 'label', 'Open_Trade', 'kal_1', 'kal_2', 'kal_3', 'kal_4', 'Close', 'Kal_900_ml']].copy()\n",
    "\n",
    "df = df.merge(raw_feat_5min, on='Date', how='left')\n",
    "print('Merged raw_feat_5min.')\n",
    "\n",
    "df = df.merge(scale_feat_5min, on='Date', how='left')\n",
    "print('Merged scale_feat_5min.')\n",
    "\n",
    "df = df.sort_values('Date').set_index('Date').ffill().reset_index()\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "combined_feature_path = results_dir / f\"{symbol}_{direction}_AllFeatures.csv\"\n",
    "df.to_csv(combined_feature_path, index=False)\n",
    "\n",
    "# keep df for ML workflow and update df_5min to include the computed features\n",
    "df_5min = df.set_index('Date')\n",
    "\n",
    "print(f\"\n",
    "Combined feature dataframe shape: {df.shape}\")\n",
    "print(f\"Saved combined feature dataframe to: {combined_feature_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jXcXWty506Ur",
   "metadata": {
    "id": "jXcXWty506Ur"
   },
   "outputs": [],
   "source": [
    "print(df.shape,'\\n')\n",
    "print('Label_Counts : ',df.label.value_counts(),'\\n')\n",
    "print(list(df.columns), '\\n')\n",
    "\n",
    "# Add NaN count per column, sorted\n",
    "print(\"NaN counts per column (sorted):\")\n",
    "print(df.isnull().sum().sort_values(ascending=False), '\\n')\n",
    "\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VeHj2KkJZfox",
   "metadata": {
    "id": "VeHj2KkJZfox"
   },
   "outputs": [],
   "source": [
    "# Define the row range for plotting (adjust as needed)\n",
    "start_row = 8000\n",
    "end_row = 8500\n",
    "\n",
    "# Select the data for the specified row range from df\n",
    "plot_df = df.iloc[start_row:end_row].copy()\n",
    "\n",
    "# Ensure 'Date' column is datetime type and set as index for plotting\n",
    "plot_df['Date'] = pd.to_datetime(plot_df['Date'])\n",
    "plot_df = plot_df.set_index('Date')\n",
    "\n",
    "\n",
    "# Define the columns to plot in each chart\n",
    "chart1_cols = ['Close']\n",
    "chart2_cols = ['scale_Kal_300', 'scale_Kal_600', 'scale_Kal_900']\n",
    "chart3_cols = ['kalman_scale_Kal_300_diff']\n",
    "chart4_cols = ['Open_Trade'] # Assuming 'Type' is represented by 'Open_Trade' in df based on previous cells\n",
    "\n",
    "# Determine the number of subplots (only include charts with available columns in plot_df)\n",
    "num_plots = 0\n",
    "if any(col in plot_df.columns for col in chart1_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart2_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart3_cols): num_plots += 1\n",
    "if any(col in plot_df.columns for col in chart4_cols): num_plots += 1\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(15, 4 * num_plots), sharex=True)\n",
    "\n",
    "# Ensure axes is an array even if only one plot\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "current_plot_index = 0\n",
    "\n",
    "# Plot Chart 1: Close Price\n",
    "if any(col in plot_df.columns for col in chart1_cols):\n",
    "    axes[current_plot_index].plot(plot_df.index, plot_df[chart1_cols])\n",
    "    axes[current_plot_index].set_title('Close Price')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart1_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "\n",
    "# Plot Chart 2: Scale Kalman indicators\n",
    "if any(col in plot_df.columns for col in chart2_cols):\n",
    "    axes[current_plot_index].plot(plot_df.index, plot_df[chart2_cols])\n",
    "    axes[current_plot_index].set_title('Scale Kalman')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart2_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "# Plot Chart 3: Kalman Scale Kalman Diff indicator\n",
    "if any(col in plot_df.columns for col in chart3_cols):\n",
    "    axes[current_plot_index].plot(plot_df.index, plot_df[chart3_cols])\n",
    "    axes[current_plot_index].set_title('Kalman Scale Kalman Diff')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart3_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "# Plot Chart 4: Trade Type (Open_Trade)\n",
    "if any(col in plot_df.columns for col in chart4_cols):\n",
    "    # For 'Open_Trade' which can be 1, -1, or 0, plotting directly might not be ideal.\n",
    "    # We can scatter plot the entry points or plot a line representing the trade state.\n",
    "    # Here, we'll plot the 'Open_Trade' values directly to show when trades are open.\n",
    "    axes[current_plot_index].plot(plot_df.index, plot_df[chart4_cols], drawstyle='steps-post')\n",
    "    axes[current_plot_index].set_title('Open Trade Status')\n",
    "    axes[current_plot_index].grid(True)\n",
    "    axes[current_plot_index].legend(chart4_cols)\n",
    "    current_plot_index += 1\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lBtQecOO08zB",
   "metadata": {
    "id": "lBtQecOO08zB"
   },
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rlkmypd9DJj",
   "metadata": {
    "id": "8rlkmypd9DJj"
   },
   "outputs": [],
   "source": [
    "# ===================== 1. ENTRENAR Y OBTENER IMPORTANCIAS =====================\n",
    "def compute_xgb_importance(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    random_state: int = 42,\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[pd.DataFrame, Any]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost y devuelve:\n",
    "      - imp_df: DataFrame con 'feature', 'importance' y 'cum_importance'.\n",
    "      - model : modelo ya entrenado.\n",
    "\n",
    "    Soporta:\n",
    "      • Clasificación binaria o multiclase (detecta nº de clases).\n",
    "      • Regresión (si task != 'classification').\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de características (sin la columna objetivo).\n",
    "    y : pd.Series\n",
    "        Etiquetas objetivo. Puede ser binaria (0/1) o multiclase (0..K-1).\n",
    "    task : str, opcional\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    random_state : int, opcional\n",
    "        Semilla para reproducibilidad.\n",
    "    **xgb_params : dict\n",
    "        Parámetros adicionales para el estimador de XGBoost.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (imp_df, model)\n",
    "        imp_df : DataFrame con importancias y su acumulado.\n",
    "        model  : instancia entrenada de XGBClassifier / XGBRegressor.\n",
    "    \"\"\"\n",
    "    default_params: Dict[str, Any] = dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    default_params.update(xgb_params)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        # Detectar nº de clases\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        # XGBClassifier ajusta objetivo automáticamente, pero lo explicitamos:\n",
    "        if n_classes > 2:\n",
    "            default_params.setdefault(\"objective\", \"multi:softprob\")\n",
    "            default_params.setdefault(\"num_class\", n_classes)\n",
    "            eval_metric = \"mlogloss\"\n",
    "        else:\n",
    "            default_params.setdefault(\"objective\", \"binary:logistic\")\n",
    "            eval_metric = \"logloss\"\n",
    "\n",
    "        model = XGBClassifier(eval_metric=eval_metric, **default_params)\n",
    "\n",
    "    else:\n",
    "        model = XGBRegressor(**default_params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance\": model.feature_importances_\n",
    "        })\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    total_imp = imp_df[\"importance\"].sum()\n",
    "    if total_imp == 0:\n",
    "        # Evitar división por cero si el modelo devuelve todo cero (raro, pero posible)\n",
    "        imp_df[\"cum_importance\"] = 0.0\n",
    "    else:\n",
    "        imp_df[\"cum_importance\"] = imp_df[\"importance\"].cumsum() / total_imp\n",
    "\n",
    "    return imp_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3r9Di4Xx9DOU",
   "metadata": {
    "id": "3r9Di4Xx9DOU"
   },
   "outputs": [],
   "source": [
    "# ===================== 2. SELECCIÓN DE FEATURES =====================\n",
    "def select_features_with_importance(\n",
    "    X: pd.DataFrame,\n",
    "    imp_df: pd.DataFrame,\n",
    "    top_n: Optional[int] = None,\n",
    "    threshold: Optional[str | float] = None,\n",
    "    cum_threshold: Optional[float] = 0.8\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Selección flexible de variables a partir de importancias de XGBoost.\n",
    "\n",
    "    Reglas:\n",
    "      - Si top_n no es None           => usa el top_n.\n",
    "      - Else si cum_threshold no None => usa importancia acumulada (p.ej. 0.8 = 80%).\n",
    "      - Else usa threshold ('median', 'mean' o valor numérico).\n",
    "\n",
    "    Devuelve (X_reducido, lista_de_features).\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Matriz de características original.\n",
    "    imp_df : pd.DataFrame\n",
    "        DataFrame devuelto por compute_xgb_importance.\n",
    "    top_n : int | None\n",
    "        Número fijo de variables a conservar.\n",
    "    threshold : str | float | None\n",
    "        Umbral de importancia. Si str, usar 'median' o 'mean'.\n",
    "    cum_threshold : float | None\n",
    "        Porcentaje acumulado de importancia (0-1). Si None, se ignora.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_sel, keep)\n",
    "        X_sel : subset de X con columnas seleccionadas.\n",
    "        keep  : lista de nombres de columnas seleccionadas.\n",
    "    \"\"\"\n",
    "    if top_n is not None:\n",
    "        keep = imp_df.head(top_n)[\"feature\"].tolist()\n",
    "\n",
    "    elif cum_threshold is not None:\n",
    "        keep_mask = imp_df[\"cum_importance\"] <= float(cum_threshold)\n",
    "        keep = imp_df.loc[keep_mask, \"feature\"].tolist()\n",
    "        # asegurar que haya al menos una más para no quedarnos exactamente en el corte\n",
    "        if len(keep) < len(imp_df):\n",
    "            keep.append(imp_df.iloc[len(keep)][\"feature\"])\n",
    "\n",
    "    else:\n",
    "        if threshold is None:\n",
    "            threshold = \"median\"\n",
    "        if isinstance(threshold, str):\n",
    "            thr_val = imp_df[\"importance\"].agg(threshold)\n",
    "        else:\n",
    "            thr_val = float(threshold)\n",
    "        keep = imp_df.loc[imp_df[\"importance\"] >= thr_val, \"feature\"].tolist()\n",
    "\n",
    "    return X[keep], keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tCDZCTz2_v9z",
   "metadata": {
    "id": "tCDZCTz2_v9z"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. BÚSQUEDA DEL MEJOR UMBRAL ACUMULADO =====================\n",
    "def find_best_cum_threshold(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: pd.Series,\n",
    "    task: str = \"classification\",\n",
    "    thresholds: Tuple[float, ...] = (0.6, 0.7, 0.8, 0.9),\n",
    "    random_state: int = 42,\n",
    "    metric: str = \"auto\",\n",
    "    **xgb_params: Any\n",
    ") -> Tuple[float, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Entrena un XGB en train, calcula importancias y prueba varios umbrales\n",
    "    acumulados para ver cuál da la mejor métrica en valid.\n",
    "\n",
    "    Para CLASIFICACIÓN:\n",
    "        - Detecta nº de clases.\n",
    "        - Métrica por defecto (metric=\"auto\"):\n",
    "            • Binaria: ROC-AUC (probabilidades de la clase positiva).\n",
    "            • Multiclase: ROC-AUC macro OVR (usa predict_proba).\n",
    "          Alternativas: metric=\"f1_macro\", \"accuracy\", \"logloss\" (se MINIMIZA).\n",
    "    Para REGRESIÓN:\n",
    "        - Usa R^2.\n",
    "\n",
    "    Devuelve:\n",
    "        best_thr, res_df_ordenado_por_score_desc, imp_df\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X_train, y_train, X_valid, y_valid : pd.DataFrame / pd.Series\n",
    "        Particiones de entrenamiento y validación.\n",
    "    task : str\n",
    "        \"classification\" (default) o \"regression\".\n",
    "    thresholds : tuple[float, ...]\n",
    "        Valores de umbral de importancia acumulada a evaluar (0-1).\n",
    "    random_state : int\n",
    "        Semilla para reproducibilidad.\n",
    "    metric : str\n",
    "        \"auto\" (default), \"roc_auc\", \"f1_macro\", \"accuracy\", \"logloss\" (clasif) o \"r2\" (regresión).\n",
    "    **xgb_params : dict\n",
    "        Parámetros extra para el estimador de XGBoost (pasan a compute y a los modelos internos).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (best_thr, res_df, imp_df)\n",
    "        best_thr : float\n",
    "            Umbral con mejor score (o menor logloss si metric='logloss').\n",
    "        res_df : pd.DataFrame\n",
    "            Tabla con resultados por umbral (n_features, score).\n",
    "        imp_df : pd.DataFrame\n",
    "            Importancias calculadas en X_train / y_train.\n",
    "    \"\"\"\n",
    "    imp_df, _ = compute_xgb_importance(\n",
    "        X_train, y_train, task=task, random_state=random_state, **xgb_params\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Detectar nº de clases si es clasificación\n",
    "    if task == \"classification\":\n",
    "        classes = np.unique(y_train)\n",
    "        n_classes = len(classes)\n",
    "        if metric == \"auto\":\n",
    "            metric_to_use = \"roc_auc\" if n_classes == 2 else \"roc_auc\"\n",
    "        else:\n",
    "            metric_to_use = metric\n",
    "    else:\n",
    "        metric_to_use = \"r2\" if metric == \"auto\" else metric\n",
    "\n",
    "    for thr in thresholds:\n",
    "        X_tr_sel, cols = select_features_with_importance(\n",
    "            X_train, imp_df, cum_threshold=thr, top_n=None, threshold=None\n",
    "        )\n",
    "        X_va_sel = X_valid[cols]\n",
    "\n",
    "        if task == \"classification\":\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "\n",
    "            if n_classes > 2:\n",
    "                params.setdefault(\"objective\", \"multi:softprob\")\n",
    "                params.setdefault(\"num_class\", n_classes)\n",
    "                eval_metric = \"mlogloss\"\n",
    "            else:\n",
    "                params.setdefault(\"objective\", \"binary:logistic\")\n",
    "                eval_metric = \"logloss\"\n",
    "\n",
    "            model_sel = XGBClassifier(eval_metric=eval_metric, **params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "\n",
    "            # Probabilidades y predicciones\n",
    "            proba = model_sel.predict_proba(X_va_sel)\n",
    "            pred  = np.argmax(proba, axis=1) if n_classes > 2 else (proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "            # Calcular métrica\n",
    "            if metric_to_use == \"roc_auc\":\n",
    "                if n_classes == 2:\n",
    "                    score = roc_auc_score(y_valid, proba[:, 1])\n",
    "                else:\n",
    "                    # AUC macro One-vs-Rest\n",
    "                    score = roc_auc_score(y_valid, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "            elif metric_to_use == \"f1_macro\":\n",
    "                score = f1_score(y_valid, pred, average=\"macro\")\n",
    "            elif metric_to_use == \"accuracy\":\n",
    "                score = accuracy_score(y_valid, pred)\n",
    "            elif metric_to_use == \"logloss\":\n",
    "                # En este caso, menor es mejor. Guardamos negativo para mantener criterio \"mayor mejor\".\n",
    "                score = -log_loss(y_valid, proba, labels=np.unique(y_train))\n",
    "            else:\n",
    "                raise ValueError(f\"Métrica no soportada: {metric_to_use}\")\n",
    "\n",
    "        else:\n",
    "            # REGRESIÓN\n",
    "            params = dict(random_state=random_state, n_jobs=-1, tree_method=\"hist\")\n",
    "            params.update(xgb_params)\n",
    "            model_sel = XGBRegressor(**params)\n",
    "            model_sel.fit(X_tr_sel, y_train)\n",
    "            pred = model_sel.predict(X_va_sel)\n",
    "\n",
    "            if metric_to_use == \"r2\":\n",
    "                score = r2_score(y_valid, pred)\n",
    "            else:\n",
    "                raise ValueError(f\"Métrica de regresión no soportada: {metric_to_use}\")\n",
    "\n",
    "        results.append({\"cum_threshold\": thr, \"n_features\": len(cols), \"score\": score})\n",
    "\n",
    "    # Ordenar (si usamos logloss negado, mayor sigue siendo mejor)\n",
    "    res_df = pd.DataFrame(results).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    best_thr = float(res_df.iloc[0][\"cum_threshold\"])\n",
    "    return best_thr, res_df, imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0Dm8ZPGcBr7",
   "metadata": {
    "id": "R0Dm8ZPGcBr7"
   },
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "\n",
    "    # Solo numéricos para evitar errores y acelerar\n",
    "    corr_matrix = df.corr(numeric_only=True).abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1))\n",
    "\n",
    "    to_drop = []\n",
    "    for col in tqdm(upper.columns, desc=f\"Pruning corr > {threshold}\", unit=\"col\", leave=False):\n",
    "        if (upper[col] > threshold).any():\n",
    "            to_drop.append(col)\n",
    "\n",
    "    return df.drop(columns=to_drop, errors=\"ignore\"), to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WodcQEBJ_wAW",
   "metadata": {
    "id": "WodcQEBJ_wAW"
   },
   "outputs": [],
   "source": [
    "# ===================== 3. PIPELINE PRINCIPAL =====================\n",
    "df = df.dropna()\n",
    "y = df['label']\n",
    "X = df.iloc[:, 2:]\n",
    "\n",
    "# --- 3.3 Split temporal (ejemplo simple 80/20) ---\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# --- 3.4 Remove correlated features ---\n",
    "X_train_filtered, dropped_features = remove_highly_correlated_features(X_train, threshold=0.9)\n",
    "X_test_filtered = X_test.drop(columns=dropped_features)\n",
    "\n",
    "# Baseline logistic regression with time-series CV\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train_filtered)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "baseline = cross_val_score(LogisticRegression(max_iter=1000), X_scaled, y_train, cv=tscv).mean()\n",
    "print('Logistic regression CV accuracy:', baseline)\n",
    "\n",
    "# --- 3.5 Importancias con XGBoost ---\n",
    "imp_df, xgb_model = compute_xgb_importance(X_train_filtered, y_train, task='classification')\n",
    "\n",
    "print('=== Importancias XGBoost ===')\n",
    "print(imp_df.head(20))\n",
    "print(f'Total features: {len(imp_df)}')\n",
    "\n",
    "# --- 3.6 Selección (elige una opción) ---\n",
    "X_train_sel, keep_cols = select_features_with_importance(X_train_filtered, imp_df, cum_threshold=0.8)\n",
    "X_test_sel = X_test_filtered[keep_cols]\n",
    "\n",
    "print(f'Features seleccionadas: {len(keep_cols)}')\n",
    "importance_map = imp_df.set_index(\"feature\")[\"importance\"]\n",
    "selected_importances = pd.DataFrame({\n",
    "    \"feature\": keep_cols,\n",
    "    \"importance\": importance_map.reindex(keep_cols).values\n",
    "})\n",
    "selected_importances.to_csv(results_dir / f\"{symbol}_{direction}_M5M10_{data_type}_ImportantCols.csv\", index=False)\n",
    "\n",
    "# Save dataset with selected features\n",
    "df_selected = df[['Date', 'label'] + keep_cols]\n",
    "df_selected.to_csv(results_dir / f\"{symbol}_{direction}_M5M10_{data_type}_Features.csv\", index=False)\n",
    "\n",
    "# Time-series cross-validation with XGBoost\n",
    "xgb_cv = XGBClassifier(eval_metric='logloss', n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "xgb_scores = cross_val_score(xgb_cv, X_train_sel, y_train, cv=tscv, scoring='accuracy')\n",
    "print('XGBoost CV accuracy:', xgb_scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ysa-7eLvEMpE",
    "m8CIB8vltFfH",
    "y6QRdBKwrX98",
    "zhTndYVi1TEV",
    "jh9-mi26wsya",
    "Hv5bsrpc03z7",
    "2RFfhHT2AwAJ"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
