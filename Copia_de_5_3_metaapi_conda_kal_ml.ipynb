{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eaf03429",
      "metadata": {
        "id": "eaf03429"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nu7i62hlGthO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu7i62hlGthO",
        "outputId": "95c7dfd0-4bd2-4138-cc33-3641e5255bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta-lib\n",
            "  Downloading ta_lib-0.6.7-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from ta-lib) (1.3.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from ta-lib) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta-lib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->ta-lib) (1.2.0)\n",
            "Downloading ta_lib-0.6.7-cp312-cp312-manylinux_2_28_x86_64.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.6.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ej4IeD7EeF0t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej4IeD7EeF0t",
        "outputId": "ff25786e-bb1d-4fc1-e1d0-73f03a2924b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metaapi-cloud-sdk\n",
            "  Downloading metaapi_cloud_sdk-29.0.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (3.12.15)\n",
            "Collecting python-engineio<4.0.0,>=3.14.2 (from metaapi-cloud-sdk)\n",
            "  Downloading python_engineio-3.14.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (4.15.0)\n",
            "Collecting iso8601 (from metaapi-cloud-sdk)\n",
            "  Downloading iso8601-2.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2025.2)\n",
            "Collecting python-socketio<5.0.0,>=4.6.0 (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk)\n",
            "  Downloading python_socketio-4.6.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from metaapi-cloud-sdk) (0.28.1)\n",
            "Collecting metaapi-cloud-copyfactory-sdk<13.0.0,>=12.0.0 (from metaapi-cloud-sdk)\n",
            "  Downloading metaapi_cloud_copyfactory_sdk-12.0.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting metaapi-cloud-metastats-sdk<7.0.0,>=6.0.0 (from metaapi-cloud-sdk)\n",
            "  Downloading metaapi_cloud_metastats_sdk-6.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting rapidfuzz (from metaapi-cloud-sdk)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4->metaapi-cloud-sdk) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio[asyncio_client]<5.0.0,>=4.6.0->metaapi-cloud-sdk) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->metaapi-cloud-sdk) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->metaapi-cloud-sdk) (1.3.1)\n",
            "Downloading metaapi_cloud_sdk-29.0.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.6/178.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading metaapi_cloud_copyfactory_sdk-12.0.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading metaapi_cloud_metastats_sdk-6.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading python_engineio-3.14.2-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_socketio-4.6.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, python-engineio, iso8601, python-socketio, pandas, metaapi-cloud-metastats-sdk, metaapi-cloud-copyfactory-sdk, metaapi-cloud-sdk\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed iso8601-2.1.0 metaapi-cloud-copyfactory-sdk-12.0.0 metaapi-cloud-metastats-sdk-6.0.0 metaapi-cloud-sdk-29.0.0 pandas-2.3.2 python-engineio-3.14.2 python-socketio-4.6.1 rapidfuzz-3.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade metaapi-cloud-sdk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DF5CD6j5nmdo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5CD6j5nmdo",
        "outputId": "cd83eb83-e66c-415b-eddc-1de8c05516f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.7\n",
            "ATR(ndarray high, ndarray low, ndarray close, int timeperiod=-0x80000000)\n",
            "\n",
            "ATR(high, low, close[, timeperiod=?])\n",
            "\n",
            "Average True Range (Volatility Indicators)\n",
            "\n",
            "Inputs:\n",
            "    prices: ['high', 'low', 'close']\n",
            "Parameters:\n",
            "    timeperiod: 14\n",
            "Outputs:\n",
            "    real\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function talib._ta_lib.ATR(high, low, close, timeperiod=-2147483648)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import talib as ta\n",
        "print(ta.__version__)  # Should print something like 0.4.28\n",
        "print(ta.ATR.__doc__)  # Confirm ATR function works\n",
        "ta.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda1a01c",
      "metadata": {
        "id": "bda1a01c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime as dt\n",
        "import logging\n",
        "import math\n",
        "import re\n",
        "\n",
        "from typing import Sequence, Tuple, Dict, Any, List, Callable, Optional\n",
        "\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "from metaapi_cloud_sdk import MetaApi\n",
        "from metaapi_cloud_sdk.clients.timeout_exception import TimeoutException\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import json, re, traceback, requests\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ==== MODEL LOADING (place this ABOVE `asyncio.run(main())`) ====\n",
        "from pathlib import Path\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXSLuBFY_SB2",
      "metadata": {
        "id": "KXSLuBFY_SB2"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost\n",
        "\n",
        "# --- Scikit-learn Models ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    VotingClassifier,\n",
        "    ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Scikit-learn Preprocessing ---\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    label_binarize)\n",
        "\n",
        "# --- Scikit-learn Model Selection ---\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV)\n",
        "\n",
        "# --- Scikit-learn Metrics ---\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ============================================================\n",
        "# 4. DEEP LEARNING (TENSORFLOW / KERAS)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# ============================================================\n",
        "# 5. MODEL PERSISTENCE\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7JweuZy755ym",
      "metadata": {
        "id": "7JweuZy755ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KFfn45ty82qv",
      "metadata": {
        "id": "KFfn45ty82qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab97eedd-cb76-4654-b098-b14a652fe7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store GitHub"
      ],
      "metadata": {
        "id": "R-3rtxFqPCFa"
      },
      "id": "R-3rtxFqPCFa"
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_USER = \"holguinmora123\"          # tu usuario de GitHub\n",
        "OWNER       = \"holguinmora123\"          # dueÃ±o del repo (tu user u organizaciÃ³n)\n",
        "REPO        = \"OpenAi_Codex\"            # nombre exacto del repo (sin .git)\n",
        "BRANCH      = \"main\"                    # rama por defecto (main / master, etc.\n",
        "NOTEBOOK = \"5_3_MetaApi_Conda_Kal+ML.ipynb\""
      ],
      "metadata": {
        "id": "08vPxGnKXlrL"
      },
      "id": "08vPxGnKXlrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Colab â†’ GitHub: exportar el notebook abierto y pushear (versiÃ³n corregida) ====\n",
        "import os, subprocess, urllib.parse, shlex, getpass, json, time\n",
        "\n",
        "# -------- utilidades --------\n",
        "def run(cmd: str, secrets: list[str] | None = None) -> None:\n",
        "    secrets = secrets or []\n",
        "    shown = cmd\n",
        "    for s in secrets:\n",
        "        shown = shown.replace(s, \"***\")\n",
        "    print(f\"$ {shown}\")\n",
        "    p = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
        "    out, err = (p.stdout or \"\"), (p.stderr or \"\")\n",
        "    for s in secrets:\n",
        "        out = out.replace(s, \"***\"); err = err.replace(s, \"***\")\n",
        "    if out.strip(): print(out, end=\"\")\n",
        "    if err.strip(): print(err, end=\"\")\n",
        "    if p.returncode != 0:\n",
        "        raise SystemExit(f\"Comando fallÃ³ con cÃ³digo {p.returncode}\")\n",
        "\n",
        "def build_github_url(user: str, owner: str, repo: str, token: str) -> str:\n",
        "    import urllib.parse\n",
        "    safe = urllib.parse.quote(token.strip(), safe=\"\")\n",
        "    return f\"https://x-access-token:{safe}@github.com/{owner}/{repo}.git\"\n",
        "\n",
        "\n",
        "def git_clear_cached_credentials() -> None:\n",
        "    \"\"\"Evita que helpers viejos inyecten credenciales incorrectas.\"\"\"\n",
        "    run('git config --global --unset credential.helper || true')\n",
        "\n",
        "def git_preflight_and_push(url: str, repo_dir: str, branch: str, token: str) -> None:\n",
        "    \"\"\"\n",
        "    - Verifica autenticaciÃ³n (ls-remote).\n",
        "    - Comprueba permisos de escritura con un push --dry-run.\n",
        "    - Si no hay permiso de escritura, muestra diagnÃ³stico claro.\n",
        "    - Si todo OK, hace el push real con helpers deshabilitados.\n",
        "    \"\"\"\n",
        "    # 1) Â¿El token autentica? (lectura)\n",
        "    run(f'git ls-remote \"{url}\" HEAD', secrets=[token])\n",
        "\n",
        "    # 2) Â¿Tiene permiso de escritura? ComprobaciÃ³n sin subir nada\n",
        "    dry_run_cmd = f'git -C \"{repo_dir}\" -c credential.helper= -c http.emptyAuth=true push --dry-run origin {branch}'\n",
        "    try:\n",
        "        run(dry_run_cmd, secrets=[token])\n",
        "    except SystemExit as e:\n",
        "        msg = str(e)\n",
        "        print(\"\\nğŸ§ª DiagnÃ³stico push --dry-run fallÃ³.\")\n",
        "        print(\"Posibles causas y cÃ³mo resolverlas:\")\n",
        "        print(\"  â€¢ PAT fine-grained sin permiso: activa Repository permissions â†’ Contents: Read and write.\")\n",
        "        print(\"  â€¢ PAT expirado o revocado: crea uno nuevo.\")\n",
        "        print(\"  â€¢ Repo incorrecto/OWNER mal: verifica OWNER/REPO.\")\n",
        "        print(\"  â€¢ Si fuera repo de organizaciÃ³n: autoriza el token con SSO (Configure SSO â†’ Authorize).\")\n",
        "        # Re-lanzar para mantener el comportamiento de error en tu flujo\n",
        "        raise\n",
        "\n",
        "    # 3) Push real\n",
        "    run(f'git -C \"{repo_dir}\" -c credential.helper= -c http.emptyAuth=true push origin {branch}', secrets=[token])\n",
        "\n",
        "\n",
        "# -------- CONFIGURA AQUÃ --------\n",
        "GITHUB_USER = \"holguinmora123\"            # tu usuario de GitHub\n",
        "OWNER       = \"holguinmora123\"            # dueÃ±o del repo (usuario u org)\n",
        "REPO        = \"OpenAi_Codex\"              # repo\n",
        "BRANCH      = \"main\"                      # rama\n",
        "TARGET_IPYNB = \"5_3_MetaApi_Conda_Kal+ML.ipynb\"  # nombre del archivo dentro del repo\n",
        "\n",
        "# -------- TOKEN / REMOTO --------\n",
        "TOKEN = getpass.getpass(\"Pega tu Personal Access Token (PAT) de GitHub: \").strip()\n",
        "if not TOKEN:\n",
        "    raise SystemExit(\"No pegaste el token.\")\n",
        "URL = build_github_url(GITHUB_USER, OWNER, REPO, TOKEN)\n",
        "print(\"URL OK:\", URL.replace(TOKEN, \"***\"))\n",
        "\n",
        "# -------- preparar repo local --------\n",
        "os.makedirs(\"/content\", exist_ok=True)\n",
        "os.chdir(\"/content\")\n",
        "DEST = f\"/content/{REPO}\"\n",
        "\n",
        "if not os.path.exists(DEST):\n",
        "    print(f\"Clonando {REPO}â€¦\")\n",
        "    run(f'git clone \"{URL}\" \"{DEST}\"', secrets=[TOKEN])\n",
        "else:\n",
        "    print(f\"Repo {REPO} ya existe, actualizando remotoâ€¦\")\n",
        "    run(f'git -C \"{DEST}\" remote set-url origin \"{URL}\"', secrets=[TOKEN])\n",
        "\n",
        "# -------- EXPORTAR el notebook ABIERTO en Colab y escribirlo en el repo --------\n",
        "# Esto captura el contenido del notebook que estÃ¡s editando (Gist o cualquier origen)\n",
        "from google.colab import _message  # API interna de Colab\n",
        "nb = _message.blocking_request('get_ipynb')  # obtiene JSON del notebook activo\n",
        "\n",
        "target_path = f\"{DEST}/{TARGET_IPYNB}\"\n",
        "with open(target_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb[\"ipynb\"], f, ensure_ascii=False)\n",
        "\n",
        "# Asegurar mtime distinto para que git detecte cambios aunque sÃ³lo sea metadata\n",
        "os.utime(target_path, (time.time(), time.time()))\n",
        "\n",
        "# -------- commit + push --------\n",
        "# Config git (global) y commit dentro del repo\n",
        "run(f'git config --global user.name \"{GITHUB_USER}\"')\n",
        "run('git config --global user.email \"alejandro@inflexion.com.co\"')\n",
        "\n",
        "# Hacemos add/commit en el directorio del repo\n",
        "run(f'git -C \"{DEST}\" add \"{TARGET_IPYNB}\"')\n",
        "run(f'git -C \"{DEST}\" commit -m \"Auto-export desde Colab â†’ {TARGET_IPYNB}\" || true')\n",
        "\n",
        "# Limpiar helpers y pushear usando el PAT embebido en el remoto\n",
        "git_clear_cached_credentials()\n",
        "git_preflight_and_push(URL, DEST, BRANCH, TOKEN)\n",
        "\n",
        "\n",
        "print(f\"âœ… Subido: {TARGET_IPYNB} actualizado en {OWNER}/{REPO}@{BRANCH}\")\n"
      ],
      "metadata": {
        "id": "saIMYjcDTQJD",
        "outputId": "f4250825-37fd-475f-b090-95cfe0d6b7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "id": "saIMYjcDTQJD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pega tu Personal Access Token (PAT) de GitHub: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "URL OK: https://x-access-token:***@github.com/holguinmora123/OpenAi_Codex.git\n",
            "Repo OpenAi_Codex ya existe, actualizando remotoâ€¦\n",
            "$ git -C \"/content/OpenAi_Codex\" remote set-url origin \"https://x-access-token:***@github.com/holguinmora123/OpenAi_Codex.git\"\n",
            "$ git config --global user.name \"holguinmora123\"\n",
            "$ git config --global user.email \"alejandro@inflexion.com.co\"\n",
            "$ git -C \"/content/OpenAi_Codex\" add \"5_3_MetaApi_Conda_Kal+ML.ipynb\"\n",
            "$ git -C \"/content/OpenAi_Codex\" commit -m \"Auto-export desde Colab â†’ 5_3_MetaApi_Conda_Kal+ML.ipynb\" || true\n",
            "[main 629d9fc] Auto-export desde Colab â†’ 5_3_MetaApi_Conda_Kal+ML.ipynb\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            "$ git config --global --unset credential.helper || true\n",
            "$ git ls-remote \"https://x-access-token:***@github.com/holguinmora123/OpenAi_Codex.git\" HEAD\n",
            "2600e02ce74adf8ee7f7dd34ba666a9162c59065\tHEAD\n",
            "$ git -C \"/content/OpenAi_Codex\" -c credential.helper= -c http.emptyAuth=true push --dry-run origin main\n",
            "To https://github.com/holguinmora123/OpenAi_Codex.git\n",
            " ! [rejected]        main -> main (fetch first)\n",
            "error: failed to push some refs to 'https://github.com/holguinmora123/OpenAi_Codex.git'\n",
            "hint: Updates were rejected because the remote contains work that you do\n",
            "hint: not have locally. This is usually caused by another repository pushing\n",
            "hint: to the same ref. You may want to first integrate the remote changes\n",
            "hint: (e.g., 'git pull ...') before pushing again.\n",
            "hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n",
            "\n",
            "ğŸ§ª DiagnÃ³stico push --dry-run fallÃ³.\n",
            "Posibles causas y cÃ³mo resolverlas:\n",
            "  â€¢ PAT fine-grained sin permiso: activa Repository permissions â†’ Contents: Read and write.\n",
            "  â€¢ PAT expirado o revocado: crea uno nuevo.\n",
            "  â€¢ Repo incorrecto/OWNER mal: verifica OWNER/REPO.\n",
            "  â€¢ Si fuera repo de organizaciÃ³n: autoriza el token con SSO (Configure SSO â†’ Authorize).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Comando fallÃ³ con cÃ³digo 1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Comando fallÃ³ con cÃ³digo 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passwords"
      ],
      "metadata": {
        "id": "OWQmjRQBsFfG"
      },
      "id": "OWQmjRQBsFfG"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_secrets_from_csv(csv_path: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Lee META_API_TOKEN y ACCOUNT_ID desde un CSV con columnas:\n",
        "    - Description  (e.g. META_API_TOKEN, ACCOUNT_ID)\n",
        "    - Password     (el valor)\n",
        "    Exporta ambos a variables de entorno y los devuelve.\n",
        "    \"\"\"\n",
        "    import os, pandas as pd\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for col in (\"Description\", \"Password\"):\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Falta la columna '{col}' en {csv_path}\")\n",
        "\n",
        "    def pick(key: str) -> str:\n",
        "        s = (df.loc[df[\"Description\"].astype(str).str.upper() == key.upper(), \"Password\"]\n",
        "               .astype(str).str.strip())\n",
        "        if s.empty or not s.iloc[0]:\n",
        "            raise RuntimeError(f\"'{key}' no encontrado o vacÃ­o en {csv_path}\")\n",
        "        return s.iloc[0]\n",
        "\n",
        "    meta_api_token = pick(\"META_API_TOKEN\")\n",
        "    account_id     = pick(\"ACCOUNT_ID\")\n",
        "\n",
        "    # Exporta al entorno para que tu cÃ³digo existente con os.getenv funcione igual\n",
        "    os.environ[\"META_API_TOKEN\"] = meta_api_token\n",
        "    os.environ[\"ACCOUNT_ID\"]     = account_id\n",
        "\n",
        "    print(\"ğŸ” Secrets cargados en entorno: META_API_TOKEN, ACCOUNT_ID\")\n",
        "    return meta_api_token, account_id\n"
      ],
      "metadata": {
        "id": "r92z_3nblM8w"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r92z_3nblM8w"
    },
    {
      "cell_type": "markdown",
      "id": "m8CIB8vltFfH",
      "metadata": {
        "id": "m8CIB8vltFfH"
      },
      "source": [
        "# Set_Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EB5RqjoAtFwl",
      "metadata": {
        "id": "EB5RqjoAtFwl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "process = 'Train'\n",
        "SYMBOL = 'BTCUSD'\n",
        "\n",
        "# OJO: esta ruta apunta a XAUUSD; ajÃºstala si vas a trabajar BTCUSD\n",
        "root_data = '/content/drive/MyDrive/Course Folder/Forex/XAUUSD/'\n",
        "print(root_data)\n",
        "\n",
        "rolling_window = 100\n",
        "\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "LOT      = 1.0\n",
        "COMMENT  = \"Insta_ml\"\n",
        "CANDLE_NUMBER = 100\n",
        "\n",
        "stoploss_1    = 1.0\n",
        "takeprofit_1  = 0.7\n",
        "\n",
        "stoploss_2    = 1.0\n",
        "takeprofit_2  = 2.0\n",
        "\n",
        "# Si estÃ¡s en Colab/Unix, exporta antes:\n",
        "META_API_TOKEN, ACCOUNT_ID = load_secrets_from_csv(\"/content/drive/MyDrive/Course Folder/Forex/XAUUSD/Passwords.csv\")\n",
        "\n",
        "# Lee primero del entorno; si no existe, usa el respaldo (si lo pusieras)\n",
        "META_API_TOKEN = os.getenv(\"META_API_TOKEN\", \"\").strip()\n",
        "if not META_API_TOKEN:\n",
        "    raise RuntimeError(\n",
        "        \"META_API_TOKEN no estÃ¡ definido. \"\n",
        "        \"ConfigÃºralo como variable de entorno o asigna un respaldo (no recomendado).\"\n",
        "    )\n",
        "\n",
        "# ACCOUNT_ID: usa el del entorno si existe; si no, usa el que ya tenÃ­as como default\n",
        "ACCOUNT_ID = os.getenv(\"ACCOUNT_ID\", \"163d9a57-1f07-4e78-a6af-036efe867c1b\").strip()\n",
        "if not ACCOUNT_ID:\n",
        "    raise RuntimeError(\"ACCOUNT_ID no estÃ¡ definido.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dTlwjirFwRfj",
      "metadata": {
        "id": "dTlwjirFwRfj"
      },
      "outputs": [],
      "source": [
        "__CONNECTION_CHECKED = False\n",
        "__ACCOUNT_CONN: Optional[Tuple[object, object]] = None  # (account, rpc_conn)\n",
        "\n",
        "async def _connect_and_validate_async(token: str, account_id: str) -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Conecta vÃ­a RPC y espera sincronizaciÃ³n. Lanza excepciÃ³n si no se logra.\n",
        "    Devuelve (account, rpc_conn).\n",
        "    \"\"\"\n",
        "    api = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "    global REGION\n",
        "    REGION = getattr(account, 'region', REGION) if 'REGION' in globals() else getattr(account, 'region', None)\n",
        "    if account.state not in ('DEPLOYED', 'CONNECTED'):\n",
        "        await account.deploy()\n",
        "    try:\n",
        "        await account.wait_connected()\n",
        "    except TimeoutException:\n",
        "        await api.close()\n",
        "        raise\n",
        "\n",
        "    # refrescamos para leer estado/connectionStatus\n",
        "    try:\n",
        "        await account.reload()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Verificar que la cuenta estÃ© conectada al broker\n",
        "    status = getattr(account, 'connection_status', getattr(account, 'connectionStatus', None))\n",
        "    if status != 'CONNECTED':\n",
        "        raise TimeoutException(f'Cuenta {account_id} no estÃ¡ conectada al broker (connection_status={status}).')\n",
        "\n",
        "    # ConexiÃ³n RPC + sincronizaciÃ³n del terminal\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    try:\n",
        "        await rpc_conn.connect()\n",
        "        await rpc_conn.wait_synchronized()  # espera a que el terminal estÃ© listo\n",
        "    except TimeoutException:\n",
        "        await rpc_conn.close()\n",
        "        raise\n",
        "\n",
        "    # Sonda rÃ¡pida para confirmar conectividad real con el terminal\n",
        "    try:\n",
        "        _ = await rpc_conn.get_account_information()\n",
        "    except Exception:\n",
        "        # si falla la sonda, igual devolvemos la conexiÃ³n (ya sincronizada)\n",
        "        pass\n",
        "\n",
        "    return account, rpc_conn\n",
        "\n",
        "def _run(coro):\n",
        "    \"\"\"Ejecuta corutinas tanto en script como en notebook.\"\"\"\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        # evento ya corriendo (Jupyter): usamos el loop actual\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "def check_connection_once(token: str, account_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida la conexiÃ³n y sincronizaciÃ³n SOLO la primera vez que se llama.\n",
        "    En llamadas posteriores no vuelve a conectar.\n",
        "    \"\"\"\n",
        "    global __CONNECTION_CHECKED, __ACCOUNT_CONN\n",
        "    if __CONNECTION_CHECKED:\n",
        "        print(\"â„¹ï¸ ConexiÃ³n ya validada en esta sesiÃ³n; no se repite.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        account, rpc_conn = _run(_connect_and_validate_async(token, account_id))\n",
        "        __ACCOUNT_CONN = (account, rpc_conn)\n",
        "        __CONNECTION_CHECKED = True\n",
        "        print(f\"âœ… Conectado y sincronizado con MetaApi. account_id={account_id}\")\n",
        "        return True\n",
        "    except TimeoutException as e:\n",
        "        print(f\"âŒ Timeout esperando sincronizaciÃ³n. Â¿La cuenta estÃ¡ CONNECTED al broker? Detalle: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ No fue posible validar la conexiÃ³n. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def _safe_json_dump(value):\n",
        "    try:\n",
        "        return json.dumps(value, indent=2, default=str)\n",
        "    except Exception:\n",
        "        return str(value)\n",
        "\n",
        "def print_order_error_details(ctx: dict, err: Exception):\n",
        "    \"\"\"Pretty-print as much structured info as we can from MetaApi errors.\"\"\"\n",
        "    print(\"\\n\" + \"âœ˜\" * 70)\n",
        "    print(\"âŒ Order failed\")\n",
        "    print(\"â€¢ Exception type:\", type(err).__name__)\n",
        "    print(\"â€¢ Message       :\", str(err))\n",
        "\n",
        "    # Known useful attributes often present on MetaApi exceptions\n",
        "    for attr in (\"details\", \"error\", \"status\", \"code\", \"description\", \"response\", \"body\"):\n",
        "        if hasattr(err, attr):\n",
        "            val = getattr(err, attr)\n",
        "            if val:\n",
        "                print(f\"â€¢ {attr:12}: {_safe_json_dump(val)}\")\n",
        "\n",
        "    # Try to parse a JSON object embedded in the message (common in SDKs)\n",
        "    msg = str(err)\n",
        "    m = re.search(r\"\\{.*\\}\", msg)\n",
        "    if m:\n",
        "        try:\n",
        "            payload = json.loads(m.group(0))\n",
        "            print(\"â€¢ parsed_json  :\", _safe_json_dump(payload))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Stack (useful while debugging)\n",
        "    print(\"â€¢ traceback    :\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Context of the attempt\n",
        "    print(\"â€¢ context      :\", _safe_json_dump(ctx))\n",
        "    print(\"âœ˜\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RjShygBIwWRp",
      "metadata": {
        "id": "RjShygBIwWRp"
      },
      "outputs": [],
      "source": [
        "check_connection_once(META_API_TOKEN, ACCOUNT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfxZvvtJeFh1",
      "metadata": {
        "id": "HfxZvvtJeFh1"
      },
      "source": [
        "# Real_Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sx6TbzBOBzZl",
      "metadata": {
        "id": "Sx6TbzBOBzZl"
      },
      "outputs": [],
      "source": [
        "SYMBOL = \"BTCUSD\"\n",
        "FILE_PATH = 'xauusd_data.csv'\n",
        "\n",
        "WARMUP_FEATURE_BARS = 900\n",
        "\n",
        "CANDEL_NUMBER = 100\n",
        "\n",
        "time_frame_data = \"1m\"\n",
        "FETCH_INTERVAL = 60\n",
        "\n",
        "LOT     = 1.0\n",
        "COMMENT = \"Insta_ml\"\n",
        "\n",
        "length_1 = 300\n",
        "length_2 = 410\n",
        "length_3 = 710\n",
        "length_4 = 870\n",
        "\n",
        "smooth_1 = 3\n",
        "smooth_2 = 3\n",
        "smooth_3 = 3\n",
        "smooth_4 = 5\n",
        "\n",
        "INITIAL_SL         = -2\n",
        "FIRST_STEP_ATR     = 0.5\n",
        "GAP_FIRST_STEP_ATR = 2\n",
        "\n",
        "REGION = \"new-york\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zhTndYVi1TEV",
      "metadata": {
        "id": "zhTndYVi1TEV"
      },
      "source": [
        "# Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kp4yJdGAjoeA",
      "metadata": {
        "id": "kp4yJdGAjoeA"
      },
      "outputs": [],
      "source": [
        "def kalman_line(source, kalman_length: int, smooth: int):\n",
        "\n",
        "    n = len(source)\n",
        "    kf_c = np.empty(n)            # nÃºcleo del filtro\n",
        "    velo_c = np.zeros(n)          # componente de velocidad\n",
        "\n",
        "    sqrt_term   = np.sqrt(kalman_length / 10000.0 * 2.0)\n",
        "    length_term = kalman_length / 10000.0\n",
        "\n",
        "    # --------- inicializaciÃ³n (mismo efecto que `var` en Pine) ----------\n",
        "    kf_c[0]   = source.iloc[0]    # nz(kf_c[1], source) para la primera barra\n",
        "    velo_c[0] = 0.0\n",
        "\n",
        "    # ------------------- bucle recursivo -------------------------------\n",
        "    for i in range(1, n):\n",
        "        prev_kf = kf_c[i-1] if not np.isnan(kf_c[i-1]) else source.iloc[i]\n",
        "        dk      = source.iloc[i] - prev_kf\n",
        "        smooth_c = prev_kf + dk * sqrt_term          # parte \"suave\"\n",
        "        velo_c[i] = velo_c[i-1] + length_term * dk   # acumulamos velocidad\n",
        "        kf_c[i]   = smooth_c + velo_c[i]             # estimaciÃ³n final\n",
        "\n",
        "    # -------------------- EMA final (ta.ema) ----------------------------\n",
        "    kf_c_series = pd.Series(kf_c, index=source.index)\n",
        "    kalman_line = kf_c_series.ewm(span=smooth, adjust=False).mean()\n",
        "    return kalman_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HahbLzEkjoeB",
      "metadata": {
        "id": "HahbLzEkjoeB"
      },
      "outputs": [],
      "source": [
        "def slope(src: pd.Series,\n",
        "          length_kal: int,\n",
        "          smooth_kal: int,\n",
        "          slopeLen: int,\n",
        "          offset: int) -> pd.DataFrame:\n",
        "\n",
        "    n = len(src)\n",
        "    kf_state = np.full(n, np.nan)\n",
        "    kf_velo  = np.zeros(n)\n",
        "    sqrt_factor = np.sqrt(length_kal / 10000.0 * 2.0)\n",
        "    vel_factor  = length_kal / 10000.0\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == 0:\n",
        "            prev_state = src.iloc[0]\n",
        "            prev_velo  = 0.0\n",
        "        else:\n",
        "            prev_state = kf_state[i-1] if not np.isnan(kf_state[i-1]) else src.iloc[i]\n",
        "            prev_velo  = kf_velo[i-1]\n",
        "\n",
        "        dk = src.iloc[i] - prev_state\n",
        "        smooth = prev_state + dk * sqrt_factor\n",
        "        kf_velo[i]  = prev_velo + vel_factor * dk\n",
        "        kf_state[i] = smooth + kf_velo[i]\n",
        "\n",
        "    # 2) EMA smoothing --------------------------------------------------\n",
        "    kal = pd.Series(kf_state, index=src.index).ewm(span=smooth_kal, adjust=False).mean()\n",
        "\n",
        "    # 3) Slope/divergence -----------------------------------------------\n",
        "    validLen = max(slopeLen, 1)\n",
        "    slope_div = kal.diff(validLen) / validLen\n",
        "    slope_signal = (slope_div > slope_div.shift(1)).astype(int)\n",
        "\n",
        "    # 4) Angle in degrees -----------------------------------------------\n",
        "    price_change = kal - kal.shift(validLen)\n",
        "    slope_angle = np.degrees(np.arctan(price_change))\n",
        "    slope_angle_signal = (slope_angle > slope_angle.shift(1)).astype(int)\n",
        "\n",
        "    # 5) Linear regression prediction ----------------------------------\n",
        "    def _linreg(y):\n",
        "        x = np.arange(len(y))\n",
        "        m, b = np.polyfit(x, y, 1)\n",
        "        return b + m * (len(y)-1)\n",
        "\n",
        "    slope_lin_reg = kal.rolling(window=slopeLen).apply(_linreg, raw=False)\n",
        "    slope_lin_reg = slope_lin_reg.shift(-offset)  # apply Pine-style offset\n",
        "    slope_lin_reg_signal = (slope_lin_reg > slope_lin_reg.shift(1)).astype(int)\n",
        "\n",
        "    # 6) Pack results ---------------------------------------------------\n",
        "    return pd.DataFrame({\n",
        "        'slope_div':            slope_div,\n",
        "        'slope_signal':         slope_signal,\n",
        "        'slope_angle':          slope_angle,\n",
        "        'slope_angle_signal':   slope_angle_signal,\n",
        "        'slope_lin_reg':        slope_lin_reg,\n",
        "        'slope_lin_reg_signal': slope_lin_reg_signal\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fB8uDm2F1TS1",
      "metadata": {
        "id": "fB8uDm2F1TS1"
      },
      "outputs": [],
      "source": [
        "# Function for creating features\n",
        "def create_features(stock_data):\n",
        "\n",
        "    short_periods = [3, 5, 7, 10, 15, 17]\n",
        "    long_periods = [20, 22, 66, 126, 252]\n",
        "\n",
        "    # Combined list of lookbacks\n",
        "    periods = short_periods + long_periods\n",
        "\n",
        "    # Initialise an empty DataFrame to store the results\n",
        "    features = pd.DataFrame(index=stock_data.index)\n",
        "\n",
        "    # Calculate technical indicators for each specified period\n",
        "    for period in periods:\n",
        "        # Relative Strength Index (RSI)\n",
        "        features[f'RSI_{period}'] = ta.RSI(\n",
        "            stock_data['Close'], timeperiod=period)\n",
        "\n",
        "        # Money Flow Index (MFI)\n",
        "        features[f'MFI_{period}'] = ta.MFI(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], stock_data['Volume'], timeperiod=period)\n",
        "\n",
        "        # Average Directional Index (ADX)\n",
        "        features[f'ADX_{period}'] = ta.ADX(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], timeperiod=period)\n",
        "\n",
        "        # On-Balance Volume (OBV)\n",
        "        features[f'OBV_{period}'] = ta.OBV(\n",
        "            stock_data['Close'], stock_data['Volume'])\n",
        "\n",
        "        # Accumulation/Distribution Line (AD)\n",
        "        features[f'AD_{period}'] = ta.AD(\n",
        "            stock_data['High'], stock_data['Low'], stock_data['Close'], stock_data['Volume'])\n",
        "\n",
        "        # Rate of Change (ROCP)\n",
        "        features[f'ROCP_{period}'] = ta.ROCP(\n",
        "            stock_data['Close'], timeperiod=period)\n",
        "\n",
        "    # Calculate Simple Moving Average and Exponential Moving Average Crossovers\n",
        "    for short_period in short_periods:\n",
        "        for long_period in long_periods:\n",
        "            # SMA Crossover\n",
        "            features[f'SMA_Crossover_{short_period}_{long_period}'] = ta.SMA(\n",
        "                stock_data['Close'], timeperiod=short_period) - ta.SMA(stock_data['Close'], timeperiod=long_period)\n",
        "\n",
        "            # EMA Crossover\n",
        "            features[f'EMA_Crossover_{short_period}_{long_period}'] = ta.EMA(\n",
        "                stock_data['Close'], timeperiod=short_period) - ta.EMA(stock_data['Close'], timeperiod=long_period)\n",
        "\n",
        "\n",
        "    features.dropna(inplace=True)\n",
        "    return features\n",
        "\n",
        "def scale_features_data(features):\n",
        "    # Standardise the input data (X)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Split the dataset into training and testing sets as 80:20\n",
        "    train_data = features.iloc[:(int(len(features) * 0.8))]\n",
        "    test_data = features.iloc[(int(len(features) * 0.8)):]\n",
        "\n",
        "    # Scale the training and testing sets\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(\n",
        "        train_data), columns=features.columns, index=train_data.index)\n",
        "    X_test = pd.DataFrame(data=scaler.transform(test_data),\n",
        "                          columns=features.columns, index=test_data.index)\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def strategy_returns_dynamic_different_thresholds(prices, threshold):\n",
        "    curr_pos = 0\n",
        "    hold_days = 0\n",
        "    for dt in prices.index:\n",
        "        if curr_pos==0 or hold_days==20:\n",
        "            if prices.loc[dt, 'Rolling Returns'] >= threshold*prices.loc[dt, 'Yearly Stdev']:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 1\n",
        "\n",
        "            elif prices.loc[dt, 'Rolling Returns'] < -threshold*prices.loc[dt, 'Yearly Stdev']:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = -1\n",
        "\n",
        "            else:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 0\n",
        "\n",
        "            curr_pos = prices.loc[dt, 'Signal_'+str(threshold)]\n",
        "            hold_days = 0\n",
        "\n",
        "        elif curr_pos!=0:\n",
        "            hold_days+=1\n",
        "\n",
        "    prices['Signal_'+str(threshold)].ffill(inplace=True)\n",
        "    prices['Strategy Returns_'+str(threshold)] = prices['Close'].pct_change() * prices['Signal_'+str(threshold)].shift(1)\n",
        "    cumulative_strategy_returns = (prices['Strategy Returns_'+str(threshold)] +1).cumprod()\n",
        "\n",
        "    return cumulative_strategy_returns\n",
        "\n",
        "def strategy_returns_different_thresholds(prices, threshold):\n",
        "    curr_pos = 0\n",
        "    hold_days = 0\n",
        "    for dt in prices.index:\n",
        "        if curr_pos==0 or hold_days==20:\n",
        "            if prices.loc[dt, 'Rolling Returns'] >= threshold:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = 1\n",
        "            elif prices.loc[dt, 'Rolling Returns'] < threshold:\n",
        "                prices.loc[dt, 'Signal_'+str(threshold)] = -1\n",
        "\n",
        "            curr_pos = prices.loc[dt, 'Signal_'+str(threshold)]\n",
        "            hold_days = 0\n",
        "\n",
        "        elif curr_pos!=0:\n",
        "            hold_days+=1\n",
        "\n",
        "    prices['Signal_'+str(threshold)].ffill(inplace=True)\n",
        "\n",
        "    prices['Strategy Returns_'+str(threshold)] = prices['Close'].pct_change() * prices['Signal_'+str(threshold)].shift(1)\n",
        "\n",
        "    cumulative_strategy_returns = (prices['Strategy Returns_'+str(threshold)] +1).cumprod()\n",
        "    return cumulative_strategy_returns\n",
        "\n",
        "def generate_trade_sheet(data):\n",
        "    trade_list = []  # Use a list to store trade data\n",
        "    current_position = 0\n",
        "    entry_date = ''\n",
        "    entry_price = ''\n",
        "    exit_date = ''\n",
        "    exit_price = ''\n",
        "    data.reset_index(inplace=True)\n",
        "\n",
        "    for i in data.index:\n",
        "\n",
        "        if current_position == 0:\n",
        "            entry_date = data.loc[i, 'Date']\n",
        "            entry_price = data.loc[i, 'Close']\n",
        "            current_position = data.loc[i, 'signal']\n",
        "\n",
        "        elif np.abs(data.loc[i, 'signal'] - data.loc[i-1, 'signal']) != 0:\n",
        "            exit_date = data.loc[i, 'Date']\n",
        "            exit_price = data.loc[i, 'Close']\n",
        "            trade_list.append(\n",
        "                (current_position, entry_date, round(entry_price,2), exit_date, round(exit_price,2))) # Append to list\n",
        "            current_position = 0\n",
        "\n",
        "    trade_sheet = pd.DataFrame(trade_list, columns=['Position', 'Entry Date', # Convert list to DataFrame\n",
        "                           'Entry Price', 'Exit Date', 'Exit Price'])\n",
        "    trade_sheet['PnL'] = round((trade_sheet['Exit Price'] - trade_sheet['Entry Price']) * trade_sheet['Position'],2)\n",
        "    return trade_sheet\n",
        "\n",
        "\n",
        "def trade_analytics(trades):\n",
        "    analytics = pd.DataFrame(index=['Strategy'])\n",
        "    analytics['Total PnL'] = round(trades.PnL.sum(),2)\n",
        "    analytics['Total Trades'] = len(trades.loc[trades.Position!=0])\n",
        "    analytics['Number of Winners'] = len(trades.loc[trades.PnL>0])\n",
        "    analytics['Number of Losers'] = len(trades.loc[trades.PnL<=0])\n",
        "    analytics['Win (%)'] = round(100*analytics['Number of Winners']/analytics['Total Trades'],2)\n",
        "    analytics['Loss (%)'] = round(100*analytics['Number of Losers']/analytics['Total Trades'],2)\n",
        "    analytics['Average Profit of Winning Trade'] = round(trades.loc[trades.PnL>0].PnL.mean(),2)\n",
        "    analytics['Average Loss of Losing Trade'] = round(np.abs(trades.loc[trades.PnL<=0].PnL.mean()),2)\n",
        "    trades['Entry Date'] = pd.to_datetime(trades['Entry Date'])\n",
        "    trades['Exit Date'] = pd.to_datetime(trades['Exit Date'])\n",
        "    holding_period = trades['Exit Date'] - trades['Entry Date']\n",
        "    analytics['Average Holding Time'] = holding_period.mean()\n",
        "    analytics['Profit Factor'] = round((analytics['Win (%)']/100*analytics['Average Profit of Winning Trade'])/(analytics['Loss (%)']/100*analytics['Average Loss of Losing Trade']),2)\n",
        "    return analytics.T\n",
        "\n",
        "def performance_metrics(data):\n",
        "    data.set_index('Date', inplace=True)\n",
        "    performance_metrics = pd.DataFrame(index=['Strategy'])\n",
        "    data['Strategy Returns'] = data.signal.shift(1) * data.Close.pct_change()\n",
        "    data['Cumulative Returns'] = (data['Strategy Returns'] + 1.0).cumprod()\n",
        "    data['Cumulative Benchmark Returns'] = (data['Close'].pct_change() +1).cumprod()\n",
        "    data['Cumulative Returns'].plot(figsize=(15, 7), label='Strategy Returns')\n",
        "    data['Cumulative Benchmark Returns'].plot(label='Benchmark Returns')\n",
        "    plt.title('Equity Curve', fontsize=14)\n",
        "    plt.ylabel('Cumulative Returns', fontsize = 12)\n",
        "    plt.xlabel('Date', fontsize = 12)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    days = len(data['Cumulative Returns'])\n",
        "    performance_metrics['CAGR'] = \"{0:.2f}%\".format(\n",
        "        (data['Cumulative Returns'].iloc[-1]**(252/days)-1)*100)\n",
        "    performance_metrics['Annualised Volatility'] = \"{0:.2f}%\".format(\n",
        "        data['Strategy Returns'].std()*np.sqrt(252) * 100)\n",
        "    risk_free_rate = 0.02/252\n",
        "    performance_metrics['Sharpe Ratio'] = round(np.sqrt(252)*(np.mean(data['Strategy Returns']) -\n",
        "                                                        (risk_free_rate))/np.std(data['Strategy Returns']),2)\n",
        "    data['Peak'] = data['Cumulative Returns'].cummax()\n",
        "    data['Drawdown'] = ((data['Cumulative Returns'] - data['Peak'])/data['Peak'])\n",
        "    performance_metrics['Maximum Drawdown'] =  \"{0:.2f}%\".format((data['Drawdown'].min())*100)\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title('Drawdowns', fontsize=14)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.plot(data['Drawdown'], color='red')\n",
        "    plt.fill_between(data['Drawdown'].index, data['Drawdown'].values, color='red')\n",
        "    plt.show()\n",
        "    print(performance_metrics.T)\n",
        "\n",
        "# Function for creating target variable\n",
        "def target_var(data, window_size=20):\n",
        "    target = pd.DataFrame(index=data.index)\n",
        "    target['signal'] = data.Close.pct_change(window_size).shift(-window_size)\n",
        "\n",
        "    # Drop the NaN values\n",
        "    target.dropna(inplace=True)\n",
        "\n",
        "    # Convert the change into binary signals: 1 for positive change, -1 for negative change\n",
        "    target['signal'] = np.where(target['signal'] > 0, 1, -1)\n",
        "\n",
        "    return target\n",
        "\n",
        "# Function to split and scale the data\n",
        "def train_test_split(features, target, split_proportion=0.8):\n",
        "\n",
        "    split_index = int(len(features) * split_proportion)\n",
        "\n",
        "    # Split the features dataset into training and testing sets\n",
        "    X_train = features.iloc[:split_index]\n",
        "    X_test = features.iloc[split_index:]\n",
        "    y_train = target.iloc[:split_index]\n",
        "    y_test = target.iloc[split_index:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "\n",
        "def compile_encoder_decoder_model(X_train, optimizer='adam', loss='mean_squared_error'):\n",
        "    # Define the architecture of the autoencoder model\n",
        "    model = Sequential()  # Create a sequential model\n",
        "\n",
        "    # Add a dense layer with 64 neurons and ReLU activation function as the input layer\n",
        "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    # Add a dense layer with 32 neurons and ReLU activation function\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # Add a dense layer with 8 neurons and ReLU activation function\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "\n",
        "    # Add another dense layer with 32 neurons and ReLU activation function\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(X_train.shape[1], activation='linear'))\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def calculate_portfolio_returns_csmom(monthly_returns, portfolio='long-short', lookback_months=12):\n",
        "    stock_monthly_returns = pd.DataFrame()\n",
        "\n",
        "    # Loop through each month after the lookback period\n",
        "    for i in range(lookback_months, len(monthly_returns)):\n",
        "        # Select the subset of monthly returns for the current lookback period\n",
        "        # Store the historical and holding monthly returns data and store in 'returns'\n",
        "        returns = monthly_returns[i - lookback_months:i + 1]\n",
        "\n",
        "        # Store the historical in 'trailing_returns'\n",
        "        trailing_returns = returns[:lookback_months]\n",
        "\n",
        "        # Extract the starting, ending, and holding months from the subset\n",
        "        starting_month = str(returns.index[0])[:7]\n",
        "        ending_month = str(returns.index[-2])[:7]\n",
        "        holding_month = str(returns.index[-1])[:7]\n",
        "\n",
        "        # Set returns data as the transposed scaled trailing returns\n",
        "        returns_data = trailing_returns.T\n",
        "\n",
        "        # Initialize the number of clusters and maximum number of stocks per cluster\n",
        "        num_clusters = 1\n",
        "        max_stocks_per_cluster = 10\n",
        "\n",
        "        # Perform hierarchical clustering using 'ward' linkage method\n",
        "        linkage_matrix = linkage(trailing_returns.T, method='ward')\n",
        "\n",
        "        # Assign cluster labels to stocks, ensuring each cluster has at most 10 stocks\n",
        "        clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "        # Assign the cluster labels to the original returns data\n",
        "        returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Adjust clusters until each cluster meets the constraint\n",
        "        while max(returns_data['Cluster'].value_counts()) > max_stocks_per_cluster:\n",
        "            num_clusters += 1\n",
        "            clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "            returns_data['Cluster'] = clusters\n",
        "\n",
        "        # Define the minimum number of stocks in a cluster\n",
        "        minimum_stocks_in_cluster = 2\n",
        "\n",
        "        # Filter out clusters with fewer than the minimum number of stocks\n",
        "        filtered_clusters = returns_data.groupby('Cluster').filter(\n",
        "            lambda x: len(x) >= minimum_stocks_in_cluster)['Cluster'].unique()\n",
        "\n",
        "        # Assign the filtered cluster labels to the original price data\n",
        "        returns_data = returns_data[returns_data['Cluster'].isin(filtered_clusters)]\n",
        "\n",
        "        # Calculate the returns for each cluster and sum across clusters\n",
        "        cluster_returns = returns_data.groupby('Cluster').mean().sum(axis=1)\n",
        "\n",
        "        if portfolio == 'long-short':\n",
        "            # Identify stocks to go short and long based on cluster returns\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long and short\n",
        "            long_returns = hold_returns[long].mean()\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long and short portfolios for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long) + list(short)][holding_month]\n",
        "\n",
        "            # Adjust returns for short positions\n",
        "            monthly_portfolio_returns[short] *= -1\n",
        "\n",
        "        elif portfolio == 'long':\n",
        "            long = np.array(returns_data[returns_data.Cluster ==\n",
        "                                         cluster_returns.idxmax()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go long\n",
        "            long_returns = hold_returns[long].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the long portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                long)][holding_month]\n",
        "\n",
        "        elif portfolio == 'short':\n",
        "            short = np.array(returns_data[returns_data.Cluster ==\n",
        "                                          cluster_returns.idxmin()].index)\n",
        "\n",
        "            # Extract the returns for holding stocks in the current month\n",
        "            hold_returns = returns.iloc[-1]\n",
        "\n",
        "            # Calculate the average returns for the stocks to go short\n",
        "            short_returns = -1 * hold_returns[short].mean()\n",
        "\n",
        "            # Copy monthly returns data for further manipulation\n",
        "            returns_monthly = monthly_returns.copy()\n",
        "\n",
        "            # Select returns for stocks in the short portfolio for the holding month\n",
        "            monthly_portfolio_returns = returns_monthly[list(\n",
        "                short)][holding_month]\n",
        "\n",
        "        # Append adjusted returns for the holding month to the stock_monthly_returns dataframe\n",
        "        stock_monthly_returns = stock_monthly_returns.append(monthly_portfolio_returns)\n",
        "\n",
        "    return stock_monthly_returns\n",
        "\n",
        "def plot_and_display_metrics_csmom(stock_monthly_returns):\n",
        "    portfolio_returns = stock_monthly_returns.mean(axis=1)\n",
        "    fig, ax = plt.subplots(figsize=(15, 7))\n",
        "    portfolio_returns.plot(ax=ax)\n",
        "\n",
        "    # Set the title and axis labels\n",
        "    ax.set_title('Portfolio Returns Over Time')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Returns')\n",
        "    ax.axhline(y=0, color='black', linestyle='-')\n",
        "\n",
        "    # Fill area below 0 with red color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns < 0, color='red', alpha=0.3)\n",
        "\n",
        "    # Fill area above 0 with green color\n",
        "    ax.fill_between(portfolio_returns.index, portfolio_returns, 0,\n",
        "                    where=portfolio_returns >= 0, color='green', alpha=0.3)\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Calculate cumulative returns\n",
        "    cumulative_returns = (portfolio_returns + 1).cumprod()\n",
        "\n",
        "    # Convert index to datetime format\n",
        "    cumulative_returns.index = pd.to_datetime(cumulative_returns.index)\n",
        "\n",
        "    # Plot cumulative returns\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    cumulative_returns.plot()\n",
        "\n",
        "    # Labeling axes and title\n",
        "    plt.ylabel('Cumulative Returns', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Returns', fontsize=14)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate maximum cumulative returns up to each point\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "\n",
        "    # Calculate drawdown\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "\n",
        "    # Plot drawdown\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    # Fill area under the drawdown curve with red color\n",
        "    plt.fill_between(drawdown.index, drawdown, 0, color='red', alpha=0.3)\n",
        "    plt.ylabel('Drawdown', fontsize=12)\n",
        "    plt.title('Cross Sectional Momentum Strategy Drawdown', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # Display the metrics\n",
        "    # Calculate monthly Sharpe ratio\n",
        "    monthly_sharpe = portfolio_returns.mean() / portfolio_returns.std()\n",
        "\n",
        "    # Annualize Sharpe ratio for monthly data\n",
        "    sharpe = round(monthly_sharpe * ((12) ** 0.5),2)\n",
        "\n",
        "    # Calculate drawdown\n",
        "    max_cumulative_returns = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - max_cumulative_returns) / max_cumulative_returns\n",
        "    max_drawdown_index = drawdown.idxmin()\n",
        "    max_drawdown_date = max_drawdown_index.strftime('%Y-%m-%d')\n",
        "    max_drawdown_value = round(drawdown.min(),2)\n",
        "\n",
        "    # Create a DataFrame to hold the metrics\n",
        "    metrics = pd.DataFrame({\n",
        "        'Metric': ['Sharpe Ratio', 'Maximum Drawdown Date', 'Maximum Drawdown Value'],\n",
        "        'Value': [sharpe, max_drawdown_date, max_drawdown_value]\n",
        "    })\n",
        "\n",
        "    # Display metrics\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    display(metrics.rename_axis(None, axis=1))\n",
        "\n",
        "def predict_signals(X_test, aapl_test_prices_ts, model, scaler):\n",
        "    # Initialise current position\n",
        "    current_pos = 0\n",
        "\n",
        "    # Initialise count of holding days\n",
        "    hold_days = 0\n",
        "\n",
        "    # Iterate through the rows of test data\n",
        "    for dt, row in X_test.iterrows():\n",
        "        # Check if there is no position or holding period reaches 20 days\n",
        "        if current_pos == 0 or hold_days == 20:\n",
        "            # Prepare test data for prediction\n",
        "            test = pd.DataFrame(data=scaler.transform(\n",
        "                row.values.reshape(1, -1)), columns=X_test.columns)\n",
        "\n",
        "            # Generate signal based on test data\n",
        "            signal = model.predict(test)[-1]\n",
        "\n",
        "            # Update current position\n",
        "            current_pos = signal\n",
        "\n",
        "            # Update predicted and actual labels for the current date\n",
        "            aapl_test_prices_ts.loc[dt, 'signal'] = current_pos\n",
        "\n",
        "            # Reset holding days counter\n",
        "            hold_days = 0\n",
        "        elif current_pos != 0:\n",
        "            # If there is an existing position, increment holding days counter\n",
        "            hold_days += 1\n",
        "\n",
        "    # Forward fill the last observed value for 'y_pred'\n",
        "    aapl_test_prices_ts['signal'].ffill(inplace=True)\n",
        "\n",
        "    return aapl_test_prices_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dunlv-TDqxB8",
      "metadata": {
        "id": "Dunlv-TDqxB8"
      },
      "outputs": [],
      "source": [
        "def get_parkinson(price_data, window=10, trading_periods=50, clean=True):\n",
        "    rs = (1.0 / (4.0 * math.log(2.0))) * ((price_data['High'] / price_data['Low']).apply(np.log))**2.0\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RLxsuLKaqxE3",
      "metadata": {
        "id": "RLxsuLKaqxE3"
      },
      "outputs": [],
      "source": [
        "def get_HodgesTompkins(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    vol = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).std() * math.sqrt(trading_periods)\n",
        "\n",
        "    h = window\n",
        "    n = (log_return.count() - h) + 1\n",
        "\n",
        "    adj_factor = 1.0 / (1.0 - (h / n) + ((h**2 - 1) / (3 * n**2)))\n",
        "\n",
        "    result = vol * adj_factor\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFCu8J7hqxHi",
      "metadata": {
        "id": "qFCu8J7hqxHi"
      },
      "outputs": [],
      "source": [
        "def get_skew(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).skew()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YnVDjmtDqxKU",
      "metadata": {
        "id": "YnVDjmtDqxKU"
      },
      "outputs": [],
      "source": [
        "def get_kurtosis(price_data, window=30, clean=True):\n",
        "\n",
        "    log_return = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "\n",
        "    result = log_return.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).kurt()\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WJvLEk7EqxM6",
      "metadata": {
        "id": "WJvLEk7EqxM6"
      },
      "outputs": [],
      "source": [
        "def get_YangZhang(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High'] / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low'] / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    log_oc = (price_data['Open'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_oc_sq = log_oc**2\n",
        "\n",
        "    log_cc = (price_data['Close'] / price_data['Close'].shift(1)).apply(np.log)\n",
        "    log_cc_sq = log_cc**2\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    close_vol = log_cc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    open_vol = log_oc_sq.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "    window_rs = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).sum() * (1.0 / (window - 1.0))\n",
        "\n",
        "    k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
        "    result = (open_vol + k * close_vol + (1 - k) * window_rs).apply(np.sqrt) * math.sqrt(trading_periods)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OHmFUyfGqxPL",
      "metadata": {
        "id": "OHmFUyfGqxPL"
      },
      "outputs": [],
      "source": [
        "def get_RogersSatchell(price_data, window=30, trading_periods=50, clean=True):\n",
        "\n",
        "    log_ho = (price_data['High']  / price_data['Open']).apply(np.log)\n",
        "    log_lo = (price_data['Low']   / price_data['Open']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(\n",
        "        window=window,\n",
        "        center=False\n",
        "    ).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cSq3oNjkrdhT",
      "metadata": {
        "id": "cSq3oNjkrdhT"
      },
      "outputs": [],
      "source": [
        "def get_GermanKlass(price_data, window=22, trading_periods=50, clean=True):\n",
        "\n",
        "    log_hl = (price_data['High'] / price_data['Low']).apply(np.log)\n",
        "    log_co = (price_data['Close'] / price_data['Open']).apply(np.log)\n",
        "\n",
        "    rs = 0.5 * log_hl**2 - (2*math.log(2)-1) * log_co**2\n",
        "\n",
        "    def f(v):\n",
        "        return (trading_periods * v.mean())**0.5\n",
        "\n",
        "    result = rs.rolling(window=window, center=False).apply(func=f)\n",
        "\n",
        "    if clean:\n",
        "        return result.dropna()\n",
        "    else:\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I23127P7lBq_",
      "metadata": {
        "id": "I23127P7lBq_"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GC3ODp84C1SH",
      "metadata": {
        "id": "GC3ODp84C1SH"
      },
      "outputs": [],
      "source": [
        "def create_features(stock_data: pd.DataFrame) -> pd.DataFrame:\n",
        "    short_periods  = [3, 5, 7, 10, 15, 17]\n",
        "    long_periods   = [20, 22, 66, 126, 252]\n",
        "    kalman_periods = [100, 300, 500, 700, 900]\n",
        "    slope_length   = [3, 6, 9, 12, 15, 18, 21]\n",
        "\n",
        "    periods = short_periods + long_periods\n",
        "    features = pd.DataFrame(index=stock_data.index)\n",
        "\n",
        "    # Import tqdm for progress bar\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    # â”€â”€ Indicadores por perÃ­odo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for period in tqdm(periods, desc=\"Calculating Period Indicators\"):\n",
        "        features[f'RSI_{period}'] = ta.RSI(stock_data['Close'], timeperiod=period)\n",
        "        features[f'MFI_{period}'] = ta.MFI(stock_data['High'], stock_data['Low'], stock_data['Close'],\n",
        "                                           stock_data['Volume'], timeperiod=period)\n",
        "        features[f'ADX_{period}'] = ta.ADX(stock_data['High'], stock_data['Low'], stock_data['Close'],\n",
        "                                           timeperiod=period)\n",
        "        # Nota: OBV y AD no usan window; conservamos el sufijo para mantener tu esquema\n",
        "        features[f'OBV_{period}'] = ta.OBV(stock_data['Close'], stock_data['Volume'])\n",
        "        features[f'AD_{period}']  = ta.AD(stock_data['High'], stock_data['Low'],\n",
        "                                          stock_data['Close'], stock_data['Volume'])\n",
        "        features[f'ROCP_{period}'] = ta.ROCP(stock_data['Close'], timeperiod=period)\n",
        "\n",
        "    # â”€â”€ Series base (una sola vez) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    log_ret = np.log(stock_data['Close']).diff()\n",
        "    features['log_ret']       = log_ret\n",
        "    features['log_ret_dif1']  = np.log(stock_data['Close']).diff(1)\n",
        "    features['vola_10']       = log_ret.rolling(window=10, min_periods=10).std()\n",
        "    features['autocorr_1']    = log_ret.rolling(window=20, min_periods=20)\\\n",
        "                                       .apply(lambda x: x.autocorr(lag=1), raw=False)\n",
        "    features['log_t1']        = log_ret.shift(1)\n",
        "\n",
        "    # â”€â”€ Estimadores de volatilidad (se escriben en `features`) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for i in tqdm(range(4, 20, 4), desc=\"Calculating Volatility Estimators\"):\n",
        "        features[f'parkinson_{i}']       = get_parkinson(stock_data,       window=i, trading_periods=50, clean=True)\n",
        "        features[f'GermanKlass_{i}']     = get_GermanKlass(stock_data,     window=i, trading_periods=50, clean=True)\n",
        "        features[f'RogersSatchell_{i}']  = get_RogersSatchell(stock_data,  window=i, trading_periods=50, clean=True)\n",
        "        features[f'YangZhang_{i}']       = get_YangZhang(stock_data,       window=i, trading_periods=50, clean=True)\n",
        "        features[f'HodgesTompkins_{i}']  = get_HodgesTompkins(stock_data,  window=i, trading_periods=50, clean=True)\n",
        "        features[f'kurtosis_{i}']        = get_kurtosis(stock_data,        window=i, clean=True)\n",
        "        features[f'skew_{i}']            = get_skew(stock_data,            window=i, clean=True)\n",
        "\n",
        "    # â”€â”€ Cruces SMA / EMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for short_period in tqdm(short_periods, desc=\"Calculating SMA/EMA Crossovers\"):\n",
        "        for long_period in long_periods:\n",
        "            features[f'SMA_Crossover_{short_period}_{long_period}'] = (\n",
        "                ta.SMA(stock_data['Close'], timeperiod=short_period)\n",
        "                - ta.SMA(stock_data['Close'], timeperiod=long_period)\n",
        "            )\n",
        "            features[f'EMA_Crossover_{short_period}_{long_period}'] = (\n",
        "                ta.EMA(stock_data['Close'], timeperiod=short_period)\n",
        "                - ta.EMA(stock_data['Close'], timeperiod=long_period)\n",
        "            )\n",
        "\n",
        "    # â”€â”€ Kalman y derivados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Kalman and Derivatives\"):\n",
        "        kal = pd.Series(kalman_line(stock_data['Close'], kalman_length=period, smooth=3),\n",
        "                        index=stock_data.index)\n",
        "        features[f'Kal_{period}']              = kal\n",
        "        features[f'Close_Kal_{period}_3']      = stock_data['Close'] - kal\n",
        "        features[f'Kal_change_{period}_3']     = kal.diff()\n",
        "\n",
        "    # â”€â”€ Slopes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for period in tqdm(kalman_periods, desc=\"Calculating Slopes\"):\n",
        "        for sLen in slope_length:\n",
        "            df_s = slope(stock_data['Close'], length_kal=period, smooth_kal=3,\n",
        "                         slopeLen=sLen, offset=-1)\n",
        "            features[f'slope_div_{period}_{sLen}']             = df_s['slope_div']\n",
        "            features[f'slope_signal_{period}_{sLen}']          = df_s['slope_signal']\n",
        "            features[f'slope_angle_{period}_{sLen}']           = df_s['slope_angle']\n",
        "            features[f'slope_angle_signal_{period}_{sLen}']    = df_s['slope_angle_signal']\n",
        "            features[f'slope_lin_reg_{period}_{sLen}']         = df_s['slope_lin_reg']\n",
        "            features[f'slope_lin_reg_signal_{period}_{sLen}']  = df_s['slope_lin_reg_signal']\n",
        "\n",
        "    features.dropna(inplace=True)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FDIu30P4Bmg-",
      "metadata": {
        "id": "FDIu30P4Bmg-"
      },
      "source": [
        "# Support Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sKKJrsFBfOoT",
      "metadata": {
        "id": "sKKJrsFBfOoT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make sure SYMBOL exists (your code elsewhere uses SYMBOL, not `symbol`)\n",
        "try:\n",
        "    SYMBOL\n",
        "except NameError:\n",
        "    SYMBOL = \"BTCUSD\"   # fallback; change if needed\n",
        "\n",
        "# Base folder where your models live\n",
        "root_data = \"/content/drive/MyDrive/Course Folder/Forex/XAUUSD/\"\n",
        "models_dir = Path(root_data) / \"Models\"\n",
        "\n",
        "# Build full paths (use straight quotes and SYMBOL)\n",
        "long_model_path  = models_dir / f\"{SYMBOL}_Long_ml_model.joblib\"\n",
        "short_model_path = models_dir / f\"{SYMBOL}_Short_ml_model.joblib\"\n",
        "\n",
        "# Helper to load and validate\n",
        "def _load_joblib_model(path: Path):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Model file not found: {path}\")\n",
        "    model = joblib.load(path)\n",
        "    return model\n",
        "\n",
        "# Load globals used by main()\n",
        "long_ml_model  = _load_joblib_model(long_model_path)\n",
        "short_ml_model = _load_joblib_model(short_model_path)\n",
        "\n",
        "print(f\"âœ… Long model loaded from:  {long_model_path}\")\n",
        "print(f\"âœ… Short model loaded from: {short_model_path}\")\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "EXPECTED_CLASSES = [0, 1, 2]\n",
        "\n",
        "def _ensure_prob_cols_present(df: pd.DataFrame, side: str) -> None:\n",
        "    \"\"\"Guarantee prob_0/1/2_{side} exist so they are always saved to CSV.\"\"\"\n",
        "    for k in EXPECTED_CLASSES:\n",
        "        col = f\"prob_{k}_{side}\"\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "def _reorder_ml(cols: list[str]) -> list[str]:\n",
        "    cols = cols.copy()\n",
        "    # unified first\n",
        "    uni = [\"label_ml\", \"prob_0\", \"prob_1\", \"prob_2\"]\n",
        "    for c in uni:\n",
        "        if c in cols:\n",
        "            cols.remove(c)\n",
        "    insert_at = cols.index(\"Type\") + 1 if \"Type\" in cols else len(cols)\n",
        "    for c in uni:\n",
        "        if c in df_out.columns:\n",
        "            cols.insert(insert_at, c)\n",
        "            insert_at += 1\n",
        "\n",
        "    # then side-specific (if you keep them)\n",
        "    ml_block = [\n",
        "        \"label_ml_long\", \"prob_0_long\", \"prob_1_long\", \"prob_2_long\",\n",
        "        \"label_ml_short\",\"prob_0_short\",\"prob_1_short\",\"prob_2_short\"\n",
        "    ]\n",
        "    for c in ml_block:\n",
        "        if c in cols:\n",
        "            cols.remove(c)\n",
        "    for c in ml_block:\n",
        "        if c in df_out.columns:\n",
        "            cols.insert(insert_at, c)\n",
        "            insert_at += 1\n",
        "    return cols\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# FIRST-RUN INITIALIZER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def initialize_csv_first_run(\n",
        "    account,\n",
        "    *,\n",
        "    bars: int = 900,\n",
        "    save_mode: str = \"all\",  # \"all\" = save full 900 rows; \"last\" = save only last row\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fetch `bars` candles, compute ATR + signals (+ ML if a trigger exists),\n",
        "    and write either the full dataset or just the last row to the CSV.\n",
        "\n",
        "    save_mode:\n",
        "      - \"all\"  â†’ write all rows (what you asked for)\n",
        "      - \"last\" â†’ compute with 900 but persist only the last bar\n",
        "    \"\"\"\n",
        "    # 1) Download history\n",
        "    df_hist = await get_candles_5m(account, start=None, limit=bars)\n",
        "    df_hist = (df_hist\n",
        "               .drop_duplicates(\"time\")\n",
        "               .sort_values(\"time\")\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "    # 2) ATR on all rows\n",
        "    if len(df_hist) >= 14:\n",
        "        df_hist[\"ATR\"] = ta.ATR(df_hist[\"high\"], df_hist[\"low\"], df_hist[\"close\"], 14).round(4)\n",
        "\n",
        "    # 3) Signals (and ML only if an entry trigger exists)\n",
        "    df_all = df_hist.copy()\n",
        "    generate_trade_signals(\n",
        "        df_all, length_1, length_2, length_3, length_4,\n",
        "        smooth_1, smooth_2, smooth_3, smooth_4\n",
        "    )\n",
        "    maybe_compute_ml_for_entry(\n",
        "        df_all,\n",
        "        long_model=long_ml_model,\n",
        "        short_model=short_ml_model,\n",
        "        feature_cols_long=FEATURES_LONG,\n",
        "        feature_cols_short=FEATURES_SHORT\n",
        "    )\n",
        "    sync_unified_ml_cols(df_all)\n",
        "    _ensure_order_cols(df_all)\n",
        "\n",
        "    # 4) Persist\n",
        "    if save_mode == \"all\":\n",
        "        df_all[\"source\"] = 1\n",
        "        stamp_system_time(df_all, mode=\"missing\")   # put System_time on every row\n",
        "        save_csv(df_all)\n",
        "        print(f\"âœ” Archivo inicial creado: {len(df_all)} filas guardadas (se analizaron {bars} velas)\")\n",
        "    else:\n",
        "        last = df_all.tail(1).copy()\n",
        "        last[\"source\"] = 1\n",
        "        stamp_system_time(last, mode=\"last\")\n",
        "        save_csv(last)\n",
        "        print(f\"âœ” Archivo inicial creado: 1 fila guardada (se analizaron {bars} velas)\")\n",
        "\n",
        "\n",
        "\n",
        "def has_new_entry_signal(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    True only if the *last* bar is an entry trigger: Open_Trade âˆˆ {+1, -1}.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return False\n",
        "    s = df[\"Open_Trade\"].iloc[-1]\n",
        "    if pd.isna(s):\n",
        "        return False\n",
        "    return s in (1, -1)\n",
        "\n",
        "def _ensure_unified_ml_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Make sure the 4 generic ML columns exist.\"\"\"\n",
        "    if \"label_ml\" not in df.columns:\n",
        "        df[\"label_ml\"] = np.nan\n",
        "    for k in (0, 1, 2):\n",
        "        c = f\"prob_{k}\"\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "\n",
        "\n",
        "def sync_unified_ml_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Populate the 4 generic columns from side-specific ones:\n",
        "      â€¢ If Open_Trade == +1  â†’ copy from *_long\n",
        "      â€¢ If Open_Trade == -1  â†’ copy from *_short\n",
        "      â€¢ Otherwise leave NaNs (no trade)\n",
        "    Works for all rows that have side predictions.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    _ensure_unified_ml_cols(df)\n",
        "\n",
        "    # Prefer rows with explicit Open_Trade signal\n",
        "    if \"Open_Trade\" in df.columns:\n",
        "        # LONG rows\n",
        "        mL = df[\"Open_Trade\"] == 1\n",
        "        if \"label_ml_long\" in df.columns:\n",
        "            df.loc[mL, \"label_ml\"] = df.loc[mL, \"label_ml_long\"]\n",
        "        for k in (0, 1, 2):\n",
        "            src = f\"prob_{k}_long\"\n",
        "            if src in df.columns:\n",
        "                df.loc[mL, f\"prob_{k}\"] = df.loc[mL, src]\n",
        "\n",
        "        # SHORT rows\n",
        "        mS = df[\"Open_Trade\"] == -1\n",
        "        if \"label_ml_short\" in df.columns:\n",
        "            df.loc[mS, \"label_ml\"] = df.loc[mS, \"label_ml_short\"]\n",
        "        for k in (0, 1, 2):\n",
        "            src = f\"prob_{k}_short\"\n",
        "            if src in df.columns:\n",
        "                df.loc[mS, f\"prob_{k}\"] = df.loc[mS, src]\n",
        "\n",
        "    # Fallback: if some rows have predictions but Open_Trade is NaN, copy whichever exists\n",
        "    if \"label_ml\" in df.columns:\n",
        "        if \"label_ml_long\" in df.columns:\n",
        "            mask = df[\"label_ml\"].isna() & df[\"label_ml_long\"].notna()\n",
        "            df.loc[mask, \"label_ml\"] = df.loc[mask, \"label_ml_long\"]\n",
        "            for k in (0, 1, 2):\n",
        "                src = f\"prob_{k}_long\"\n",
        "                if src in df.columns:\n",
        "                    df.loc[mask, f\"prob_{k}\"] = df.loc[mask, src]\n",
        "\n",
        "        if \"label_ml_short\" in df.columns:\n",
        "            mask = df[\"label_ml\"].isna() & df[\"label_ml_short\"].notna()\n",
        "            df.loc[mask, \"label_ml\"] = df.loc[mask, \"label_ml_short\"]\n",
        "            for k in (0, 1, 2):\n",
        "                src = f\"prob_{k}_short\"\n",
        "                if src in df.columns:\n",
        "                    df.loc[mask, f\"prob_{k}\"] = df.loc[mask, src]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def maybe_compute_ml_for_entry(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    long_model,\n",
        "    short_model,\n",
        "    feature_cols_long: list[str] | None = None,\n",
        "    feature_cols_short: list[str] | None = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs the heavy feature/ML block **only** when the last bar has an entry signal.\n",
        "    Otherwise, it NOOPs (and ensures ML columns exist so downstream code never breaks).\n",
        "    \"\"\"\n",
        "    # No entry trigger on the last bar â†’ do nothing (but create empty ML cols)\n",
        "    if not has_new_entry_signal(df):\n",
        "        for side in (\"long\", \"short\"):\n",
        "            lab = f\"label_ml_{side}\"\n",
        "            if lab not in df.columns:\n",
        "                df[lab] = np.nan\n",
        "            for k in (0, 1, 2):\n",
        "                col = f\"prob_{k}_{side}\"\n",
        "                if col not in df.columns:\n",
        "                    df[col] = np.nan\n",
        "        return df\n",
        "\n",
        "    # Entry trigger right now â†’ run the real ML block (this computes features internally)\n",
        "    return run_side_models_inplace(\n",
        "        df,\n",
        "        long_model=long_model,\n",
        "        short_model=short_model,\n",
        "        feature_cols_long=feature_cols_long,\n",
        "        feature_cols_short=feature_cols_short\n",
        "    )\n",
        "\n",
        "def prediction_table_action(trade_side: int, pred: int):\n",
        "    \"\"\"\n",
        "    Implements your table:\n",
        "      trade=+1 (long): pred 0â†’None, 1â†’(tp1,sl1), 2â†’(tp2,sl2)\n",
        "      trade=-1 (short): pred 0â†’None, 1â†’(tp1,sl1), 2â†’(tp2,sl2)\n",
        "    Returns (\"none\"|\"long\"|\"short\", tp_mult, sl_mult).\n",
        "    \"\"\"\n",
        "    if pred == 0 or not np.isfinite(pred):\n",
        "        return (\"none\", None, None)\n",
        "    if trade_side == 1:\n",
        "        return (\"long\",  \"takeprofit_1\" if pred == 1 else \"takeprofit_2\",\n",
        "                        \"stoploss_1\"   if pred == 1 else \"stoploss_2\")\n",
        "    if trade_side == -1:\n",
        "        return (\"short\", \"takeprofit_1\" if pred == 1 else \"takeprofit_2\",\n",
        "                        \"stoploss_1\"   if pred == 1 else \"stoploss_2\")\n",
        "    return (\"none\", None, None)\n",
        "\n",
        "\n",
        "\n",
        "def has_new_entry_signal(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"True only if the *last* bar is an entry trigger: Open_Trade âˆˆ {+1, -1}.\"\"\"\n",
        "    if df is None or df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return False\n",
        "    s = df[\"Open_Trade\"].iloc[-1]\n",
        "    return np.isfinite(s) and (s == 1 or s == -1)\n",
        "\n",
        "def _encoded_span_from(names: list[str]) -> int:\n",
        "    \"\"\"Return N if Encoded_0..Encoded_{N-1} appear in names; else 0.\"\"\"\n",
        "    if not names:\n",
        "        return 0\n",
        "    idxs = []\n",
        "    for n in names:\n",
        "        m = re.match(r\"^Encoded_(\\d+)$\", str(n))\n",
        "        if m:\n",
        "            idxs.append(int(m.group(1)))\n",
        "    return (max(idxs) + 1) if idxs else 0\n",
        "\n",
        "\n",
        "# --- add this helper (near your other small utils) -------------------\n",
        "def _preferred_feature_order(side: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return the exact column order to encode based on your files.\n",
        "    side: 'long' or 'short'\n",
        "    \"\"\"\n",
        "    if side == \"long\" and FEATURES_LONG:\n",
        "        return FEATURES_LONG\n",
        "    if side == \"short\" and FEATURES_SHORT:\n",
        "        return FEATURES_SHORT\n",
        "    return []\n",
        "\n",
        "\n",
        "def _ensure_10m_prefix_aliases(X: pd.DataFrame, required_cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If your CSV expects '10_min_10_min_' but you produced '10min_10min_' (or vice-versa),\n",
        "    make alias columns so required names exist without recomputing features.\n",
        "    \"\"\"\n",
        "    if not required_cols:\n",
        "        return X\n",
        "\n",
        "    pref_a = \"10min_10min_\"\n",
        "    pref_b = \"10_min_10_min_\"\n",
        "\n",
        "    have = set(X.columns)\n",
        "    for col in required_cols:\n",
        "        if not isinstance(col, str):\n",
        "            continue\n",
        "\n",
        "        # CSV wants B, we have A â†’ create B from A\n",
        "        if col.startswith(pref_b) and col not in have:\n",
        "            alt = pref_a + col[len(pref_b):]\n",
        "            if alt in have:\n",
        "                X[col] = X[alt]\n",
        "\n",
        "        # CSV wants A, we have B â†’ create A from B\n",
        "        if col.startswith(pref_a) and col not in have:\n",
        "            alt = pref_b + col[len(pref_a):]\n",
        "            if alt in have:\n",
        "                X[col] = X[alt]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def compute_10m_features_from_5m(df5: pd.DataFrame,\n",
        "                                 *,\n",
        "                                 prefix: str = \"10min_10min_\",\n",
        "                                 pre_scale: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    From a 5-minute OHLCV df (columns: time, open, high, low, close, volume, ...),\n",
        "    resample to 10m, compute indicators via `create_features`, optionally scale them,\n",
        "    prefix with `prefix`, then align back to the 5m time index (as-of, ffill).\n",
        "\n",
        "    Returns a dataframe indexed by the 5m timestamps with only the prefixed columns.\n",
        "    \"\"\"\n",
        "    if \"time\" in df5.columns:\n",
        "        idx5 = pd.to_datetime(df5[\"time\"], utc=True)\n",
        "        base = df5.set_index(idx5)\n",
        "    else:\n",
        "        base = df5.copy()\n",
        "        base.index = pd.to_datetime(base.index, utc=True)\n",
        "        idx5 = base.index\n",
        "\n",
        "    # robust lower/upper column map for resample\n",
        "    cols = {c.lower(): c for c in base.columns}\n",
        "    # minimal OHLCV present?\n",
        "    for need in (\"open\", \"high\", \"low\", \"close\"):\n",
        "        if need not in cols:\n",
        "            raise ValueError(f\"compute_10m_features_from_5m: missing column '{need}'\")\n",
        "\n",
        "    agg = {\n",
        "        cols.get(\"open\", \"open\"):  \"first\",\n",
        "        cols.get(\"high\", \"high\"):  \"max\",\n",
        "        cols.get(\"low\", \"low\"):    \"min\",\n",
        "        cols.get(\"close\", \"close\"): \"last\",\n",
        "    }\n",
        "    # volume-like fields are optional\n",
        "    if \"volume\" in cols:\n",
        "        agg[cols[\"volume\"]] = \"sum\"\n",
        "    if \"tickvolume\" in cols:\n",
        "        agg[cols[\"tickvolume\"]] = \"sum\"\n",
        "    if \"spread\" in cols:\n",
        "        agg[cols[\"spread\"]] = \"mean\"\n",
        "\n",
        "    ohlc10 = (base\n",
        "              .resample(\"10T\", label=\"right\", closed=\"right\")\n",
        "              .agg(agg)\n",
        "              .dropna(subset=[cols[\"open\"], cols[\"high\"], cols[\"low\"], cols[\"close\"]]))\n",
        "\n",
        "    # TA-Lib expects Title-case OHLCV names\n",
        "    block = ohlc10.rename(columns={\n",
        "        cols.get(\"open\", \"open\"):  \"Open\",\n",
        "        cols.get(\"high\", \"high\"):  \"High\",\n",
        "        cols.get(\"low\", \"low\"):    \"Low\",\n",
        "        cols.get(\"close\", \"close\"): \"Close\",\n",
        "        cols.get(\"volume\", \"volume\"): \"Volume\",\n",
        "    })\n",
        "\n",
        "    f10 = create_features(block)  # <- your function above\n",
        "\n",
        "    if pre_scale:\n",
        "        sc = StandardScaler()\n",
        "        f10 = pd.DataFrame(sc.fit_transform(f10), index=f10.index, columns=f10.columns)\n",
        "\n",
        "    # prefix\n",
        "    f10.columns = [f\"{prefix}{c}\" for c in f10.columns]\n",
        "\n",
        "    # align back to the 5m timestamps (as-of / forward fill)\n",
        "    aligned = f10.reindex(idx5.union(f10.index)).ffill().reindex(idx5)\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def append_10m_features_inplace(\n",
        "    df_5m: pd.DataFrame,\n",
        "    *,\n",
        "    prefix: str = \"10min_10min_\",\n",
        "    pre_scale: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Builds a 10-minute OHLCV from your 5m data, computes your `create_features`,\n",
        "    optionally scales them, and appends to df_5m with the given prefix.\n",
        "\n",
        "    Requirements:\n",
        "      - df_5m has at least ['time','open','high','low','close','volume'].\n",
        "      - `create_features(stock_data)` is defined and returns a DataFrame indexed by time.\n",
        "    \"\"\"\n",
        "    if df_5m is None or df_5m.empty or \"time\" not in df_5m.columns:\n",
        "        return\n",
        "\n",
        "    # Ensure we won't create a column literally named 0 (from accidental index resets)\n",
        "    df5 = df_5m.copy()\n",
        "    t = pd.to_datetime(df5[\"time\"], utc=True, errors=\"coerce\")\n",
        "    base = df5.set_index(t).sort_index()\n",
        "\n",
        "    # Build 10m OHLCV\n",
        "    agg = {\n",
        "        \"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\",\n",
        "        \"volume\": \"sum\"\n",
        "    }\n",
        "    # optional columns if present\n",
        "    if \"tickVolume\" in base.columns: agg[\"tickVolume\"] = \"sum\"\n",
        "    if \"spread\"     in base.columns: agg[\"spread\"]     = \"mean\"\n",
        "\n",
        "    ohlc10 = base.resample(\"10T\").agg(agg).dropna(subset=[\"open\",\"close\"])\n",
        "\n",
        "    # Map to Title-case columns for your create_features()\n",
        "    stock10 = ohlc10.copy()\n",
        "    stock10[\"Open\"]   = stock10[\"open\"]\n",
        "    stock10[\"High\"]   = stock10[\"high\"]\n",
        "    stock10[\"Low\"]    = stock10[\"low\"]\n",
        "    stock10[\"Close\"]  = stock10[\"close\"]\n",
        "    stock10[\"Volume\"] = stock10[\"volume\"]\n",
        "\n",
        "    # Compute indicators with your existing function\n",
        "    try:\n",
        "        feats10 = create_features(stock10[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]])\n",
        "    except Exception as e:\n",
        "        # If create_features fails, do nothing (but do not crash live loop)\n",
        "        print(f\"âš ï¸ 10m feature block skipped (create_features failed): {e}\")\n",
        "        return\n",
        "\n",
        "    # Scale the 10m features if requested\n",
        "    if pre_scale and not feats10.empty:\n",
        "        scaler_10 = StandardScaler()\n",
        "        feats10 = pd.DataFrame(\n",
        "            scaler_10.fit_transform(feats10),\n",
        "            index=feats10.index, columns=feats10.columns\n",
        "        )\n",
        "\n",
        "    # Bring the 10m features to 5m timestamps via asof/ffill\n",
        "    feats10 = feats10.sort_index()\n",
        "    # Align index (5m time index)\n",
        "    idx5 = base.index\n",
        "    feats10_5m = pd.merge_asof(\n",
        "        pd.DataFrame(index=idx5).sort_index(),\n",
        "        feats10.sort_index(),\n",
        "        left_index=True, right_index=True, direction=\"backward\"\n",
        "    )\n",
        "\n",
        "    # Prefix and append to original df_5m (in place)\n",
        "    feats10_5m.columns = [prefix + str(c) for c in feats10_5m.columns]\n",
        "    # Write back by 'time' (align on row order using the same index)\n",
        "    for col in feats10_5m.columns:\n",
        "        df_5m[col] = feats10_5m[col].values\n",
        "\n",
        "\n",
        "# === Utilidades de columnas / guardado ===\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con la hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la Ãºltima fila\n",
        "    mode=\"missing\": rellena donde estÃ© NaT/NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:  # \"missing\"\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "def _ensure_order_cols(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Crea columnas con dtypes correctos; quita 'id' y 'actionType'. Incluye 'source', 'base_px', 'atr_base' y 'Real_SL'.\"\"\"\n",
        "    # Elimina columnas heredadas\n",
        "    for col in (\"id\", \"actionType\"):\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    col_types = {\n",
        "        \"System_time\":   \"datetime64[ns, UTC]\",\n",
        "        \"orderId\":       \"string\",\n",
        "        \"magic\":         \"Int64\",\n",
        "        \"symbol\":        \"string\",\n",
        "        \"openPrice\":     \"float64\",\n",
        "        \"comment\":       \"string\",\n",
        "        \"Type\":          \"string\",               # â† Ãºnica columna de direcciÃ³n\n",
        "        \"Entry_Date\":    \"datetime64[ns, UTC]\",\n",
        "        \"Stop_Loss_atr\": \"float64\",\n",
        "        \"Stop_Loss_$\":   \"float64\",\n",
        "        \"Real_SL\":       \"float64\",              # â† NUEVO\n",
        "        \"ATR\":           \"float64\",\n",
        "        \"atr_mult_high\": \"float64\",\n",
        "        \"atr_mult_low\":  \"float64\",\n",
        "        \"trade_size\":    \"float64\",\n",
        "        \"profits\":       \"float64\",\n",
        "        \"base_px\":       \"float64\",\n",
        "        \"atr_base\":      \"float64\",\n",
        "        \"source\":        \"Int64\",\n",
        "    }\n",
        "\n",
        "    for c, dtp in col_types.items():\n",
        "        if c not in df.columns:\n",
        "            if isinstance(dtp, str) and dtp.startswith(\"datetime64\"):\n",
        "                df[c] = pd.NaT\n",
        "            elif dtp == \"Int64\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"Int64\")\n",
        "            elif dtp == \"string\":\n",
        "                df[c] = pd.Series(pd.NA, dtype=\"string\")\n",
        "            else:\n",
        "                df[c] = np.nan\n",
        "\n",
        "    # normaliza sin romper datos existentes\n",
        "    for c, dtp in col_types.items():\n",
        "        try:\n",
        "            if dtp == \"string\": df[c] = df[c].astype(\"string\")\n",
        "            if dtp == \"Int64\":  df[c] = df[c].astype(\"Int64\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Reordena y guarda el CSV con formato estable:\n",
        "      â€¢ System_time antes de 'time'\n",
        "      â€¢ 'source' justo a la derecha de 'time' y antes de 'open'\n",
        "      â€¢ Entry_Date justo ANTES de 'Stop_Loss_atr'\n",
        "      â€¢ Real_SL a la derecha de 'Stop_Loss_$'; luego base_px y atr_base\n",
        "      â€¢ Bloque ML (label/prob_*) inmediatamente despuÃ©s de 'Type'\n",
        "    Formatea System_time/time/Entry_Date sin tz. Elimina 'id', 'brokerTime' y 'actionType' si aparecieran.\n",
        "    \"\"\"\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Quitar columnas heredadas si existen\n",
        "    df_out.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "    # Formatear tiempos como strings sin tz\n",
        "    for col in [\"System_time\", \"time\", \"Entry_Date\"]:\n",
        "        if col in df_out.columns:\n",
        "            ser = pd.to_datetime(df_out[col], errors=\"coerce\", utc=True)\n",
        "            df_out[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers de reorden â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols:\n",
        "            return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca Real_SL inmediatamente despuÃ©s de Stop_Loss_$; luego base_px y atr_base.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        # quitar para reinsertar\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns:\n",
        "                    cols.insert(i, c)\n",
        "                    i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df_out.columns and c not in cols:\n",
        "                    cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca 'source' inmediatamente despuÃ©s de 'time' y antes de 'open' si aplica.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols:\n",
        "            cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            # si quedÃ³ despuÃ©s de 'open', muÃ©velo antes\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols:\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else:\n",
        "                cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_ml(cols: list[str]) -> list[str]:\n",
        "        \"\"\"Coloca las columnas del modelo (predicciÃ³n y probas) despuÃ©s de 'Type'.\"\"\"\n",
        "        cols = cols.copy()\n",
        "        ml_block = [\n",
        "            \"label_ml_long\", \"prob_0_long\", \"prob_1_long\", \"prob_2_long\",\n",
        "            \"label_ml_short\", \"prob_0_short\", \"prob_1_short\", \"prob_2_short\"\n",
        "        ]\n",
        "        # eliminar del orden actual para reinsertarlas en bloque\n",
        "        for c in ml_block:\n",
        "            if c in cols:\n",
        "                cols.remove(c)\n",
        "        insert_at = cols.index(\"Type\") + 1 if \"Type\" in cols else len(cols)\n",
        "        for c in ml_block:\n",
        "            if c in df_out.columns:\n",
        "                cols.insert(insert_at, c)\n",
        "                insert_at += 1\n",
        "        return cols\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "    # Orden base (System_time inmediatamente antes de 'time')\n",
        "    cols = list(df_out.columns)\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    # Aplicar reglas de reorden\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "    ordered = _reorder_ml(ordered)\n",
        "\n",
        "    # Guardar\n",
        "    df_out.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "def migrate_csv_if_needed(path: str = FILE_PATH) -> None:\n",
        "    \"\"\"\n",
        "    Migra CSV existente:\n",
        "      â€¢ Elimina 'id', 'brokerTime' y 'actionType' si existen.\n",
        "      â€¢ Agrega si faltan: 'profits','trade_size','base_px','atr_base','source','System_time','Real_SL'.\n",
        "      â€¢ Formatea System_time/time/Entry_Date sin tz.\n",
        "      â€¢ Reordena: System_time antes de time; source despuÃ©s de time y antes de open;\n",
        "                  Entry_Date antes de Stop_Loss_atr;\n",
        "                  Real_SL despuÃ©s de Stop_Loss_$; base_px/atr_base despuÃ©s de Real_SL.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\")\n",
        "\n",
        "    ensure_cols = {\n",
        "        \"System_time\": pd.NaT,\n",
        "        \"profits\":     np.nan,\n",
        "        \"trade_size\":  np.nan,\n",
        "        \"base_px\":     np.nan,\n",
        "        \"atr_base\":    np.nan,\n",
        "        \"source\":      pd.NA,\n",
        "        \"Real_SL\":     np.nan,  # â† NUEVO\n",
        "    }\n",
        "    for c, default in ensure_cols.items():\n",
        "        if c not in df.columns:\n",
        "            df[c] = default\n",
        "\n",
        "    for col in [\"System_time\", \"time\", \"Entry_Date\"]:\n",
        "        if col in df.columns:\n",
        "            ser = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            df[col] = ser.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    def _reorder_for_entry_date(cols: list[str]) -> list[str]:\n",
        "        if \"Entry_Date\" not in cols: return cols\n",
        "        cols = cols.copy()\n",
        "        cols.remove(\"Entry_Date\")\n",
        "        if \"Stop_Loss_atr\" in cols:\n",
        "            cols.insert(cols.index(\"Stop_Loss_atr\"), \"Entry_Date\")\n",
        "        elif \"Type\" in cols:\n",
        "            cols.insert(cols.index(\"Type\") + 1, \"Entry_Date\")\n",
        "        else:\n",
        "            cols.append(\"Entry_Date\")\n",
        "        return cols\n",
        "\n",
        "    def _reorder_stop_cols(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "            if c in cols: cols.remove(c)\n",
        "        if \"Stop_Loss_$\" in cols:\n",
        "            i = cols.index(\"Stop_Loss_$\") + 1\n",
        "            if \"Real_SL\" in df.columns: cols.insert(i, \"Real_SL\"); i += 1\n",
        "            for c in [\"base_px\", \"atr_base\"]:\n",
        "                if c in df.columns:\n",
        "                    cols.insert(i, c); i += 1\n",
        "        else:\n",
        "            for c in [\"Real_SL\", \"base_px\", \"atr_base\"]:\n",
        "                if c in df.columns and c not in cols: cols.append(c)\n",
        "        return cols\n",
        "\n",
        "    def _reorder_source(cols: list[str]) -> list[str]:\n",
        "        cols = cols.copy()\n",
        "        if \"source\" in cols: cols.remove(\"source\")\n",
        "        if \"time\" in cols:\n",
        "            i = cols.index(\"time\") + 1\n",
        "            cols.insert(i, \"source\")\n",
        "            if \"open\" in cols and cols.index(\"source\") > cols.index(\"open\"):\n",
        "                cols.remove(\"source\")\n",
        "                cols.insert(cols.index(\"open\"), \"source\")\n",
        "        else:\n",
        "            if \"open\" in cols: cols.insert(cols.index(\"open\"), \"source\")\n",
        "            else: cols.append(\"source\")\n",
        "        return cols\n",
        "\n",
        "    cols = list(df.columns)\n",
        "    if \"time\" in cols:\n",
        "        cols_wo_sys = [c for c in cols if c != \"System_time\"]\n",
        "        i = cols_wo_sys.index(\"time\")\n",
        "        ordered = cols_wo_sys[:i] + [\"System_time\"] + cols_wo_sys[i:]\n",
        "    else:\n",
        "        ordered = cols\n",
        "\n",
        "    ordered = _reorder_source(ordered)\n",
        "    ordered = _reorder_for_entry_date(ordered)\n",
        "    ordered = _reorder_stop_cols(ordered)\n",
        "\n",
        "    df.to_csv(path, index=False, columns=ordered)\n",
        "\n",
        "def _load_csv() -> pd.DataFrame:\n",
        "    \"\"\"Lee el CSV preservando tipos; convierte tiempos a UTC tz-aware; elimina 'id', 'actionType' y 'brokerTime'.\"\"\"\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        FILE_PATH,\n",
        "        dtype={\n",
        "            \"orderId\": \"string\",\n",
        "            \"symbol\":  \"string\",\n",
        "            \"comment\": \"string\",\n",
        "            \"Type\":    \"string\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    df.drop(columns=[\"id\", \"brokerTime\", \"actionType\"], errors=\"ignore\")\n",
        "\n",
        "    for col in [\"time\", \"Entry_Date\", \"System_time\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def stamp_system_time(df: pd.DataFrame, mode: str = \"last\") -> None:\n",
        "    \"\"\"\n",
        "    Sella System_time con hora del sistema (UTC, sin milisegundos).\n",
        "    mode=\"last\": solo la Ãºltima fila; mode=\"missing\": rellena las que estÃ©n vacÃ­as.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    _ensure_order_cols(df)\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    if mode == \"last\":\n",
        "        df.at[df.index[-1], \"System_time\"] = now_utc\n",
        "    else:\n",
        "        mask = df[\"System_time\"].isna()\n",
        "        if mask.any():\n",
        "            df.loc[mask, \"System_time\"] = now_utc\n",
        "\n",
        "async def get_current_candle_snapshot(account,\n",
        "                                      rpc_conn,\n",
        "                                      symbol: str = SYMBOL,\n",
        "                                      timeframe: str = time_frame_data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve 1 fila con la vela mÃ¡s reciente (puede ser la vela en curso)\n",
        "    con campos: time, open, high, low, close, volume, tickVolume, spread.\n",
        "\n",
        "    Intenta primero vÃ­a RPC (candles en vivo) y, si falla, usa el\n",
        "    endpoint histÃ³rico como respaldo.\n",
        "    \"\"\"\n",
        "    def _to_row(c: dict) -> dict:\n",
        "        return {\n",
        "            \"time\":       pd.to_datetime(c.get(\"time\"), utc=True, errors=\"coerce\"),\n",
        "            \"open\":       float(c.get(\"open\"))       if c.get(\"open\")       is not None else np.nan,\n",
        "            \"high\":       float(c.get(\"high\"))       if c.get(\"high\")       is not None else np.nan,\n",
        "            \"low\":        float(c.get(\"low\"))        if c.get(\"low\")        is not None else np.nan,\n",
        "            \"close\":      float(c.get(\"close\"))      if c.get(\"close\")      is not None else np.nan,\n",
        "            \"volume\":     float(c.get(\"volume\"))     if c.get(\"volume\")     is not None else np.nan,\n",
        "            \"tickVolume\": float(c.get(\"tickVolume\") if c.get(\"tickVolume\") is not None else c.get(\"tick_volume\") or np.nan),\n",
        "            \"spread\":     float(c.get(\"spread\"))     if c.get(\"spread\")     is not None else np.nan,\n",
        "        }\n",
        "\n",
        "    # 1) Intento RPC (normalmente expone la vela actual)\n",
        "    try:\n",
        "        # Variantes posibles segÃºn SDK; probamos dos firmas comunes.\n",
        "        try:\n",
        "            # a) lÃ­mite por cantidad\n",
        "            candles = await rpc_conn.get_candles(symbol=SYMBOL, timeframe=timeframe, limit=1)\n",
        "        except TypeError:\n",
        "            # b) rango por tiempo (Ãºltimos minutos); tomamos la Ãºltima\n",
        "            to_ts   = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            from_ts = to_ts - dt.timedelta(minutes=10)\n",
        "            candles = await rpc_conn.get_candles(symbol=SYMBOL, timeframe=timeframe,\n",
        "                                                 start_time=from_ts, end_time=to_ts)\n",
        "        if candles:\n",
        "            row = _to_row(candles[-1])\n",
        "            return pd.DataFrame([row])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Respaldo: histÃ³rico (toma la mÃ¡s reciente cerrada si el broker no sirve la actual)\n",
        "    try:\n",
        "        candles = await account.get_historical_candles(symbol=SYMBOL, timeframe=timeframe, start_time=None, limit=1)\n",
        "        if candles:\n",
        "            row = _to_row(candles[-1])\n",
        "            return pd.DataFrame([row])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 3) Si todo falla, devolver DF vacÃ­o con columnas estandarizadas\n",
        "    return pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tickVolume\", \"spread\"])\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# CONFIGURACIÃ“N GENERAL\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logging.getLogger(\"metaapi_cloud_sdk\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"websockets\").setLevel(logging.ERROR)\n",
        "\n",
        "#from metaapi_cloud_sdk.clients.error_handling import TradeException\n",
        "\n",
        "MetaApi.enable_logging()\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "for noisy in (\"metaapi_cloud_sdk\", \"socketio\", \"engineio\"):\n",
        "    logging.getLogger(noisy).setLevel(logging.ERROR)\n",
        "\n",
        "###############################################################################\n",
        "# CONEXIÃ“N A METAAPI\n",
        "\n",
        "async def connect_metaapi(token: str, account_id: str):\n",
        "    api     = MetaApi(token)\n",
        "    account = await api.metatrader_account_api.get_account(account_id)\n",
        "    conn    = account.get_rpc_connection()\n",
        "    await conn.connect(); await conn.wait_synchronized()\n",
        "    return account\n",
        "\n",
        "###############################################################################\n",
        "# DESCARGA DE VELAS\n",
        "\n",
        "async def get_candles_5m(account, start: dt.datetime | None, limit: int = CANDEL_NUMBER):\n",
        "    candles = await account.get_historical_candles(\n",
        "        symbol=SYMBOL, timeframe= time_frame_data, start_time=start, limit = limit)\n",
        "    return pd.DataFrame([{\n",
        "        \"time\": pd.to_datetime(c[\"time\"], utc=True),\n",
        "        \"open\": c[\"open\"], \"high\": c[\"high\"], \"low\": c[\"low\"], \"close\": c[\"close\"],\n",
        "        \"volume\": c[\"volume\"], \"tickVolume\": c[\"tickVolume\"], \"spread\": c[\"spread\"]\n",
        "    } for c in candles])\n",
        "\n",
        "###############################################################################\n",
        "# SINCRONIZAR CON EL RELOJ (prÃ³ximo minuto:07)\n",
        "\n",
        "def seconds_until_next_m07() -> float:\n",
        "    now = dt.datetime.utcnow()\n",
        "    nxt = (now + dt.timedelta(minutes=1)).replace(second=3, microsecond=0)\n",
        "    return max((nxt - now).total_seconds(), 0.5)\n",
        "\n",
        "def seconds_until_next_5m03() -> float:\n",
        "    \"\"\"\n",
        "    Seconds until the next 5-minute boundary + 3s (to let the bar close on most feeds).\n",
        "    Triggers at hh:00:03, :05:03, :10:03, ... UTC.\n",
        "    \"\"\"\n",
        "    now = dt.datetime.utcnow()\n",
        "    # floor to current 5-minute block start\n",
        "    start_5m = now.replace(minute=(now.minute // 5) * 5, second=0, microsecond=0)\n",
        "    nxt = start_5m + dt.timedelta(minutes=5, seconds=3)\n",
        "    return max((nxt - now).total_seconds(), 0.5)\n",
        "\n",
        "\n",
        "def generate_trade_signals(\n",
        "    df: pd.DataFrame,\n",
        "    length_1: int,\n",
        "    length_2: int,\n",
        "    length_3: int,\n",
        "    length_4: int,\n",
        "    smooth_1: int,\n",
        "    smooth_2: int,\n",
        "    smooth_3: int,\n",
        "    smooth_4: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula 4 lÃ­neas de Kalman sobre 'close' y crea:\n",
        "      â€¢ kal_1, kal_2, kal_3, kal_4\n",
        "      â€¢ Open_Trade: +1 (BUY) / -1 (SELL) cuando CAMBIA sesgo (k1..k3)\n",
        "      â€¢ Close_Trade: -1 si kal_4 < kal_4.shift()  â†’ cierra BUY\n",
        "                     +1 si kal_4 > kal_4.shift()  â†’ cierra SELL\n",
        "        (seÃ±ales consecutivas iguales se deduplican)\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    if \"close\" not in df.columns:\n",
        "        raise ValueError(\"generate_trade_signals: falta columna 'close'.\")\n",
        "\n",
        "    close = pd.to_numeric(df[\"close\"], errors=\"coerce\").fillna(method=\"ffill\")\n",
        "\n",
        "    def _clamp_int(x, mn=1):\n",
        "        try: x = int(x)\n",
        "        except Exception: x = mn\n",
        "        return max(x, mn)\n",
        "\n",
        "    length_1 = _clamp_int(length_1); length_2 = _clamp_int(length_2)\n",
        "    length_3 = _clamp_int(length_3); length_4 = _clamp_int(length_4)\n",
        "    smooth_1 = _clamp_int(smooth_1); smooth_2 = _clamp_int(smooth_2)\n",
        "    smooth_3 = _clamp_int(smooth_3); smooth_4 = _clamp_int(smooth_4)\n",
        "\n",
        "    # Kalman lines\n",
        "    df[\"kal_1\"] = kalman_line(close, length_1, smooth_1)\n",
        "    df[\"kal_2\"] = kalman_line(close, length_2, smooth_2)\n",
        "    df[\"kal_3\"] = kalman_line(close, length_3, smooth_3)\n",
        "    df[\"kal_4\"] = kalman_line(close, length_4, smooth_4)\n",
        "\n",
        "    # Sesgo por k1..k3\n",
        "    k1_up, k2_up, k3_up = df[\"kal_1\"] > df[\"kal_1\"].shift(1), df[\"kal_2\"] > df[\"kal_2\"].shift(1), df[\"kal_3\"] > df[\"kal_3\"].shift(1)\n",
        "    k1_dn, k2_dn, k3_dn = df[\"kal_1\"] < df[\"kal_1\"].shift(1), df[\"kal_2\"] < df[\"kal_2\"].shift(1), df[\"kal_3\"] < df[\"kal_3\"].shift(1)\n",
        "\n",
        "    bull = k1_up & k2_up & k3_up\n",
        "    bear = k1_dn & k2_dn & k3_dn\n",
        "    aux  = np.where(bull, 1, np.where(bear, -1, np.nan))\n",
        "    df[\"Open_Trade\"] = np.where(pd.Series(aux).shift(1) != aux, aux, np.nan)\n",
        "\n",
        "    # Cierre estrictamente por pendiente de kal_4\n",
        "    k4_up = df[\"kal_4\"] > df[\"kal_4\"].shift(1)   # +1 â†’ cierra SELL\n",
        "    k4_dn = df[\"kal_4\"] < df[\"kal_4\"].shift(1)   # -1 â†’ cierra BUY\n",
        "    close_raw  = np.where(k4_dn, -1, np.where(k4_up, 1, np.nan))\n",
        "    close_sr   = pd.Series(close_raw)\n",
        "    df[\"Close_Trade\"] = close_sr.where(close_sr != close_sr.shift(1), np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "def _rest_place_order(auth_token: str,\n",
        "                      account_id: str,\n",
        "                      region: str,\n",
        "                      symbol: str,\n",
        "                      side: str,\n",
        "                      volume: float,\n",
        "                      comment: str = \"Kal\",\n",
        "                      magic: int | None = None,\n",
        "                      stop_loss: float | None = None,\n",
        "                      take_profit: float | None = None,\n",
        "                      timeout: int = 20):\n",
        "    side = side.upper().strip()\n",
        "    action_map = {\"BUY\": \"ORDER_TYPE_BUY\", \"SELL\": \"ORDER_TYPE_SELL\"}\n",
        "    if side not in action_map:\n",
        "        raise ValueError(\"side must be 'BUY' or 'SELL'\")\n",
        "\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\n",
        "        \"symbol\": symbol,\n",
        "        \"actionType\": action_map[side],\n",
        "        \"volume\": float(volume),\n",
        "        \"comment\": str(comment)\n",
        "    }\n",
        "    if magic is not None:\n",
        "        try: payload[\"magic\"] = int(magic)\n",
        "        except Exception: pass\n",
        "    if stop_loss is not None:\n",
        "        try: payload[\"stopLoss\"] = float(stop_loss)\n",
        "        except Exception: pass\n",
        "    if take_profit is not None:\n",
        "        try: payload[\"takeProfit\"] = float(take_profit)\n",
        "        except Exception: pass\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- FIX 1: really use the symbol param here -------------------------------\n",
        "def _rest_get_positions(auth_token: str,\n",
        "                        account_id: str,\n",
        "                        region: str,\n",
        "                        symbol: str | None = None,\n",
        "                        timeout: int = 15):\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions\"\n",
        "    headers = {\"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    params = {}\n",
        "    if symbol:\n",
        "        params[\"symbol\"] = str(symbol)   # << was using the global SYMBOL\n",
        "    try:\n",
        "        return requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        class _Dummy:\n",
        "            status_code = 0\n",
        "            def json(self): return {\"error\": str(e)}\n",
        "            text = str(e)\n",
        "        return _Dummy()\n",
        "\n",
        "\n",
        "def _rest_modify_position(auth_token: str,\n",
        "                          account_id: str,\n",
        "                          region: str,\n",
        "                          position_id: str,\n",
        "                          stop_loss: float | None = None,\n",
        "                          take_profit: float | None = None,\n",
        "                          timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Modifica una posiciÃ³n abierta (SL/TP) usando el endpoint REST de MetaApi.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_MODIFY\", \"positionId\": str(position_id)}\n",
        "\n",
        "    if stop_loss is not None:\n",
        "        payload[\"stopLoss\"] = float(stop_loss)\n",
        "\n",
        "    if take_profit is not None:\n",
        "        payload[\"takeProfit\"] = float(take_profit)\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "# === DROP-IN REPLACEMENT ==============================================\n",
        "async def open_trade(df: pd.DataFrame,\n",
        "                     rpc_conn,\n",
        "                     symbol: str = \"BTCUSD\",\n",
        "                     lot: float = 1.0,\n",
        "                     comment: str = \"Kal\",\n",
        "                     magic: int = 900002):\n",
        "    \"\"\"\n",
        "    Entra solo si:\n",
        "      â€¢ Open_Trade âˆˆ {+1, -1}\n",
        "      â€¢ y la predicciÃ³n del modelo del lado (label_ml_long/label_ml_short) âˆˆ {1, 2}\n",
        "\n",
        "    SL/TP al abrir (en precio) se calculan con ATR del Ãºltimo bar:\n",
        "      â€¢ Para BUY:  SL = close - ATR * sl_mult ; TP = close + ATR * tp_mult\n",
        "      â€¢ Para SELL: SL = close + ATR * sl_mult ; TP = close - ATR * tp_mult\n",
        "\n",
        "    Donde (sl_mult, tp_mult) dependen de la clase:\n",
        "      â€¢ class=1 â†’ (stoploss_1, takeprofit_1)\n",
        "      â€¢ class=2 â†’ (stoploss_2, takeprofit_2)\n",
        "      â€¢ class=0 â†’ no abre\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df.empty or \"Open_Trade\" not in df.columns:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    # ---------- helpers for existing open/magic sync ----------\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic): return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\"  in tt: return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt: return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    # ---------- already open? just sync and exit ----------\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    open_with_magic = [p for p in positions if _has_magic(p)]\n",
        "\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"s\")\n",
        "    row = df.index[-1]\n",
        "\n",
        "    if open_with_magic:\n",
        "        p = open_with_magic[0]\n",
        "        df.at[row, \"System_time\"] = now_utc\n",
        "        pm = p.get(\"magic\")\n",
        "        pc = str(p.get(\"comment\") or \"\")\n",
        "        clean_cmt, mag_from_cmt = _split_comment_magic(pc)\n",
        "\n",
        "        if pm is not None and str(pm).strip() != \"\":\n",
        "            try: df.at[row, \"magic\"] = int(pm)\n",
        "            except Exception: df.at[row, \"magic\"] = int(magic)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.at[row, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.at[row, \"magic\"] = int(magic)\n",
        "\n",
        "        df.at[row, \"symbol\"]    = str(p.get(\"symbol\") or symbol)\n",
        "        df.at[row, \"openPrice\"] = float(p.get(\"openPrice\") or p.get(\"price\") or np.nan)\n",
        "        df.at[row, \"comment\"]   = clean_cmt or str(comment)\n",
        "\n",
        "        vol = p.get(\"volume\") or p.get(\"lots\") or None\n",
        "        if vol is not None:\n",
        "            try: df.at[row, \"trade_size\"] = float(vol)\n",
        "            except Exception: pass\n",
        "\n",
        "        side = _side_of(p)\n",
        "        if side: df.at[row, \"Type\"] = \"Long\" if side == \"BUY\" else \"Short\"\n",
        "        if pd.isna(df.at[row, \"Entry_Date\"]): df.at[row, \"Entry_Date\"] = now_utc\n",
        "\n",
        "        save_csv(df)\n",
        "        print(\"â„¹ Position already open; synced last row and skipped new order.\")\n",
        "        return\n",
        "\n",
        "    # ---------- side signal from Open_Trade ----------\n",
        "    sig = df[\"Open_Trade\"].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "    side_req = \"BUY\" if sig == 1 else (\"SELL\" if sig == -1 else None)\n",
        "    if side_req is None:\n",
        "        return\n",
        "\n",
        "    # ---------- read prediction for that side (0/1/2) ----------\n",
        "    pred_col = \"label_ml_long\" if sig == 1 else \"label_ml_short\"\n",
        "    pred_val = df[pred_col].iloc[-1] if pred_col in df.columns else np.nan\n",
        "    try:\n",
        "        pred_cls = int(pred_val)\n",
        "    except Exception:\n",
        "        pred_cls = np.nan\n",
        "\n",
        "    # Rule: class 0 â†’ do not place any order\n",
        "    if not np.isfinite(pred_cls) or pred_cls == 0:\n",
        "        print(f\"â„¹ No entry: {pred_col}={pred_val} â†’ skip order.\")\n",
        "        return\n",
        "\n",
        "    # Choose ATR multiples from prediction\n",
        "    if pred_cls == 1:\n",
        "        sl_mult, tp_mult = float(stoploss_1), float(takeprofit_1)\n",
        "    elif pred_cls == 2:\n",
        "        sl_mult, tp_mult = float(stoploss_2), float(takeprofit_2)\n",
        "    else:\n",
        "        print(f\"â„¹ Unknown class {pred_cls}; skip order.\")\n",
        "        return\n",
        "\n",
        "    # ---------- prices from ATR ----------\n",
        "    prev_close = float(df[\"close\"].iloc[-1]) if pd.notna(df[\"close\"].iloc[-1]) else np.nan\n",
        "    atr_val    = float(df[\"ATR\"].iloc[-1])   if \"ATR\" in df.columns and pd.notna(df[\"ATR\"].iloc[-1]) else np.nan\n",
        "    if not (np.isfinite(prev_close) and np.isfinite(atr_val)):\n",
        "        print(\"âœ˜ Missing close/ATR â†’ skip order.\")\n",
        "        return\n",
        "\n",
        "    if side_req == \"BUY\":\n",
        "        sl_to_send = prev_close - atr_val * sl_mult\n",
        "        tp_to_send = prev_close + atr_val * tp_mult\n",
        "    else:  # SELL\n",
        "        sl_to_send = prev_close + atr_val * sl_mult\n",
        "        tp_to_send = prev_close - atr_val * tp_mult\n",
        "\n",
        "    # ---------- send order with SL & TP ----------\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_place_order(\n",
        "            auth_token=META_API_TOKEN,\n",
        "            account_id=ACCOUNT_ID,\n",
        "            region=REGION,\n",
        "            symbol=symbol,\n",
        "            side=side_req,\n",
        "            volume=float(lot),\n",
        "            comment=str(comment),\n",
        "            magic=int(magic),\n",
        "            stop_loss=sl_to_send,\n",
        "            take_profit=tp_to_send,\n",
        "            timeout=20\n",
        "        )\n",
        "    )\n",
        "    if resp.status_code != 200:\n",
        "        try: err = resp.json()\n",
        "        except Exception: err = {\"raw\": resp.text[:500]}\n",
        "        print(\"âœ˜ Order failed\", resp.status_code, json.dumps(err, indent=2, ensure_ascii=False))\n",
        "        return\n",
        "\n",
        "    data        = resp.json()\n",
        "    order_id    = str(data.get(\"orderId\") or \"\")\n",
        "    position_id = str(data.get(\"positionId\") or \"\")\n",
        "\n",
        "    # ---------- write metadata ----------\n",
        "    df.at[row, \"System_time\"]   = now_utc\n",
        "    df.at[row, \"orderId\"]       = order_id\n",
        "    df.at[row, \"magic\"]         = int(magic)\n",
        "    df.at[row, \"symbol\"]        = symbol\n",
        "    df.at[row, \"comment\"]       = str(comment)\n",
        "    df.at[row, \"Entry_Date\"]    = now_utc\n",
        "    df.at[row, \"trade_size\"]    = float(lot)\n",
        "    df.at[row, \"Stop_Loss_$\"]   = float(sl_to_send)\n",
        "    df.at[row, \"Stop_Loss_atr\"] = float(sl_mult)\n",
        "    # (opcional) conservar TP en el CSV para trazabilidad\n",
        "    try: df.at[row, \"Take_Profit_$\"] = float(tp_to_send)\n",
        "    except Exception: pass\n",
        "\n",
        "    # Try to fetch openPrice & type\n",
        "    open_price, fetched_typ = np.nan, None\n",
        "    if rpc_conn:\n",
        "        try:\n",
        "            for _ in range(60):\n",
        "                pos_list = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "                match = None\n",
        "                for p in (pos_list or []):\n",
        "                    pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                    if pid == position_id or pid == order_id or _has_magic(p):\n",
        "                        match = p; break\n",
        "                if match:\n",
        "                    val = match.get(\"openPrice\") or match.get(\"price\") or match.get(\"open_price\")\n",
        "                    if val is not None: open_price = float(val)\n",
        "                    t = _side_of(match)\n",
        "                    if t: fetched_typ = \"Long\" if t == \"BUY\" else \"Short\"\n",
        "                    break\n",
        "                await asyncio.sleep(0.5)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not np.isfinite(open_price):\n",
        "        try: open_price = float(df.at[row, \"close\"])\n",
        "        except Exception: open_price = np.nan\n",
        "    df.at[row, \"openPrice\"] = open_price\n",
        "    df.at[row, \"Type\"]      = fetched_typ if fetched_typ else (\"Long\" if side_req == \"BUY\" else \"Short\")\n",
        "\n",
        "    save_csv(df)\n",
        "    print(f\"âœ… {side_req} placed | cls={pred_cls} | SL@{sl_to_send:.2f} (x{sl_mult}) | \"\n",
        "          f\"TP@{tp_to_send:.2f} (x{tp_mult}) | orderId={order_id} positionId={position_id}\")\n",
        "\n",
        "\n",
        "def atr_close(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Por bloque de trade:\n",
        "      â€¢ forward-fill de metadatos (incluye Type)\n",
        "      â€¢ fija 'base_px' y 'atr_base' con los valores de APERTURA del trade (constantes en el bloque)\n",
        "      â€¢ calcula atr_mult_* usando ese 'atr_base'\n",
        "      â€¢ calcula 'profits' SOLO si estÃ¡ NaN (para no pisar el de la API)\n",
        "      â€¢ propaga Real_SL en el bloque con el Ãºltimo valor no nulo\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    for c in ('time', 'Entry_Date'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=True)\n",
        "\n",
        "    # Reiniciamos solo columnas derivadas de ATR (no profits)\n",
        "    for c in ('atr_mult_high', 'atr_mult_low'):\n",
        "        if c not in df.columns: df[c] = np.nan\n",
        "        else: df[c].values[:] = np.nan\n",
        "    if 'profits' not in df.columns:\n",
        "        df['profits'] = np.nan\n",
        "    if 'base_px'  not in df.columns: df['base_px']  = np.nan\n",
        "    if 'atr_base' not in df.columns: df['atr_base'] = np.nan\n",
        "    if 'Real_SL'  not in df.columns: df['Real_SL']  = np.nan\n",
        "\n",
        "    # Detectar inicios de trade (por orderId; fallback Entry_Date)\n",
        "    starts_mask = pd.Series(False, index=df.index)\n",
        "    if 'orderId' in df.columns:\n",
        "        oid = df['orderId']\n",
        "        starts_mask = oid.notna() & (oid != oid.shift(1))\n",
        "    if not starts_mask.any() and 'Entry_Date' in df.columns:\n",
        "        ed = pd.to_datetime(df['Entry_Date'], errors='coerce', utc=True)\n",
        "        starts_mask = ed.notna() & (ed != ed.shift(1))\n",
        "    if not starts_mask.any():\n",
        "        return df\n",
        "\n",
        "    groups = starts_mask.cumsum()\n",
        "    trade_ids = groups[starts_mask].unique()\n",
        "\n",
        "    meta_cols = [\"orderId\",\"magic\",\"symbol\",\"openPrice\",\"comment\",\"Type\",\"Entry_Date\",\"trade_size\"]\n",
        "\n",
        "    for gid in trade_ids:\n",
        "        mask = (groups == gid)\n",
        "        start_idx = df.index[mask][0]\n",
        "\n",
        "        base_px = df.at[start_idx, 'openPrice'] if 'openPrice' in df.columns else np.nan\n",
        "        try: base_px = float(base_px)\n",
        "        except Exception: base_px = np.nan\n",
        "        if not np.isfinite(base_px) and 'close' in df.columns:\n",
        "            try: base_px = float(df.at[start_idx, 'close'])\n",
        "            except Exception: base_px = np.nan\n",
        "\n",
        "        atr_base = np.nan\n",
        "        if 'atr_base' in df.columns and pd.notna(df.at[start_idx, 'atr_base']):\n",
        "            try: atr_base = float(df.at[start_idx, 'atr_base'])\n",
        "            except Exception: atr_base = np.nan\n",
        "        if not np.isfinite(atr_base) and 'ATR' in df.columns and pd.notna(df.at[start_idx, 'ATR']):\n",
        "            try: atr_base = float(df.at[start_idx, 'ATR'])\n",
        "            except Exception: atr_base = np.nan\n",
        "\n",
        "        typ = str(df.at[start_idx, 'Type']) if 'Type' in df.columns and pd.notna(df.at[start_idx, 'Type']) else None\n",
        "\n",
        "        # Forward-fill de metadatos del bloque\n",
        "        df.loc[mask, [c for c in meta_cols if c in df.columns]] = df.loc[start_idx, [c for c in meta_cols if c in df.columns]].values\n",
        "\n",
        "        if np.isfinite(base_px):  df.loc[mask, 'base_px']  = base_px\n",
        "        if np.isfinite(atr_base): df.loc[mask, 'atr_base'] = atr_base\n",
        "\n",
        "        if np.isfinite(base_px) and np.isfinite(atr_base) and atr_base != 0.0 and typ in ('Long','Short'):\n",
        "            if typ == 'Long':\n",
        "                df.loc[mask, 'atr_mult_high'] = ((df.loc[mask, 'high'] - base_px) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low']  = ((df.loc[mask,  'low'] - base_px) / atr_base).round(2)\n",
        "            else:\n",
        "                df.loc[mask, 'atr_mult_high'] = ((base_px - df.loc[mask, 'high']) / atr_base).round(2)\n",
        "                df.loc[mask, 'atr_mult_low']  = ((base_px - df.loc[mask,  'low']) / atr_base).round(2)\n",
        "\n",
        "        # Solo rellenar profits donde estÃ© NaN (API manda en sync_stop_loss_from_df)\n",
        "        size = float(df.at[start_idx, 'trade_size']) if 'trade_size' in df.columns and pd.notna(df.at[start_idx, 'trade_size']) else np.nan\n",
        "        if np.isfinite(base_px) and np.isfinite(size) and typ in ('Long','Short'):\n",
        "            m_nan = mask & df['profits'].isna()\n",
        "            if m_nan.any():\n",
        "                if typ == 'Long':\n",
        "                    df.loc[m_nan, 'profits'] = ((df.loc[m_nan, 'close'] - base_px) * size).round(2)\n",
        "                else:\n",
        "                    df.loc[m_nan, 'profits'] = ((base_px - df.loc[m_nan, 'close']) * size).round(2)\n",
        "\n",
        "    # === Propagar Real_SL dentro de cada bloque (Ãºltimo valor no nulo) =======\n",
        "    if 'orderId' in df.columns and df['orderId'].notna().any():\n",
        "        starts = df['orderId'].notna() & (df['orderId'] != df['orderId'].shift(1))\n",
        "    else:\n",
        "        ed2 = pd.to_datetime(df.get('Entry_Date'), errors='coerce', utc=True)\n",
        "        starts = ed2.notna() & (ed2 != ed2.shift(1))\n",
        "\n",
        "    if starts.any():\n",
        "        grp = starts.cumsum()\n",
        "        for gid in grp[starts].unique():\n",
        "            m = (grp == gid)\n",
        "            ser = df.loc[m, 'Real_SL']\n",
        "            if ser.notna().any():\n",
        "                val = float(np.round(ser.dropna().iloc[-1], 2))\n",
        "                df.loc[m, 'Real_SL'] = val\n",
        "\n",
        "    return df\n",
        "\n",
        "def tick_dyn_atr(df: pd.DataFrame,\n",
        "                 initial_atr: float = INITIAL_SL,\n",
        "                 first_step_atr: float = FIRST_STEP_ATR,\n",
        "                 gap_first_step_atr: float = GAP_FIRST_STEP_ATR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    DinÃ¡mica de stop en mÃºltiplos de ATR usando atr_mult_high/atr_mult_low.\n",
        "\n",
        "    AdemÃ¡s de calcular la columna 'tick_dyn_atr' (mÃºltiplos ATR),\n",
        "    **escribe el precio del stop en 'Stop_Loss_$'** a partir de:\n",
        "      â€¢ base_px\n",
        "      â€¢ atr_base\n",
        "      â€¢ Type ('Long' o 'Short')\n",
        "      â€¢ tick_dyn_atr (mÃºltiplos)\n",
        "\n",
        "    FÃ³rmulas:\n",
        "      Long  -> Stop_Loss_$ = base_px + atr_base * tick_dyn_atr\n",
        "      Short -> Stop_Loss_$ = base_px - atr_base * tick_dyn_atr\n",
        "    \"\"\"\n",
        "    col_name = 'tick_dyn_atr'\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = np.nan\n",
        "    if 'Stop_Loss_$' not in df.columns:\n",
        "        df['Stop_Loss_$'] = np.nan\n",
        "    # (opcional) mantener tambiÃ©n el mÃºltiplo en Stop_Loss_atr si existe esa columna\n",
        "    if 'Stop_Loss_atr' not in df.columns:\n",
        "        df['Stop_Loss_atr'] = np.nan\n",
        "\n",
        "    in_trade       = False\n",
        "    trade_active   = False\n",
        "    broken         = False\n",
        "    sl_val         = initial_atr\n",
        "    next_threshold = first_step_atr\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        # Detecta nueva apertura real de trade\n",
        "        new_open = (\n",
        "            (('orderId'   in df.columns) and pd.notna(row.get('orderId'))) or\n",
        "            (('openPrice' in df.columns) and pd.notna(row.get('openPrice'))) or\n",
        "            (pd.notna(row.get('Entry_Date')))\n",
        "        )\n",
        "\n",
        "        if new_open and not in_trade:\n",
        "            in_trade       = True\n",
        "            trade_active   = True\n",
        "            broken         = False\n",
        "            sl_val         = initial_atr\n",
        "            next_threshold = first_step_atr\n",
        "            prev_sl        = sl_val\n",
        "\n",
        "            # Alinear Entry_Date si fuera necesario (usamos 'time')\n",
        "            entry_dt = row.get('time')\n",
        "            if entry_dt is not None:\n",
        "                df.at[idx, 'Entry_Date'] = entry_dt\n",
        "\n",
        "        if in_trade:\n",
        "            m_high = row.get('atr_mult_high', np.nan)\n",
        "            m_low  = row.get('atr_mult_low',  np.nan)\n",
        "            best_pnl = np.nanmax([m_high, m_low])\n",
        "            best_pnl = 0.0 if np.isnan(best_pnl) else float(best_pnl)\n",
        "\n",
        "            if trade_active and not broken:\n",
        "                # subir el mÃºltiplo del stop cada vez que cruzamos un umbral\n",
        "                while best_pnl >= next_threshold:\n",
        "                    sl_val         += gap_first_step_atr\n",
        "                    next_threshold += gap_first_step_atr\n",
        "\n",
        "                # si volvemos por debajo del Ãºltimo SL, marcamos roto\n",
        "                below_prev = (\n",
        "                    (np.isfinite(m_high) and m_high < prev_sl) or\n",
        "                    (np.isfinite(m_low)  and m_low  < prev_sl)\n",
        "                )\n",
        "                if below_prev:\n",
        "                    broken       = True\n",
        "                    trade_active = False\n",
        "                    in_trade     = False\n",
        "\n",
        "            df.at[idx, col_name]      = np.nan if broken else sl_val\n",
        "            df.at[idx, 'Stop_Loss_atr'] = np.nan if broken else sl_val\n",
        "            prev_sl = sl_val\n",
        "\n",
        "    # â”€â”€ Convertir mÃºltiplos a precio y guardarlo en Stop_Loss_$ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    # Requiere: base_px, atr_base, Type y tick_dyn_atr vÃ¡lidos\n",
        "    try:\n",
        "        sl_mult = df[col_name].astype(float)\n",
        "        base    = df['base_px'].astype(float)\n",
        "        atr     = df['atr_base'].astype(float)\n",
        "        typ     = df['Type'].astype('string')\n",
        "\n",
        "        # precio = base Â± atr * mÃºltiplos (segÃºn direcciÃ³n)\n",
        "        stop_price = np.where(\n",
        "            (typ == 'Long')  & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "            base + atr * sl_mult,\n",
        "            np.where(\n",
        "                (typ == 'Short') & np.isfinite(sl_mult) & np.isfinite(base) & np.isfinite(atr),\n",
        "                base - atr * sl_mult,\n",
        "                np.nan\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Redondeo suave (mantÃ©n o ajusta si lo prefieres con mÃ¡s decimales)\n",
        "        df['Stop_Loss_$'] = pd.Series(stop_price, index=df.index).round(2)\n",
        "    except Exception:\n",
        "        # si faltan columnas o tipos, no rompemos el flujo\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- REST fallback to close a position --------------------------------------\n",
        "def _rest_close_position(auth_token: str,\n",
        "                         account_id: str,\n",
        "                         region: str,\n",
        "                         position_id: str,\n",
        "                         timeout: int = 20):\n",
        "    \"\"\"\n",
        "    Cierra una posiciÃ³n por REST. En MetaApi el actionType es POSITION_CLOSE_ID.\n",
        "    \"\"\"\n",
        "    url = f\"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade\"\n",
        "    payload = {\"actionType\": \"POSITION_CLOSE_ID\", \"positionId\": str(position_id)}\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"auth-token\": auth_token}\n",
        "    return requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
        "\n",
        "\n",
        "# --- Pull positions: fast attempts via RPC (short timeout) then REST --------\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    \"\"\"\n",
        "    Positions with short RPC timeouts + REST fallback.\n",
        "    Avoids hangs when websocket subscribe is flaky in Colab.\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    positions = []\n",
        "\n",
        "    async def _rpc_try(call, *args, **kwargs):\n",
        "        try:\n",
        "            return await asyncio.wait_for(call(*args, **kwargs), timeout=4)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    # 1) RPC (with symbol), then RPC (all)\n",
        "    if rpc_conn:\n",
        "        positions = await _rpc_try(rpc_conn.get_positions, symbol=symbol)\n",
        "        if not positions:\n",
        "            positions = await _rpc_try(rpc_conn.get_positions)\n",
        "\n",
        "    # 2) REST fallback\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "\n",
        "    return positions\n",
        "\n",
        "\n",
        "\n",
        "# --- Close order with RPC, fallback to REST if RPC fails --------------------\n",
        "async def close_order(df: pd.DataFrame,\n",
        "                      rpc_conn,\n",
        "                      symbol: str = SYMBOL,\n",
        "                      magic: int = 900001,\n",
        "                      close_col: str = \"Close_Trade\") -> None:\n",
        "    if df.empty or close_col not in df.columns:\n",
        "        return\n",
        "\n",
        "    sig = df[close_col].iloc[-1]\n",
        "    if not np.isfinite(sig):\n",
        "        return\n",
        "\n",
        "    sides_to_close = {\"BUY\"} if sig == -1 else {\"SELL\"}\n",
        "\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception as e:\n",
        "        print(\"âœ˜ No se pudieron leer posiciones:\", e)\n",
        "        return\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pmagic = p.get(\"magic\", None)\n",
        "        if pmagic is not None:\n",
        "            try:\n",
        "                return int(pmagic) == int(magic)\n",
        "            except Exception:\n",
        "                pass\n",
        "        cmt = str(p.get(\"comment\") or \"\")\n",
        "        return f\"magic={magic}\" in cmt\n",
        "\n",
        "    def _side_of(p) -> str:\n",
        "        t = p.get(\"type\")\n",
        "        if isinstance(t, str):\n",
        "            tt = t.upper()\n",
        "            if \"BUY\" in tt:  return \"BUY\"\n",
        "            if \"SELL\" in tt: return \"SELL\"\n",
        "        if t == 0: return \"BUY\"\n",
        "        if t == 1: return \"SELL\"\n",
        "        return \"\"\n",
        "\n",
        "    async def _try_close_rpc(pid: str) -> bool:\n",
        "        try:\n",
        "            await asyncio.wait_for(rpc_conn.close_position(pid), timeout=6)\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                await asyncio.wait_for(rpc_conn.close_position({\"positionId\": pid}), timeout=6)\n",
        "                return True\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "    for p in positions:\n",
        "        if not _has_magic(p):\n",
        "            continue\n",
        "        pid  = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "        side = _side_of(p)\n",
        "        if not pid or side not in sides_to_close:\n",
        "            continue\n",
        "\n",
        "        ok = await _try_close_rpc(pid)\n",
        "        if not ok:\n",
        "            # REST fallback\n",
        "            r = _rest_close_position(META_API_TOKEN, ACCOUNT_ID, REGION, pid)\n",
        "            ok = getattr(r, \"status_code\", 0) == 200\n",
        "\n",
        "        if ok:\n",
        "            print(f\"âœ… Cerrada {side} positionId={pid} (magic={magic})\")\n",
        "        else:\n",
        "            print(f\"âœ˜ No se pudo cerrar {side} positionId={pid} (RPC y REST fallaron)\")\n",
        "\n",
        "\n",
        "async def sync_stop_loss_from_df(df: pd.DataFrame,\n",
        "                                 rpc_conn,\n",
        "                                 symbol: str = SYMBOL,\n",
        "                                 magic: int = 900002,\n",
        "                                 tol: float = 0.01) -> None:\n",
        "    \"\"\"\n",
        "    Copia desde la posiciÃ³n viva los campos de mercado al DF.\n",
        "    Â¡Importante!: 'profits' solo se escribe en la ÃšLTIMA FILA para no\n",
        "    sobreescribir el histÃ³rico de filas previas.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "\n",
        "    _ensure_order_cols(df)\n",
        "\n",
        "    # 1) Leer posiciones (robusto a SDK sin 'symbol=')\n",
        "    try:\n",
        "        positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    except Exception:\n",
        "        positions = []\n",
        "\n",
        "    if not positions:\n",
        "        return\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        pm = p.get(\"magic\", None)\n",
        "        if pm is not None:\n",
        "            try:\n",
        "                if int(pm) == int(magic):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    def _split_comment_magic(cmt: str) -> tuple[str, int | None]:\n",
        "        if not cmt:\n",
        "            return \"\", None\n",
        "        m = re.search(r\"magic\\s*=\\s*(\\d+)\", cmt, flags=re.IGNORECASE)\n",
        "        mag = int(m.group(1)) if m else None\n",
        "        clean = cmt.split(\"|\", 1)[0].strip()\n",
        "        return clean, mag\n",
        "\n",
        "    # Preferir misma symbol + magic; si no, cualquiera con magic\n",
        "    pos = next((p for p in positions if _has_magic(p) and _sym_ok(p)), None)\n",
        "    if not pos:\n",
        "        pos = next((p for p in positions if _has_magic(p)), None)\n",
        "    if not pos:\n",
        "        return\n",
        "\n",
        "    order_id    = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    magic_val   = pos.get(\"magic\")\n",
        "    symbol_val  = pos.get(\"symbol\")\n",
        "    open_price  = pos.get(\"openPrice\") or pos.get(\"price\")\n",
        "    comment_raw = pos.get(\"comment\") or pos.get(\"brokerComment\") or None\n",
        "    stop_loss   = pos.get(\"stopLoss\")\n",
        "    volume_val  = pos.get(\"volume\") or pos.get(\"lots\")\n",
        "    profit_val  = (pos.get(\"profit\") if pos.get(\"profit\") is not None\n",
        "                   else pos.get(\"unrealizedProfit\") or pos.get(\"unrealized_profit\"))\n",
        "\n",
        "    # Type â†’ Long/Short\n",
        "    t = pos.get(\"type\")\n",
        "    typ = None\n",
        "    if isinstance(t, str):\n",
        "        tu = t.upper()\n",
        "        if \"BUY\" in tu:  typ = \"Long\"\n",
        "        if \"SELL\" in tu: typ = \"Short\"\n",
        "    elif t == 0:\n",
        "        typ = \"Long\"\n",
        "    elif t == 1:\n",
        "        typ = \"Short\"\n",
        "\n",
        "    entry_dt = pd.to_datetime(pos.get(\"time\"), errors=\"coerce\", utc=True)\n",
        "\n",
        "    clean_cmt, mag_from_cmt = _split_comment_magic(str(comment_raw or \"\"))\n",
        "\n",
        "    # 3) Determinar bloque del trade activo donde escribir (sin tocar 'profits' del bloque)\n",
        "    block_mask = pd.Series(False, index=df.index)\n",
        "    if order_id and (\"orderId\" in df.columns) and df[\"orderId\"].notna().any():\n",
        "        block_mask = (df[\"orderId\"] == order_id)\n",
        "    if (not block_mask.any()) and (\"Entry_Date\" in df.columns) and pd.notna(entry_dt):\n",
        "        ed = pd.to_datetime(df[\"Entry_Date\"], errors=\"coerce\", utc=True)\n",
        "        starts = ed.notna() & (ed != ed.shift(1))\n",
        "        if starts.any():\n",
        "            last_start = df.index[starts].max()\n",
        "            block_mask = (df.index >= last_start)\n",
        "        else:\n",
        "            block_mask = ed.notna() & (ed >= entry_dt)\n",
        "    if not block_mask.any():\n",
        "        block_mask.iloc[-1] = True  # asegura al menos la Ãºltima fila\n",
        "\n",
        "    # 4) Escribir valores de mercado en el bloque (EXCEPTO 'profits')\n",
        "    try:\n",
        "        if order_id:               df.loc[block_mask, \"orderId\"]   = str(order_id)\n",
        "        if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "            df.loc[block_mask, \"magic\"] = int(magic_val)\n",
        "        elif mag_from_cmt is not None:\n",
        "            df.loc[block_mask, \"magic\"] = int(mag_from_cmt)\n",
        "        else:\n",
        "            df.loc[block_mask, \"magic\"] = int(magic)\n",
        "\n",
        "        if symbol_val:             df.loc[block_mask, \"symbol\"]    = str(symbol_val)\n",
        "        if open_price is not None: df.loc[block_mask, \"openPrice\"] = float(open_price)\n",
        "        if clean_cmt:              df.loc[block_mask, \"comment\"]   = clean_cmt\n",
        "        if typ:                    df.loc[block_mask, \"Type\"]      = typ\n",
        "        if pd.notna(entry_dt):     df.loc[block_mask, \"Entry_Date\"]= entry_dt\n",
        "        if stop_loss is not None:  df.loc[block_mask, \"Real_SL\"]   = float(stop_loss)\n",
        "        if volume_val is not None: df.loc[block_mask, \"trade_size\"]= float(volume_val)\n",
        "        # â›” NO: df.loc[block_mask, \"profits\"] = profit_val  (no sobrescribir histÃ³rico)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 5) Escribir SIEMPRE en la ÃšLTIMA FILA los valores actuales (incluye 'profits')\n",
        "    last_idx = df.index[-1]\n",
        "    if order_id:               df.at[last_idx, \"orderId\"]   = str(order_id)\n",
        "    if magic_val is not None and str(magic_val).strip() != \"\":\n",
        "        df.at[last_idx, \"magic\"] = int(magic_val)\n",
        "    elif mag_from_cmt is not None:\n",
        "        df.at[last_idx, \"magic\"] = int(mag_from_cmt)\n",
        "    else:\n",
        "        df.at[last_idx, \"magic\"] = int(magic)\n",
        "\n",
        "    if symbol_val:             df.at[last_idx, \"symbol\"]    = str(symbol_val)\n",
        "    if open_price is not None: df.at[last_idx, \"openPrice\"] = float(open_price)\n",
        "    if clean_cmt:              df.at[last_idx, \"comment\"]   = clean_cmt\n",
        "    if typ:                    df.at[last_idx, \"Type\"]      = typ\n",
        "    if pd.notna(entry_dt):     df.at[last_idx, \"Entry_Date\"]= entry_dt\n",
        "    if stop_loss is not None:  df.at[last_idx, \"Real_SL\"]   = float(stop_loss)\n",
        "    if volume_val is not None: df.at[last_idx, \"trade_size\"]= float(volume_val)\n",
        "    if profit_val is not None: df.at[last_idx, \"profits\"]   = float(profit_val)\n",
        "\n",
        "\n",
        "def _last_two_distinct(values: pd.Series) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Devuelve (prev, last) con los dos Ãºltimos valores no-NaN distintos.\n",
        "    Si no hay prev distinto, prev = np.nan.\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        return (np.nan, np.nan)\n",
        "    last = float(s.iloc[-1])\n",
        "    prev = float(s[s != last].iloc[-1]) if (s != last).any() else np.nan\n",
        "    return (prev, last)\n",
        "\n",
        "\n",
        "# --- FIX 2: find position by magic robustly (with fallbacks) ---------------\n",
        "async def get_pos_with_magic(rpc_conn, symbol: str, magic: int) -> dict | None:\n",
        "    positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "    if not positions:\n",
        "        return None\n",
        "\n",
        "    def _has_magic(p) -> bool:\n",
        "        # magic can be under different keys/types depending on source\n",
        "        for key in (\"magic\", \"expertMagicNumber\", \"eaMagicNumber\"):\n",
        "            if key in p and p[key] is not None:\n",
        "                try:\n",
        "                    if int(p[key]) == int(magic):\n",
        "                        return True\n",
        "                except Exception:\n",
        "                    # sometimes it's a plain string \"900002\"\n",
        "                    if str(p[key]).strip() == str(magic):\n",
        "                        return True\n",
        "        # optional safety: encoded in comment (not your case, but harmless)\n",
        "        if f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _sym_ok(p) -> bool:\n",
        "        ps = str(p.get(\"symbol\") or \"\")\n",
        "        return (not symbol) or (ps.upper() == str(symbol).upper())\n",
        "\n",
        "    # Prefer same symbol first\n",
        "    for p in positions:\n",
        "        if _sym_ok(p) and _has_magic(p):\n",
        "            return p\n",
        "    # Then any matching magic (in case broker returns different casing/suffix)\n",
        "    for p in positions:\n",
        "        if _has_magic(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- FIX 3: stronger fallback in SL sync/modify ----------------------------\n",
        "async def modify_stoploss_if_changed(df_all: pd.DataFrame,\n",
        "                                     rpc_conn,\n",
        "                                     *,\n",
        "                                     symbol: str,\n",
        "                                     magic: int,\n",
        "                                     auth_token: str,\n",
        "                                     account_id: str,\n",
        "                                     region: str,\n",
        "                                     tol: float = 0.0) -> dict:\n",
        "    prev_sl, last_sl = _last_two_distinct(df_all.get(\"Stop_Loss_$\", pd.Series(dtype=float)))\n",
        "    if not np.isfinite(last_sl):\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": np.nan,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"Stop_Loss_$ vacÃ­o\"}\n",
        "\n",
        "    changed = (not np.isfinite(prev_sl)) or (abs(last_sl - prev_sl) > tol)\n",
        "    if not changed:\n",
        "        return {\"changed\": False, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": None}\n",
        "\n",
        "    # A) try by magic (now robust & with REST fallback)\n",
        "    pos = await get_pos_with_magic(rpc_conn, symbol=symbol, magic=magic)\n",
        "\n",
        "    # B) if still not found, try to match the last orderId in the CSV to broker position id\n",
        "    if not pos:\n",
        "        last_oid = None\n",
        "        if \"orderId\" in df_all.columns and df_all[\"orderId\"].notna().any():\n",
        "            last_oid = str(df_all[\"orderId\"].dropna().iloc[-1])\n",
        "        if last_oid:\n",
        "            positions = await _pull_positions_all_sources(rpc_conn, symbol)\n",
        "            for p in positions:\n",
        "                pid = str(p.get(\"id\") or p.get(\"positionId\") or \"\")\n",
        "                if pid == last_oid:\n",
        "                    pos = p\n",
        "                    break\n",
        "\n",
        "    if not pos:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"No hay posiciÃ³n con ese magic\"}\n",
        "\n",
        "    position_id = str(pos.get(\"id\") or pos.get(\"positionId\") or \"\")\n",
        "    if not position_id:\n",
        "        return {\"changed\": True, \"sent\": False, \"price\": last_sl,\n",
        "                \"position_id\": \"\", \"status_code\": None, \"err\": \"positionId vacÃ­o\"}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(\n",
        "        None,\n",
        "        lambda: _rest_modify_position(\n",
        "            auth_token=auth_token,\n",
        "            account_id=account_id,\n",
        "            region=region,\n",
        "            position_id=position_id,\n",
        "            stop_loss=float(last_sl),\n",
        "            timeout=15\n",
        "        )\n",
        "    )\n",
        "    ok = getattr(resp, \"status_code\", 0) == 200\n",
        "    err = None\n",
        "    if not ok:\n",
        "        try: err = json.dumps(resp.json())[:300]\n",
        "        except Exception: err = (getattr(resp, \"text\", \"\") or \"\")[:300]\n",
        "\n",
        "    return {\"changed\": True, \"sent\": ok, \"price\": last_sl,\n",
        "            \"position_id\": position_id, \"status_code\": getattr(resp, \"status_code\", None), \"err\": err}\n",
        "\n",
        "async def _pull_positions_all_sources(rpc_conn, symbol: str | None):\n",
        "    positions = []\n",
        "    # 1) RPC with symbol\n",
        "    try:\n",
        "        positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "    except Exception:\n",
        "        positions = []\n",
        "    # 2) RPC without symbol\n",
        "    if not positions:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions() or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "    # 3) REST fallback\n",
        "    if not positions:\n",
        "        r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "        if getattr(r, \"status_code\", 0) == 200:\n",
        "            try:\n",
        "                positions = r.json() or []\n",
        "            except Exception:\n",
        "                positions = []\n",
        "    return positions\n",
        "\n",
        "# Rutas a listas de columnas (una por lado)\n",
        "FEATS_LONG_PATH  = f\"{root_data}Results/{SYMBOL}_Long_M5M10_ImportantCols.csv\"\n",
        "FEATS_SHORT_PATH = f\"{root_data}Results/{SYMBOL}_Short_M5M10_ImportantCols.csv\"\n",
        "\n",
        "def _load_feature_list(path: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Reads a 1-column CSV (with or without header), removes junk entries like '0',\n",
        "    'Unnamed:*', '', and de-duplicates while preserving order.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"âš ï¸ Feature list not found: {path}. Falling back to model introspection.\")\n",
        "        return []\n",
        "\n",
        "    dfc = pd.read_csv(path, header=None)\n",
        "    # If there is an accidental header or extra columns, take the first column\n",
        "    if dfc.shape[1] > 1:\n",
        "        dfc = dfc.iloc[:, [0]]\n",
        "\n",
        "    raw = dfc.iloc[:, 0].astype(str).tolist()\n",
        "\n",
        "    def _ok(s: str) -> bool:\n",
        "        s2 = s.strip()\n",
        "        if s2 == \"\" or s2.lower().startswith(\"unnamed\") or s2.lower() == \"index\" or s2.isdigit():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    cleaned = [s.strip() for s in raw if _ok(s)]\n",
        "    # de-dup preserve order\n",
        "    seen, out = set(), []\n",
        "    for c in cleaned:\n",
        "        if c not in seen:\n",
        "            out.append(c)\n",
        "            seen.add(c)\n",
        "    return out\n",
        "\n",
        "\n",
        "FEATURES_LONG  = _load_feature_list(FEATS_LONG_PATH)\n",
        "FEATURES_SHORT = _load_feature_list(FEATS_SHORT_PATH)\n",
        "\n",
        "def _safe_create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uses your create_features(df) on 5m OHLCV (Title-case),\n",
        "    and ALSO carries through any precomputed 10m feature columns already\n",
        "    present on `df` (prefix '10min_10min_' or '10_min_10_min').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_in = df.copy()\n",
        "        col_map = {\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"}\n",
        "        for lo, hi in col_map.items():\n",
        "            if lo in df_in.columns and hi not in df_in.columns:\n",
        "                df_in[hi] = df_in[lo]\n",
        "\n",
        "        feats = create_features(df_in)  # your big feature builder (5m)\n",
        "\n",
        "        # carry 10m block that we already appended to df\n",
        "        extra_cols = [c for c in df.columns\n",
        "                      if c.startswith(\"10min_10min_\") or c.startswith(\"10_min_10_min\")]\n",
        "        if extra_cols:\n",
        "            # ensure same index length; most flows use the df index as the feature index\n",
        "            feats = feats.reindex(df.index)\n",
        "            feats = feats.join(df[extra_cols], how=\"left\")\n",
        "\n",
        "        return feats\n",
        "    except NameError:\n",
        "        # if create_features is unavailable, fall back to numeric columns only\n",
        "        return df.select_dtypes(include=[np.number]).copy()\n",
        "    except Exception:\n",
        "        return df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "def _expected_features_from_model(model) -> list[str] | None:\n",
        "    \"\"\"\n",
        "    Best-effort introspection for sklearn-style models to get the exact\n",
        "    feature name list used at fit time. Returns None if not available.\n",
        "    \"\"\"\n",
        "    # 1) Direct attribute (most sklearn estimators/pipelines after fit)\n",
        "    for attr in (\"feature_names_in_\",):\n",
        "        if hasattr(model, attr):\n",
        "            try:\n",
        "                names = list(getattr(model, attr))\n",
        "                if names:\n",
        "                    return [str(c) for c in names]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # 2) sklearn Pipeline / Voting / Bagging wrappers\n",
        "    for attr in (\"estimators_\", \"named_estimators_\", \"steps\", \"named_steps\"):\n",
        "        if hasattr(model, attr):\n",
        "            try:\n",
        "                obj = getattr(model, attr)\n",
        "                # pipeline.steps â†’ [('scaler', StandardScaler), ('clf', ...)]\n",
        "                if isinstance(obj, list):  # steps or list of estimators\n",
        "                    for _, step in obj:\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "                elif isinstance(obj, dict):  # named_steps / named_estimators_\n",
        "                    for step in obj.values():\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "                elif isinstance(obj, (tuple, set)):\n",
        "                    for step in obj:\n",
        "                        if hasattr(step, \"feature_names_in_\"):\n",
        "                            names = list(step.feature_names_in_)\n",
        "                            if names:\n",
        "                                return [str(c) for c in names]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _align_matrix(X: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Alinea X a 'cols': agrega faltantes con 0.0 y ordena columnas.\"\"\"\n",
        "    if not cols:\n",
        "        # sin lista â†’ usar todas las columnas disponibles\n",
        "        Z = X.copy()\n",
        "        return Z.fillna(0.0)\n",
        "    missing = [c for c in cols if c not in X.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            X[c] = 0.0\n",
        "        print(f\"âš ï¸ Faltan {len(missing)} features en live: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
        "    Z = X.loc[:, cols].copy()\n",
        "    return Z.fillna(0.0)\n",
        "\n",
        "def _predict_with_optional_proba(model, X: pd.DataFrame):\n",
        "    \"\"\"Devuelve (y_pred, proba, classes) funcionando con sklearn/keras.\"\"\"\n",
        "    proba, classes = None, None\n",
        "    # try predict_proba (sklearn)\n",
        "    try:\n",
        "        proba = model.predict_proba(X)\n",
        "        try:   classes = np.array(model.classes_)\n",
        "        except Exception: classes = np.arange(proba.shape[1])\n",
        "    except Exception:\n",
        "        # fallback keras: predict â†’ proba\n",
        "        try:\n",
        "            raw = model.predict(X)\n",
        "            raw = np.asarray(raw)\n",
        "            if raw.ndim == 2:\n",
        "                proba = raw\n",
        "                if proba.shape[1] == 1:  # binario [p1] â†’ [p0,p1]\n",
        "                    p1 = proba[:, 0]\n",
        "                    proba = np.column_stack([1.0 - p1, p1])\n",
        "                classes = np.arange(proba.shape[1])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # y_pred\n",
        "    try:\n",
        "        y_pred = model.predict(X)\n",
        "        y_pred = np.asarray(y_pred).ravel()\n",
        "    except Exception:\n",
        "        if proba is not None:\n",
        "            y_pred = np.argmax(proba, axis=1)\n",
        "        else:\n",
        "            raise RuntimeError(\"El modelo no soporta predict/predict_proba compatibles.\")\n",
        "    if classes is None and proba is not None:\n",
        "        classes = np.arange(proba.shape[1])\n",
        "    return y_pred, proba, classes\n",
        "\n",
        "# --- REPLACE your run_side_models_inplace with this version ------------\n",
        "def run_side_models_inplace(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    long_model,\n",
        "    short_model,\n",
        "    feature_cols_long: list[str] | None,\n",
        "    feature_cols_short: list[str] | None,\n",
        "    max_lookback: int = 1200,          # enough for Kalman 1000 + slope/vol windows\n",
        "    triggers_per_side: int = 1         # only predict at the latest trigger(s)\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute features & run ML ONLY where a trade could open:\n",
        "      - Open_Trade == +1 â†’ run LONG model at those bars\n",
        "      - Open_Trade == -1 â†’ run SHORT model at those bars\n",
        "\n",
        "    We build features on a compact tail slice (max_lookback) and only\n",
        "    materialize predictions on the trigger rows.\n",
        "    \"\"\"\n",
        "\n",
        "    if df is None or df.empty or (\"Open_Trade\" not in df.columns):\n",
        "        return df\n",
        "\n",
        "    # --- which rows really need predictions?\n",
        "    idx_long_all  = df.index[df[\"Open_Trade\"] ==  1]\n",
        "    idx_short_all = df.index[df[\"Open_Trade\"] == -1]\n",
        "\n",
        "    if len(idx_long_all) == 0 and len(idx_short_all) == 0:\n",
        "        # ensure prob columns exist, then nothing to do\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "        return df\n",
        "\n",
        "    # take the most recent trigger(s) per side to avoid recomputing everything\n",
        "    idx_long_targets  = list(idx_long_all[-triggers_per_side:])   if len(idx_long_all)  else []\n",
        "    idx_short_targets = list(idx_short_all[-triggers_per_side:])  if len(idx_short_all) else []\n",
        "\n",
        "    # quick exit: if the last trigger rows already have labels, skip\n",
        "    def _missing_any(pred_col: str, targets: list) -> bool:\n",
        "        if not targets:\n",
        "            return False\n",
        "        if pred_col not in df.columns:\n",
        "            return True\n",
        "        return df.loc[targets, pred_col].isna().any()\n",
        "\n",
        "    need_long  = _missing_any(\"label_ml_long\",  idx_long_targets)\n",
        "    need_short = _missing_any(\"label_ml_short\", idx_short_targets)\n",
        "    if not (need_long or need_short):\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "        return df\n",
        "\n",
        "    # ---- pick a compact tail slice to compute features once\n",
        "    # weâ€™ll include enough history and then only read the rows we care about\n",
        "    tail = df.tail(max_lookback + 50).copy()\n",
        "\n",
        "    # 10m block is expensive â†’ compute it only if required by the modelâ€™s feature lists\n",
        "    model_req_long  = _expected_features_from_model(long_model)  or []\n",
        "    model_req_short = _expected_features_from_model(short_model) or []\n",
        "\n",
        "    csv_req_long    = _preferred_feature_order(\"long\")  or (feature_cols_long  or [])\n",
        "    csv_req_short   = _preferred_feature_order(\"short\") or (feature_cols_short or [])\n",
        "\n",
        "    req_long  = model_req_long  or csv_req_long\n",
        "    req_short = model_req_short or csv_req_short\n",
        "\n",
        "    def _needs_10m(required_cols: list[str]) -> bool:\n",
        "        return any(isinstance(c, str) and (c.startswith(\"10min_10min_\") or c.startswith(\"10_min_10_min_\"))\n",
        "                   for c in (required_cols or []))\n",
        "\n",
        "    try:\n",
        "        if _needs_10m(req_long) or _needs_10m(req_short):\n",
        "            append_10m_features_inplace(tail, prefix=\"10min_10min_\", pre_scale=True)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ 10m feature block skipped: {e}\")\n",
        "\n",
        "    # Build base feature matrix once on the tail\n",
        "    feats_all = _safe_create_features(tail)\n",
        "\n",
        "    # Fix '10min' vs '10_min' aliasing if needed\n",
        "    feats_all = _ensure_10m_prefix_aliases(feats_all, (req_long or []) + (req_short or []))\n",
        "\n",
        "    # Ensure Encoded_* columns (if expected)\n",
        "    enc_n_long  = _encoded_span_from(req_long)\n",
        "    enc_n_short = _encoded_span_from(req_short)\n",
        "    for i in range(max(enc_n_long, enc_n_short)):\n",
        "        c = f\"Encoded_{i}\"\n",
        "        if c not in feats_all.columns:\n",
        "            feats_all[c] = 0.0\n",
        "\n",
        "    # Align matrices exactly to each sideâ€™s requirements\n",
        "    def _align_exact(X: pd.DataFrame, req: list[str]) -> pd.DataFrame:\n",
        "        if not req:\n",
        "            Z = X.select_dtypes(include=[np.number]).copy()\n",
        "            return Z.fillna(0.0)\n",
        "        missing = [c for c in req if c not in X.columns]\n",
        "        for c in missing:\n",
        "            X[c] = 0.0\n",
        "        Z = X.loc[:, req].copy()\n",
        "        return Z.fillna(0.0)\n",
        "\n",
        "    X_long_all  = _align_exact(feats_all.copy(), req_long)\n",
        "    X_short_all = _align_exact(feats_all.copy(), req_short)\n",
        "\n",
        "    # restrict to the trigger rows that actually live inside the tail slice\n",
        "    tail_idx = feats_all.index\n",
        "    idx_long  = [i for i in idx_long_targets  if i in tail_idx] if need_long  else []\n",
        "    idx_short = [i for i in idx_short_targets if i in tail_idx] if need_short else []\n",
        "\n",
        "    # helper â†’ works with sklearn or keras-like models\n",
        "    def _predict_with_optional_proba(model, X: pd.DataFrame):\n",
        "        proba, classes = None, None\n",
        "        try:\n",
        "            proba = model.predict_proba(X)\n",
        "            try:\n",
        "                classes = np.array(model.classes_)\n",
        "            except Exception:\n",
        "                classes = np.arange(proba.shape[1])\n",
        "        except Exception:\n",
        "            try:\n",
        "                raw = model.predict(X)\n",
        "                raw = np.asarray(raw)\n",
        "                if raw.ndim == 2:\n",
        "                    proba = raw\n",
        "                    if proba.shape[1] == 1:\n",
        "                        p1 = proba[:, 0]\n",
        "                        proba = np.column_stack([1.0 - p1, p1])\n",
        "                    classes = np.arange(proba.shape[1])\n",
        "            except Exception:\n",
        "                pass\n",
        "        try:\n",
        "            y_pred = model.predict(X)\n",
        "            y_pred = np.asarray(y_pred).ravel()\n",
        "        except Exception:\n",
        "            if proba is not None:\n",
        "                y_pred = np.argmax(proba, axis=1)\n",
        "            else:\n",
        "                raise RuntimeError(\"El modelo no soporta predict/predict_proba compatibles.\")\n",
        "        if classes is None and proba is not None:\n",
        "            classes = np.arange(proba.shape[1])\n",
        "        return y_pred, proba, classes\n",
        "\n",
        "    # LONG side only where Open_Trade == +1\n",
        "    if idx_long:\n",
        "        X = X_long_all.loc[idx_long]\n",
        "        yL, pL, cL = _predict_with_optional_proba(long_model, X)\n",
        "        df.loc[idx_long, \"label_ml_long\"] = yL\n",
        "        if pL is not None:\n",
        "            for j in range(pL.shape[1]):\n",
        "                cname = f\"prob_{(cL[j] if cL is not None else j)}_long\"\n",
        "                df.loc[idx_long, cname] = pL[:, j]\n",
        "        _ensure_prob_cols_present(df, \"long\")\n",
        "\n",
        "    # SHORT side only where Open_Trade == -1\n",
        "    if idx_short:\n",
        "        X = X_short_all.loc[idx_short]\n",
        "        yS, pS, cS = _predict_with_optional_proba(short_model, X)\n",
        "        df.loc[idx_short, \"label_ml_short\"] = yS\n",
        "        if pS is not None:\n",
        "            for j in range(pS.shape[1]):\n",
        "                cname = f\"prob_{(cS[j] if cS is not None else j)}_short\"\n",
        "                df.loc[idx_short, cname] = pS[:, j]\n",
        "        _ensure_prob_cols_present(df, \"short\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cvhz3M-fTIw",
      "metadata": {
        "id": "5cvhz3M-fTIw"
      },
      "outputs": [],
      "source": [
        "# ========================= MAIN (copy/paste) =========================\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    First run:\n",
        "      â€¢ fetch 900 5m candles\n",
        "      â€¢ compute ATR + signals (+ ML if an entry trigger exists)\n",
        "      â€¢ save ALL 900 rows to CSV (source=1, System_time filled)\n",
        "\n",
        "    Then:\n",
        "      â€¢ poll at 5m+3s while no position\n",
        "      â€¢ when a position with magic exists, follow every 20s:\n",
        "          - refresh data, recompute signals, close by Close_Trade\n",
        "          - recompute atr_close + tick_dyn_atr\n",
        "          - push SL change if Stop_Loss_$ changed\n",
        "          - sync market fields from API (incl. Real_SL)\n",
        "          - save to CSV\n",
        "    \"\"\"\n",
        "    # 0) Connect MetaApi / RPC\n",
        "    account  = await connect_metaapi(META_API_TOKEN, ACCOUNT_ID)\n",
        "    rpc_conn = account.get_rpc_connection()\n",
        "    await rpc_conn.connect()\n",
        "    asyncio.create_task(collect_10m_data(account))\n",
        "\n",
        "    # ---- strategy params ----------------------------------------------------\n",
        "    MAGIC    = 900002\n",
        "    bars_init = 900  # << first-run bars to compute features on\n",
        "\n",
        "    length_1 = 300\n",
        "    length_2 = 410\n",
        "    length_3 = 710\n",
        "    length_4 = 870\n",
        "\n",
        "    smooth_1 = 3\n",
        "    smooth_2 = 3\n",
        "    smooth_3 = 3\n",
        "    smooth_4 = 5\n",
        "\n",
        "    # ---- small helpers (local) ---------------------------------------------\n",
        "    def _frame_delta(tf: str) -> dt.timedelta:\n",
        "        tf = (tf or \"\").strip().lower()\n",
        "        if tf.endswith(\"mn\"):\n",
        "            tf = tf[:-2] + \"m\"\n",
        "        if tf.endswith(\"m\"):\n",
        "            return dt.timedelta(minutes=int(tf[:-1] or \"1\"))\n",
        "        if tf.endswith(\"h\"):\n",
        "            return dt.timedelta(hours=int(tf[:-1] or \"1\"))\n",
        "        if tf.endswith(\"d\"):\n",
        "            return dt.timedelta(days=int(tf[:-1] or \"1\"))\n",
        "        return dt.timedelta(minutes=1)\n",
        "\n",
        "    def _floor_to_frame(ts: dt.datetime, delta: dt.timedelta) -> dt.datetime:\n",
        "        if ts.tzinfo is None:\n",
        "            ts = ts.replace(tzinfo=dt.timezone.utc)\n",
        "        epoch = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
        "        secs  = int((ts - epoch).total_seconds())\n",
        "        step  = int(delta.total_seconds()) or 60\n",
        "        return epoch + dt.timedelta(seconds=(secs // step) * step)\n",
        "\n",
        "    async def _has_open_position_magic(rpc_conn, symbol: str, magic: int) -> bool:\n",
        "        # try RPC\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "        # REST fallback\n",
        "        if not positions:\n",
        "            r = _rest_get_positions(META_API_TOKEN, ACCOUNT_ID, REGION, symbol)\n",
        "            if getattr(r, \"status_code\", 0) == 200:\n",
        "                try:\n",
        "                    positions = r.json() or []\n",
        "                except Exception:\n",
        "                    positions = []\n",
        "        if not positions:\n",
        "            return False\n",
        "\n",
        "        for p in positions:\n",
        "            pm = p.get(\"magic\")\n",
        "            has_magic = False\n",
        "            if pm is not None:\n",
        "                try:\n",
        "                    has_magic = int(pm) == int(magic)\n",
        "                except Exception:\n",
        "                    has_magic = False\n",
        "            if not has_magic and f\"magic={magic}\" in str(p.get(\"comment\") or \"\"):\n",
        "                has_magic = True\n",
        "            if has_magic:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    async def _get_api_type(rpc_conn, symbol: str, magic: int) -> Optional[str]:\n",
        "        try:\n",
        "            positions = await rpc_conn.get_positions(symbol=symbol) or []\n",
        "        except Exception:\n",
        "            positions = []\n",
        "        def _has_magic(p):\n",
        "            pm = p.get(\"magic\", None)\n",
        "            ok = False\n",
        "            if pm is not None:\n",
        "                try:\n",
        "                    ok = int(pm) == int(magic)\n",
        "                except Exception:\n",
        "                    ok = False\n",
        "            if not ok:\n",
        "                ok = f\"magic={magic}\" in str(p.get(\"comment\") or \"\")\n",
        "            return ok\n",
        "        def _side_of(p):\n",
        "            t = p.get(\"type\")\n",
        "            if isinstance(t, str):\n",
        "                tt = t.upper()\n",
        "                if \"BUY\" in tt:  return \"BUY\"\n",
        "                if \"SELL\" in tt: return \"SELL\"\n",
        "            if t == 0: return \"BUY\"\n",
        "            if t == 1: return \"SELL\"\n",
        "            return \"\"\n",
        "        for p in positions:\n",
        "            if not _has_magic(p):\n",
        "                continue\n",
        "            s = _side_of(p)\n",
        "            if s:\n",
        "                return \"Long\" if s == \"BUY\" else \"Short\"\n",
        "        return None\n",
        "\n",
        "    def _sync_type_in_df(df_all: pd.DataFrame, api_type: Optional[str]) -> None:\n",
        "        if not api_type or df_all.empty:\n",
        "            return\n",
        "        last_oid = df_all.get(\"orderId\")\n",
        "        if last_oid is not None and last_oid.notna().any():\n",
        "            last_oid_val = last_oid.dropna().iloc[-1]\n",
        "            mask = df_all[\"orderId\"] == last_oid_val\n",
        "        else:\n",
        "            ed = pd.to_datetime(df_all.get(\"Entry_Date\"), errors=\"coerce\", utc=True)\n",
        "            starts = ed.notna() & (ed != ed.shift(1))\n",
        "            if starts.any():\n",
        "                start_idx = df_all.index[starts][-1]\n",
        "                mask = (df_all.index >= start_idx)\n",
        "            else:\n",
        "                mask = pd.Series(False, index=df_all.index)\n",
        "        if mask.any():\n",
        "            df_all.loc[mask, \"Type\"] = api_type\n",
        "        else:\n",
        "            df_all.at[df_all.index[-1], \"Type\"] = api_type\n",
        "\n",
        "    # 1) First-run: create/migrate CSV ----------------------------------------\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        # --- fetch 900 bars, compute, and SAVE ALL ROWS ---\n",
        "        df_hist = await get_candles_5m(account, start=None, limit=bars_init)\n",
        "        df_hist = (df_hist\n",
        "                   .drop_duplicates(\"time\")\n",
        "                   .sort_values(\"time\")\n",
        "                   .reset_index(drop=True))\n",
        "\n",
        "        if len(df_hist) >= 14:\n",
        "            df_hist[\"ATR\"] = ta.ATR(df_hist[\"high\"], df_hist[\"low\"], df_hist[\"close\"], 14).round(4)\n",
        "\n",
        "        df_all = df_hist.copy()\n",
        "\n",
        "        generate_trade_signals(\n",
        "            df_all, length_1, length_2, length_3, length_4,\n",
        "            smooth_1, smooth_2, smooth_3, smooth_4\n",
        "        )\n",
        "\n",
        "        maybe_compute_ml_for_entry(\n",
        "            df_all,\n",
        "            long_model=long_ml_model,\n",
        "            short_model=short_ml_model,\n",
        "            feature_cols_long=FEATURES_LONG,\n",
        "            feature_cols_short=FEATURES_SHORT\n",
        "        )\n",
        "        sync_unified_ml_cols(df_all)\n",
        "        _ensure_order_cols(df_all)\n",
        "\n",
        "        # mark provenance and stamp time on every row\n",
        "        df_all[\"source\"] = 1\n",
        "        stamp_system_time(df_all, mode=\"missing\")\n",
        "\n",
        "        save_csv(df_all)\n",
        "        print(f\"âœ” Archivo inicial creado: {len(df_all)} filas guardadas (se analizaron {bars_init} velas)\")\n",
        "    else:\n",
        "        migrate_csv_if_needed(FILE_PATH)\n",
        "\n",
        "    # 2) Live loop -------------------------------------------------------------\n",
        "    while True:\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ NO open position â†’ wait to next 5m+03s tick â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if not await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "            await asyncio.sleep(FETCH_INTERVAL)\n",
        "\n",
        "            now_utc   = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
        "            delta     = _frame_delta(time_frame_data)\n",
        "            this_bar  = _floor_to_frame(now_utc, delta)\n",
        "            prev_bar  = this_bar - delta\n",
        "\n",
        "            # fetch the last closed bar only\n",
        "            df_new = await get_candles_5m(account, start=None, limit=50)\n",
        "            df_new = (df_new[df_new[\"time\"] <= prev_bar]\n",
        "                      .drop_duplicates(\"time\")\n",
        "                      .sort_values(\"time\"))\n",
        "            if not df_new.empty:\n",
        "                df_new = df_new.iloc[[-1]]\n",
        "\n",
        "            df_all = _load_csv()\n",
        "            existing_times = set(pd.to_datetime(df_all[\"time\"], utc=True)) if not df_all.empty and \"time\" in df_all.columns else set()\n",
        "            if df_all.empty:\n",
        "                df_all = df_new.copy()\n",
        "            else:\n",
        "                df_all = (pd.concat([df_all, df_new], ignore_index=True)\n",
        "                            .drop_duplicates(\"time\")\n",
        "                            .sort_values(\"time\")\n",
        "                            .reset_index(drop=True))\n",
        "\n",
        "            if len(df_all) >= 14:\n",
        "                df_all[\"ATR\"] = ta.ATR(df_all[\"high\"], df_all[\"low\"], df_all[\"close\"], 14).round(4)\n",
        "\n",
        "            generate_trade_signals(\n",
        "                df_all, length_1, length_2, length_3, length_4,\n",
        "                smooth_1, smooth_2, smooth_3, smooth_4\n",
        "            )\n",
        "            maybe_compute_ml_for_entry(\n",
        "                df_all,\n",
        "                long_model=long_ml_model,\n",
        "                short_model=short_ml_model,\n",
        "                feature_cols_long=FEATURES_LONG,\n",
        "                feature_cols_short=FEATURES_SHORT\n",
        "            )\n",
        "            _ensure_order_cols(df_all)\n",
        "            sync_unified_ml_cols(df_all)\n",
        "\n",
        "            # mark new row(s) as source=1\n",
        "            if not df_new.empty:\n",
        "                new_times = set(pd.to_datetime(df_new[\"time\"], utc=True)) - existing_times\n",
        "                if new_times:\n",
        "                    df_all.loc[pd.to_datetime(df_all[\"time\"], utc=True).isin(new_times), \"source\"] = 1\n",
        "\n",
        "            # try to open trade (if ML permits)\n",
        "            await open_trade(df_all, rpc_conn, symbol=SYMBOL, lot=LOT, comment=COMMENT, magic=MAGIC)\n",
        "\n",
        "            # sync type and market fields, then save\n",
        "            api_type = await _get_api_type(rpc_conn, SYMBOL, MAGIC)\n",
        "            _sync_type_in_df(df_all, api_type)\n",
        "            await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "            stamp_system_time(df_all, \"last\")\n",
        "            save_csv(df_all)\n",
        "            print(dt.datetime.utcnow().strftime(\"%H:%M:%S\"),\n",
        "                  \"| actualizaciÃ³n (modo bÃºsqueda de entrada)\")\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ OPEN position â†’ follow every 20s â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        else:\n",
        "            print(\"â–¶ Modo seguimiento: posiciÃ³n con magic 900002 detectada\")\n",
        "\n",
        "            while await _has_open_position_magic(rpc_conn, SYMBOL, MAGIC):\n",
        "\n",
        "                await asyncio.sleep(FETCH_INTERVAL)\n",
        "\n",
        "                # load CSV and last time\n",
        "                df_all = _load_csv()\n",
        "                last_time = None\n",
        "                if not df_all.empty and \"time\" in df_all.columns:\n",
        "                    last_time = pd.to_datetime(df_all[\"time\"], errors=\"coerce\", utc=True).max()\n",
        "\n",
        "                # live snapshot; append only strictly new timestamps\n",
        "                df_new = await get_current_candle_snapshot(account, rpc_conn, symbol=SYMBOL, timeframe=time_frame_data)\n",
        "                df_new = df_new.drop_duplicates(\"time\").sort_values(\"time\")\n",
        "                df_inc = df_new[df_new[\"time\"] > last_time] if last_time is not None and pd.notna(last_time) else df_new\n",
        "\n",
        "                if not df_inc.empty:\n",
        "                    df_all = pd.concat([df_all, df_inc], ignore_index=True)\n",
        "\n",
        "                # recalc ATR and signals\n",
        "                if len(df_all) >= 14:\n",
        "                    df_all[\"ATR\"] = ta.ATR(df_all[\"high\"], df_all[\"low\"], df_all[\"close\"], 14).round(4)\n",
        "\n",
        "                generate_trade_signals(\n",
        "                    df_all, length_1, length_2, length_3, length_4,\n",
        "                    smooth_1, smooth_2, smooth_3, smooth_4\n",
        "                )\n",
        "                maybe_compute_ml_for_entry(\n",
        "                    df_all,\n",
        "                    long_model=long_ml_model,\n",
        "                    short_model=short_ml_model,\n",
        "                    feature_cols_long=FEATURES_LONG,\n",
        "                    feature_cols_short=FEATURES_SHORT\n",
        "                )\n",
        "                sync_unified_ml_cols(df_all)\n",
        "\n",
        "                # close order if Close_Trade says so\n",
        "                await close_order(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC, close_col=\"Close_Trade\")\n",
        "\n",
        "                # recompute ATR-based tracking + dynamic SL\n",
        "                atr_close(df_all)\n",
        "                tick_dyn_atr(df_all)\n",
        "\n",
        "                # if Stop_Loss_$ changed, push modification\n",
        "                sl_res = await modify_stoploss_if_changed(\n",
        "                    df_all, rpc_conn,\n",
        "                    symbol=SYMBOL, magic=MAGIC,\n",
        "                    auth_token=META_API_TOKEN,\n",
        "                    account_id=ACCOUNT_ID,\n",
        "                    region=REGION,\n",
        "                    tol=0.0\n",
        "                )\n",
        "\n",
        "                # always sync market fields (including Real_SL) from API\n",
        "                await sync_stop_loss_from_df(df_all, rpc_conn, symbol=SYMBOL, magic=MAGIC)\n",
        "\n",
        "                # stamp and mark source for new rows, then save\n",
        "                stamp_system_time(df_all, \"last\")\n",
        "                _ensure_order_cols(df_all)\n",
        "                if not df_inc.empty:\n",
        "                    df_all.loc[df_all[\"time\"].isin(df_inc[\"time\"]), \"source\"] = 9\n",
        "                else:\n",
        "                    df_all.at[df_all.index[-1], \"source\"] = 9\n",
        "                save_csv(df_all)\n",
        "\n",
        "                # logs\n",
        "                base = (dt.datetime.utcnow().strftime(\"%H:%M:%S\")\n",
        "                        + \" | seguimiento 20s: seÃ±ales, posible cierre, atr_close + tick_dyn_atr + sync_market\")\n",
        "                print(base)\n",
        "                if sl_res.get(\"changed\"):\n",
        "                    if sl_res.get(\"sent\"):\n",
        "                        print(f\"   â†³ SL actualizado en broker a {sl_res['price']:.2f} (positionId={sl_res.get('position_id','?')})\")\n",
        "                    else:\n",
        "                        msg_err = sl_res.get(\"err\") or \"error desconocido\"\n",
        "                        price = sl_res.get(\"price\", np.nan)\n",
        "                        print(f\"   â†³ SL cambiÃ³ a {price:.2f} pero no se enviÃ³ ({msg_err})\")\n",
        "\n",
        "            print(\"â¹ La posiciÃ³n con magic 900002 se cerrÃ³ â†’ regreso a bÃºsqueda de entrada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68155fee",
      "metadata": {
        "id": "68155fee"
      },
      "outputs": [],
      "source": [
        "# Background task to store 10-minute candles\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import asyncio\n",
        "\n",
        "FILE_PATH_10M = \"xauusd_data_10m.csv\"\n",
        "BARS_INIT_10M = 900\n",
        "\n",
        "async def collect_10m_data(account, symbol=SYMBOL, path=FILE_PATH_10M):\n",
        "    \"\"\"Fetch initial 10m candles (900 bars) and append new candle every 10 minutes.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe='10m', start_time=None, limit=BARS_INIT_10M)\n",
        "        df = pd.DataFrame([{ 'time': pd.to_datetime(c['time'], utc=True), 'open': c['open'], 'high': c['high'], 'low': c['low'], 'close': c['close'], 'volume': c.get('volume'), 'tickVolume': c.get('tickVolume'), 'spread': c.get('spread') } for c in candles])\n",
        "        if not df.empty:\n",
        "            df['time'] = pd.to_datetime(df['time'], utc=True).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            df.to_csv(path, index=False)\n",
        "    while True:\n",
        "        await asyncio.sleep(600)\n",
        "        candles = await account.get_historical_candles(symbol=symbol, timeframe='10m', start_time=None, limit=1)\n",
        "        if candles:\n",
        "            c = candles[-1]\n",
        "            row = pd.DataFrame([{ 'time': pd.to_datetime(c['time'], utc=True), 'open': c['open'], 'high': c['high'], 'low': c['low'], 'close': c['close'], 'volume': c.get('volume'), 'tickVolume': c.get('tickVolume'), 'spread': c.get('spread') }])\n",
        "            row['time'] = pd.to_datetime(row['time'], utc=True).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            if os.path.exists(path):\n",
        "                existing = pd.read_csv(path)\n",
        "                existing_times = pd.to_datetime(existing['time'], errors='coerce')\n",
        "                row_time = pd.to_datetime(row['time']).iloc[0]\n",
        "                if row_time not in existing_times:\n",
        "                    row.to_csv(path, mode='a', header=False, index=False)\n",
        "            else:\n",
        "                row.to_csv(path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W481PrVufZ8Z",
      "metadata": {
        "id": "W481PrVufZ8Z"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yI6v2q_xfRtg",
      "metadata": {
        "id": "yI6v2q_xfRtg"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# EJECUCIÃ“N\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()     # solo en notebooks\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cupR_iWNnzWJ"
      },
      "id": "cupR_iWNnzWJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}